# PLAN DE REFACTORISATION - GUIDE COPILOT
## Validation CH7 â†’ Validation CH7 V2 (Architecture Clean)

**StratÃ©gie**: Construction PARALLÃˆLE - on ne touche PAS Ã  `validation_ch7/` existant
**Dossier cible**: `validation_ch7_v2/` (nouveau, cohabitation avec l'ancien)
**Philosophie**: Ã‰LÃ‰VATION, pas destruction. PrÃ©servation totale des innovations.

---

## ğŸ¯ OBJECTIFS STRATÃ‰GIQUES

### Objectif #1: ZÃ©ro RÃ©gression Fonctionnelle
âœ… Tous les tests actuels continuent de marcher  
âœ… SystÃ¨me existant reste 100% opÃ©rationnel  
âœ… Migration progressive, pas "big bang"

### Objectif #2: PrÃ©servation des 7 Innovations Majeures
1. âœ… Cache additif intelligent (extension 600s â†’ 3600s sans recalcul)
2. âœ… Config-hashing MD5 pour validation checkpoint â†” config
3. âœ… Architecture de Controller avec state tracking autonome
4. âœ… Dual cache system (baseline universal + RL config-specific)
5. âœ… Checkpoint rotation automatique avec archivage
6. âœ… Templates LaTeX avec placeholders
7. âœ… Session tracking JSON (artifact counting)

### Objectif #3: Respect des 10 Principes SOLID
- SRP, OCP, LSP, ISP, DIP (voir PRINCIPES_ARCHITECTURAUX.md)
- DRY, Configuration externe, SoC, Testability, Explicit over implicit

### Objectif #4: MÃ©triques de SuccÃ¨s
| MÃ©trique | Avant | Cible |
|----------|-------|-------|
| Lignes domain test (section 7.6) | 1876 | <500 |
| Temps ajout section 7.8 | ~4h (4 fichiers modifiÃ©s) | <30min (2 fichiers crÃ©Ã©s) |
| TestabilitÃ© (mocks) | Impossible | 100% mockable |
| Couverture tests unitaires | 0% | >80% |
| Duplication code (logging, cache) | 5x rÃ©pÃ©tÃ© | 1x centralisÃ© |

---

## ğŸ“‚ STRUCTURE CIBLE: `validation_ch7_v2/`

```
validation_ch7_v2/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ entry_points/                          â† Layer 0: CLI, Kaggle
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cli.py ........................... CLI principal (arg parsing)
â”‚   â”‚   â”œâ”€â”€ kaggle_manager.py ................ Manager Kaggle (upload kernels)
â”‚   â”‚   â””â”€â”€ local_runner.py .................. Runner local (tests rapides)
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestration/                         â† Layer 1: Orchestration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py .......................... IOrchestrator interface
â”‚   â”‚   â”œâ”€â”€ validation_orchestrator.py ....... Orchestre tous les tests
â”‚   â”‚   â”œâ”€â”€ test_runner.py ................... ExÃ©cute un test individuel
â”‚   â”‚   â””â”€â”€ test_factory.py .................. Factory pattern pour tests
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                                â† Layer 2: Validation Domain (MÃ‰TIER)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py .......................... ValidationTest abstract class
â”‚   â”‚   â”œâ”€â”€ models.py ........................ ValidationResult, TestConfig
â”‚   â”‚   â”œâ”€â”€ section_7_3_analytical.py ........ Tests analytiques (Riemann, convergence)
â”‚   â”‚   â”œâ”€â”€ section_7_4_calibration.py ....... Calibration sur donnÃ©es rÃ©elles
â”‚   â”‚   â”œâ”€â”€ section_7_5_digital_twin.py ...... Validation jumeau numÃ©rique
â”‚   â”‚   â”œâ”€â”€ section_7_6_rl_performance.py .... Performance RL (CÅ’UR, 400 lignes mÃ©tier pur)
â”‚   â”‚   â””â”€â”€ section_7_7_robustness.py ........ Tests de robustesse
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/                        â† Layer 3: Infrastructure
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py ........................ Logging centralisÃ© (DRY)
â”‚   â”‚   â”œâ”€â”€ config.py ........................ Config manager (chargement YAML)
â”‚   â”‚   â”œâ”€â”€ artifact_manager.py .............. Gestion artefacts (checkpoints, cache)
â”‚   â”‚   â”œâ”€â”€ session.py ....................... Session metadata (JSON tracking)
â”‚   â”‚   â””â”€â”€ errors.py ........................ Custom exceptions
â”‚   â”‚
â”‚   â””â”€â”€ reporting/                             â† Sub-layer: Reporting
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ latex_generator.py ............... GÃ©nÃ©ration LaTeX (templates)
â”‚       â””â”€â”€ metrics_aggregator.py ............ AgrÃ©gation mÃ©triques
â”‚
â”œâ”€â”€ configs/                                   â† Configuration externalisÃ©e
â”‚   â”œâ”€â”€ base.yml ............................. Config par dÃ©faut
â”‚   â”œâ”€â”€ quick_test.yml ....................... Config tests rapides (CI/CD)
â”‚   â”œâ”€â”€ full_test.yml ........................ Config tests complets (Kaggle)
â”‚   â””â”€â”€ sections/
â”‚       â”œâ”€â”€ section_7_3.yml .................. Config analytique
â”‚       â”œâ”€â”€ section_7_4.yml .................. Config calibration
â”‚       â”œâ”€â”€ section_7_5.yml .................. Config digital twin
â”‚       â”œâ”€â”€ section_7_6.yml .................. Config RL (hyperparams)
â”‚       â””â”€â”€ section_7_7.yml .................. Config robustesse
â”‚
â”œâ”€â”€ templates/                                 â† Templates LaTeX
â”‚   â”œâ”€â”€ base.tex ............................. Template base
â”‚   â”œâ”€â”€ section_7_3.tex ...................... Section analytique
â”‚   â”œâ”€â”€ section_7_4.tex ...................... Section calibration
â”‚   â”œâ”€â”€ section_7_5.tex ...................... Section digital twin
â”‚   â”œâ”€â”€ section_7_6.tex ...................... Section RL
â”‚   â””â”€â”€ section_7_7.tex ...................... Section robustesse
â”‚
â”œâ”€â”€ tests/                                     â† Tests unitaires (NOUVEAU!)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_domain/
â”‚   â”‚   â”œâ”€â”€ test_section_7_3_analytical.py ... Tests unitaires section 7.3
â”‚   â”‚   â”œâ”€â”€ test_section_7_6_rl_performance.py Tests unitaires section 7.6 (mocks!)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ test_infrastructure/
â”‚   â”‚   â”œâ”€â”€ test_artifact_manager.py ......... Tests cache/checkpoint logic
â”‚   â”‚   â”œâ”€â”€ test_config.py ................... Tests config loading
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ test_integration/
â”‚       â”œâ”€â”€ test_full_workflow.py ............ Tests end-to-end
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ checkpoints/                               â† Checkpoints RL (Git-tracked)
â”‚   â””â”€â”€ section_7_6/
â”‚       â”œâ”€â”€ traffic_light_control_checkpoint_abc12345_100_steps.zip
â”‚       â””â”€â”€ archived/ ........................ Checkpoints incompatibles
â”‚
â”œâ”€â”€ cache/                                     â† Cache simulation (Git-tracked)
â”‚   â””â”€â”€ section_7_6/
â”‚       â”œâ”€â”€ traffic_light_control_baseline_cache.pkl (universel)
â”‚       â””â”€â”€ traffic_light_control_abc12345_rl_cache.pkl (config-specific)
â”‚
â””â”€â”€ README.md ................................ Documentation architecture
```

---

## ğŸš€ PLAN D'IMPLÃ‰MENTATION PAR PHASES

### Phase 0: PrÃ©paration (30 minutes)
**Objectif**: CrÃ©er structure de base, pas de code mÃ©tier

**Actions**:
1. CrÃ©er dossier `validation_ch7_v2/`
2. CrÃ©er structure de dossiers (vide)
3. CrÃ©er `README.md` avec architecture expliquÃ©e
4. CrÃ©er `__init__.py` partout

**Validation Phase 0**:
```bash
# VÃ©rifier structure
tree validation_ch7_v2/ -L 3
# â†’ Doit montrer arborescence complÃ¨te
```

---

### Phase 1: Interfaces & Base Classes (2 heures)
**Objectif**: CrÃ©er les abstractions (interfaces, classes de base)

#### ğŸ¤– PROMPT COPILOT #1.1: Base Classes Domain

```markdown
# CONTEXTE
Je refactorise un systÃ¨me de validation (validation_ch7 â†’ validation_ch7_v2).
Architecture cible: Domain-Driven Design avec layers sÃ©parÃ©s.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/domain/base.py` avec:

1. **ValidationTest** (abstract class):
   - MÃ©thode abstraite: `run() -> ValidationResult`
   - PropriÃ©tÃ© abstraite: `name: str`
   - MÃ©thode helper: `_validate_prerequisites()` (optionnel)

2. **ValidationResult** (dataclass):
   - `passed: bool` - Test passed or failed
   - `metrics: Dict[str, float]` - Numerical metrics (convergence order, improvement %, etc.)
   - `artifacts: Dict[str, Path]` - Generated files (figures, data, LaTeX)
   - `errors: List[str]` - Error messages if failed
   - `warnings: List[str]` - Non-critical warnings
   - `metadata: Dict[str, Any]` - Additional info (duration, device, timestamp)

3. **TestConfig** (dataclass):
   - `section_name: str` - e.g., "section_7_6_rl_performance"
   - `quick_test: bool` - Fast mode for CI/CD
   - `scenario: Optional[str]` - For sections with multiple scenarios
   - `device: str` - "cpu" or "gpu"
   - `output_dir: Path` - Where to save results

# CONTRAINTES
- Type hints PARTOUT
- Docstrings complÃ¨tes (Google style)
- ABC (Abstract Base Class) pour ValidationTest
- dataclass pour ValidationResult et TestConfig
- NO implementation logic (juste les structures)

# EXEMPLE D'UTILISATION ATTENDU
```python
# Dans domain/section_7_6_rl_performance.py (futur)
class RLPerformanceTest(ValidationTest):
    @property
    def name(self) -> str:
        return "section_7_6_rl_performance"
    
    def run(self) -> ValidationResult:
        # Logique mÃ©tier ici
        return ValidationResult(
            passed=True,
            metrics={"travel_time_improvement": 28.7},
            artifacts={"before_after_plot": Path("figure.png")},
            errors=[],
            warnings=[],
            metadata={"duration_seconds": 3600}
        )
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `domain/base.py` (~100 lignes, abstraction pure)

---

#### ğŸ¤– PROMPT COPILOT #1.2: Custom Exceptions

```markdown
# CONTEXTE
SystÃ¨me de validation avec architecture en layers. Besoin d'exceptions spÃ©cifiques.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/infrastructure/errors.py` avec hiÃ©rarchie d'exceptions:

1. **ValidationError** (base):
   - Classe mÃ¨re pour toutes les erreurs de validation
   - Attribut: `context: Dict[str, Any]` (metadata pour debugging)

2. **ConfigError** (hÃ©rite ValidationError):
   - Config YAML invalide ou manquante
   - Exemple: "section_7_6.yml not found"

3. **CheckpointError** (hÃ©rite ValidationError):
   - Erreurs liÃ©es aux checkpoints RL
   - Exemple: "Checkpoint config hash mismatch"

4. **CacheError** (hÃ©rite ValidationError):
   - Erreurs de cache (corruption, incompatibilitÃ©)
   - Exemple: "Cache coherence validation failed"

5. **SimulationError** (hÃ©rite ValidationError):
   - Erreurs pendant simulation ARZ
   - Exemple: "Mass conservation violated"

6. **OrchestrationError** (hÃ©rite ValidationError):
   - Erreurs d'orchestration de tests
   - Exemple: "Test dependency not met"

# CONTRAINTES
- Type hints partout
- Docstrings explicatives
- Message d'erreur DESCRIPTIF (pas juste "Error")
- Context dict pour debugging riche

# EXEMPLE D'UTILISATION
```python
try:
    checkpoint = load_checkpoint(path)
except FileNotFoundError:
    raise CheckpointError(
        f"Checkpoint not found: {path}",
        context={"path": path, "section": "7.6", "scenario": "traffic_light"}
    )
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `infrastructure/errors.py` (~80 lignes)

---

#### ğŸ¤– PROMPT COPILOT #1.3: Orchestrator Interface

```markdown
# CONTEXTE
Architecture avec orchestration centralisÃ©e. Besoin d'interface IOrchestrator.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/orchestration/base.py` avec:

1. **IOrchestrator** (Protocol ou ABC):
   - `run_all_tests() -> List[ValidationResult]` - ExÃ©cute tous les tests
   - `run_single_test(test: ValidationTest) -> ValidationResult` - ExÃ©cute un test
   - `run_section(section_name: str) -> ValidationResult` - ExÃ©cute une section

2. **ITestRunner** (Protocol ou ABC):
   - `run(test: ValidationTest) -> ValidationResult` - ExÃ©cute un test
   - `setup()` - PrÃ©paration avant test
   - `teardown()` - Nettoyage aprÃ¨s test

# CONTRAINTES
- Utiliser `typing.Protocol` (duck typing) ou ABC
- Type hints partout
- Docstrings claires
- Interface MINIMALE (ISP - Interface Segregation Principle)

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `orchestration/base.py` (~60 lignes)

---

**Validation Phase 1**:
```python
# Test d'import
from validation_ch7_v2.scripts.domain.base import ValidationTest, ValidationResult
from validation_ch7_v2.scripts.infrastructure.errors import ConfigError
from validation_ch7_v2.scripts.orchestration.base import IOrchestrator

# â†’ Pas d'erreurs d'import
```

---

### Phase 2: Infrastructure Layer (4 heures)
**Objectif**: CrÃ©er les modules d'infrastructure (logger, config, artifact manager, session)

#### ğŸ¤– PROMPT COPILOT #2.1: Logger CentralisÃ©

```markdown
# CONTEXTE
Ancien systÃ¨me: chaque test a son propre logging setup (VIOLATION DRY).
Nouveau systÃ¨me: logger centralisÃ©, utilisÃ© par tous.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/infrastructure/logger.py` avec:

1. **setup_logger**(name: str, level: int = logging.INFO, log_file: Optional[Path] = None) -> logging.Logger:
   - Configure un logger avec nom spÃ©cifique
   - Format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`
   - Handler console (stdout) + handler fichier optionnel
   - Flush immÃ©diat (Kaggle stdout buffering issue)

2. **get_logger**(name: str) -> logging.Logger:
   - RÃ©cupÃ¨re un logger existant ou crÃ©e s'il n'existe pas

3. **PATTERNS DE LOGGING CONSTANTS**:
   - DEBUG_BC_RESULT = "[DEBUG_BC_RESULT]"
   - DEBUG_PRIMITIVES = "[DEBUG_PRIMITIVES]"
   - DEBUG_FLUXES = "[DEBUG_FLUXES]"
   - DEBUG_CACHE = "[DEBUG_CACHE]"
   - DEBUG_CHECKPOINT = "[DEBUG_CHECKPOINT]"

# INNOVATION Ã€ PRÃ‰SERVER
L'ancien systÃ¨me avait des patterns de logging structurÃ©s (e.g., [DEBUG_BC_RESULT])
qui permettent de filtrer les logs facilement. GARDER cette approche.

# CONTRAINTES
- Type hints partout
- Docstrings
- Thread-safe (logging est thread-safe par dÃ©faut)
- Pas de global state mutable

# EXEMPLE D'UTILISATION
```python
from validation_ch7_v2.scripts.infrastructure.logger import setup_logger, DEBUG_CHECKPOINT

logger = setup_logger("rl_validation", log_file=Path("debug.log"))
logger.info(f"{DEBUG_CHECKPOINT} Loading checkpoint from {path}")
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `infrastructure/logger.py` (~100 lignes)

---

#### ğŸ¤– PROMPT COPILOT #2.2: Config Manager

```markdown
# CONTEXTE
Ancien systÃ¨me: configs hardcodÃ©es dans le code (TRAINING_EPISODES=100).
Nouveau systÃ¨me: configs externalisÃ©es en YAML.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/infrastructure/config.py` avec:

1. **ConfigManager** class:
   - `__init__(config_dir: Path)` - Initialise avec dossier de configs
   - `load_base_config() -> Dict` - Charge base.yml
   - `load_section_config(section_name: str) -> SectionConfig` - Charge section_7_X.yml
   - `load_all_sections() -> List[SectionConfig]` - DÃ©couverte automatique

2. **SectionConfig** dataclass:
   - `name: str` - e.g., "section_7_6_rl_performance"
   - `description: str` - Description textuelle
   - `revendication: str` - e.g., "R5: Performance supÃ©rieure RL"
   - `estimated_duration_minutes: int` - DurÃ©e estimÃ©e
   - `quick_test_duration_minutes: int` - DurÃ©e en mode rapide
   - `hyperparameters: Dict[str, Any]` - Hyperparams spÃ©cifiques (RL, calibration, etc.)
   - `output_subdir: str` - e.g., "section_7_6_rl_performance"

3. **MÃ©thode helper**:
   - `merge_configs(base: Dict, section: Dict) -> Dict` - Merge base + section

# FORMAT YAML ATTENDU
```yaml
# configs/sections/section_7_6.yml
name: section_7_6_rl_performance
description: "Validation performance agents RL vs baseline"
revendication: "R5: Performance supÃ©rieure des agents RL"
estimated_duration_minutes: 180
quick_test_duration_minutes: 15

hyperparameters:
  training:
    episodes: 5000
    buffer_size: 50000
    learning_rate: 1e-3
    batch_size: 32
  quick_test:
    episodes: 100
    duration_per_episode: 120
  full_test:
    episodes: 5000
    duration_per_episode: 3600
```

# CONTRAINTES
- Utiliser pyyaml
- Type hints partout
- Validation de config (schema checking optionnel)
- Docstrings

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `infrastructure/config.py` (~150 lignes)

---

#### ğŸ¤– PROMPT COPILOT #2.3: Artifact Manager (Cache + Checkpoints)

```markdown
# CONTEXTE
INNOVATION MAJEURE Ã  prÃ©server: systÃ¨me sophistiquÃ© de cache/checkpoints.
- Cache additif intelligent (extension 600s â†’ 3600s sans recalcul)
- Config-hashing MD5 pour validation checkpoint â†” config
- Dual cache: baseline universel + RL config-specific
- Checkpoint rotation automatique avec archivage

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/infrastructure/artifact_manager.py` avec:

## 1. **ArtifactManager** class:

### Cache Baseline (universel)
- `save_baseline_cache(scenario: str, states: List, duration: float, control_interval: float)`
  - Format: `{scenario}_baseline_cache.pkl` (PAS de config_hash)
  - Rationale: Fixed-time controller â†’ comportement universel

- `load_baseline_cache(scenario: str, required_duration: float) -> Optional[List]`
  - Charge cache si existe et suffisant
  - Retourne None si pas de cache

- `extend_baseline_cache(scenario: str, existing_states: List, target_duration: float) -> List`
  - Extension ADDITIVE: reprend depuis cached_states[-1]
  - Simule UNIQUEMENT l'extension manquante
  - ConcatÃ¨ne cached + extension

### Cache RL (config-specific)
- `save_rl_cache(scenario: str, config_hash: str, model_path: Path, total_timesteps: int)`
  - Format: `{scenario}_{config_hash}_rl_cache.pkl`
  - Stocke metadata: model_path, timesteps, config_hash

- `load_rl_cache(scenario: str, config_hash: str, required_timesteps: int) -> Optional[Dict]`
  - Valide config_hash
  - VÃ©rifie existence model file
  - Retourne metadata ou None

### Checkpoints RL
- `save_checkpoint(model, scenario: str, config_hash: str, steps: int)`
  - Format: `{scenario}_checkpoint_{config_hash}_{steps}_steps.zip`
  - Sauvegarde model + replay buffer

- `load_checkpoint(scenario: str, config_hash: str) -> Optional[Path]`
  - Cherche checkpoint le plus rÃ©cent avec config_hash
  - Valide config_hash
  - Retourne path ou None

- `validate_checkpoint_config(checkpoint_path: Path, config_hash: str) -> bool`
  - Extrait hash depuis checkpoint filename
  - Compare avec current config_hash

- `archive_incompatible_checkpoint(checkpoint_path: Path, old_config_hash: str)`
  - DÃ©place vers archived/ avec suffix _CONFIG_{old_hash}

### Config Hashing
- `compute_config_hash(scenario_path: Path) -> str`
  - MD5 du YAML (8 chars)
  - Pour validation checkpoint â†” config

## 2. **Filesystem Helpers**:
- `ensure_dir(path: Path)` - CrÃ©e dossier si n'existe pas
- `list_checkpoints(scenario: str) -> List[Path]` - Liste tous les checkpoints d'un scenario

# CONTRAINTES
- Type hints partout
- Docstrings DÃ‰TAILLÃ‰ES (cette logique est complexe!)
- Logging avec patterns ([CACHE], [CHECKPOINT])
- Thread-safe si possible (file locking)

# EXEMPLE D'UTILISATION
```python
mgr = ArtifactManager(base_dir=Path("validation_ch7_v2/"))

# Cache baseline (universel)
mgr.save_baseline_cache("traffic_light_control", states, 3600.0, 15.0)
cached = mgr.load_baseline_cache("traffic_light_control", 7200.0)
if cached and len(cached) < required_steps:
    extended = mgr.extend_baseline_cache("traffic_light_control", cached, 7200.0)

# Cache RL (config-specific)
config_hash = mgr.compute_config_hash(Path("scenario.yml"))
mgr.save_rl_cache("traffic_light", config_hash, Path("model.zip"), 10000)

# Checkpoint
mgr.save_checkpoint(model, "traffic_light", config_hash, 5000)
checkpoint_path = mgr.load_checkpoint("traffic_light", config_hash)
if checkpoint_path:
    is_valid = mgr.validate_checkpoint_config(checkpoint_path, config_hash)
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `infrastructure/artifact_manager.py` (~400 lignes, CÅ’UR de l'innovation)

---

#### ğŸ¤– PROMPT COPILOT #2.4: Session Manager

```markdown
# CONTEXTE
Innovation Ã  prÃ©server: session_summary.json (tracking des artefacts gÃ©nÃ©rÃ©s).

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/infrastructure/session.py` avec:

1. **SessionManager** class:
   - `__init__(section_name: str, output_dir: Path)`
   - `create_directory_structure()` - CrÃ©e figures/, data/, latex/, etc.
   - `register_artifact(artifact_type: str, path: Path)` - Track un artefact
   - `save_summary()` - GÃ©nÃ¨re session_summary.json

2. **Structure de dossiers crÃ©Ã©e**:
   ```
   output_dir/
   â”œâ”€â”€ figures/ ................ PNG/PDF generated
   â”œâ”€â”€ data/
   â”‚   â”œâ”€â”€ npz/ ................ NumPy arrays
   â”‚   â”œâ”€â”€ scenarios/ .......... YAML configs
   â”‚   â””â”€â”€ metrics/ ............ JSON metrics
   â””â”€â”€ latex/ .................. Generated LaTeX snippets
   ```

3. **Format session_summary.json**:
   ```json
   {
     "section_name": "section_7_6_rl_performance",
     "timestamp": "2025-10-16T14:30:00",
     "artifacts": {
       "figures": ["before_after.png", "learning_curve.png"],
       "data_npz": ["baseline_trajectory.npz", "rl_trajectory.npz"],
       "data_metrics": ["performance_metrics.json"],
       "latex": ["section_7_6_content.tex"]
     },
     "artifact_count": 6
   }
   ```

# CONTRAINTES
- Type hints
- Docstrings
- Logging des opÃ©rations
- mkdir avec parents=True, exist_ok=True

# EXEMPLE D'UTILISATION
```python
session = SessionManager("section_7_6_rl_performance", Path("outputs/"))
session.create_directory_structure()
session.register_artifact("figure", Path("outputs/figures/plot.png"))
session.save_summary()  # â†’ CrÃ©e session_summary.json
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `infrastructure/session.py` (~150 lignes)

---

**Validation Phase 2**:
```python
# Test imports
from validation_ch7_v2.scripts.infrastructure.logger import setup_logger
from validation_ch7_v2.scripts.infrastructure.config import ConfigManager, SectionConfig
from validation_ch7_v2.scripts.infrastructure.artifact_manager import ArtifactManager
from validation_ch7_v2.scripts.infrastructure.session import SessionManager

# Test fonctionnel simple
logger = setup_logger("test")
logger.info("âœ“ Logger works")

config_mgr = ConfigManager(Path("validation_ch7_v2/configs"))
# (configs YAML pas encore crÃ©Ã©s, mais classe fonctionne)

artifact_mgr = ArtifactManager(Path("validation_ch7_v2/"))
config_hash = artifact_mgr.compute_config_hash(Path("dummy.yml"))
print(f"âœ“ Config hash: {config_hash}")

session = SessionManager("test", Path("outputs/"))
session.create_directory_structure()
print("âœ“ Session structure created")
```

---

### Phase 3: Domain Layer - Extraction MÃ©tier (6 heures)
**Objectif**: Extraire la logique mÃ©tier pure depuis ancien systÃ¨me

**FOCUS**: `test_section_7_6_rl_performance.py` (1876 lignes â†’ 400 lignes mÃ©tier pur)

#### ğŸ¤– PROMPT COPILOT #3.1: Section 7.6 RL Performance (MÃ‰TIER PUR)

```markdown
# CONTEXTE CRITIQUE
Tu vas extraire la LOGIQUE MÃ‰TIER PURE depuis un fichier monolithique de 1876 lignes.
Ce fichier est le CÅ’UR du systÃ¨me, il a survÃ©cu Ã  35 bugs, il contient des innovations majeures.

**Fichier source**: `validation_ch7/scripts/test_section_7_6_rl_performance.py`
**Fichier cible**: `validation_ch7_v2/scripts/domain/section_7_6_rl_performance.py`

**Principe**: EXTRAIRE le mÃ©tier, DÃ‰LÃ‰GUER l'infrastructure.

# TÃ‚CHE

## 1. Analyser l'ancien fichier (lignes 1-1876)

**Sections Ã  EXTRAIRE (mÃ©tier pur)**:
- Lines 612-702: `BaselineController` class (logique contrÃ´le fixed-time)
- Lines 637-702: `RLController` class (logique contrÃ´le RL)
- Lines 705-938: `run_control_simulation()` (simulation ARZ avec controller)
- Lines 941-1021: `evaluate_traffic_performance()` (calcul mÃ©triques)
- Lines 1024-1283: `train_rl_agent()` (entraÃ®nement RL avec PPO/DQN)
- Lines 1286-1468: `run_performance_comparison()` (baseline vs RL)

**Sections Ã  NE PAS COPIER (infrastructure, dÃ©lÃ©guer)**:
- Lines 127-160: `_setup_debug_logging()` â†’ Utiliser `infrastructure.logger`
- Lines 230-327: Checkpoint validation/archiving â†’ Utiliser `ArtifactManager`
- Lines 350-580: Cache management â†’ Utiliser `ArtifactManager`
- Lines 1585-1731: Figure generation + LaTeX â†’ Utiliser `reporting.latex_generator`
- Lines 1734-1850: Template filling â†’ Utiliser `reporting.latex_generator`

## 2. CrÃ©er la nouvelle classe

```python
from validation_ch7_v2.scripts.domain.base import ValidationTest, ValidationResult, TestConfig
from validation_ch7_v2.scripts.infrastructure.logger import get_logger, DEBUG_CHECKPOINT
from validation_ch7_v2.scripts.infrastructure.artifact_manager import ArtifactManager
from validation_ch7_v2.scripts.infrastructure.config import SectionConfig

class RLPerformanceTest(ValidationTest):
    """
    Validation Section 7.6: Performance RL vs Baseline.
    
    Teste la revendication R5: Performance supÃ©rieure des agents RL
    dans le contexte bÃ©ninois (comparaison avec baseline fixed-time).
    
    INNOVATIONS PRÃ‰SERVÃ‰ES:
    - Cache additif intelligent (dÃ©lÃ©guÃ© Ã  ArtifactManager)
    - Config-hashing MD5 (dÃ©lÃ©guÃ© Ã  ArtifactManager)
    - Controller autonome avec state tracking
    - Dual cache system (dÃ©lÃ©guÃ© Ã  ArtifactManager)
    """
    
    def __init__(
        self,
        config: SectionConfig,
        artifact_manager: ArtifactManager,
        logger: logging.Logger = None
    ):
        self.config = config
        self.artifact_manager = artifact_manager
        self.logger = logger or get_logger(__name__)
        
        # Hyperparameters depuis config
        self.hyperparams = config.hyperparameters
    
    @property
    def name(self) -> str:
        return "section_7_6_rl_performance"
    
    def run(self) -> ValidationResult:
        """
        ExÃ©cute validation RL performance.
        
        Pipeline:
        1. Charger/crÃ©er cache baseline (universel)
        2. ExÃ©cuter simulation baseline
        3. EntraÃ®ner agent RL (ou charger checkpoint)
        4. ExÃ©cuter simulation RL
        5. Comparer performances
        6. Retourner rÃ©sultats
        """
        # MÃ©tier pur ici
        # NO I/O direct (dÃ©lÃ©guer Ã  artifact_manager)
        # NO logging setup (logger dÃ©jÃ  injectÃ©)
        # NO figure generation (dÃ©lÃ©guer Ã  reporting)
        
        pass  # Ã€ remplir avec logique mÃ©tier extraite
    
    # ========== CONTROLLERS (COPIER depuis ancien) ==========
    class BaselineController:
        """Controller fixed-time (60s GREEN / 60s RED)."""
        def __init__(self, scenario_type: str):
            # COPIER depuis ancien (lines 612-634)
            pass
    
    class RLController:
        """Controller RL avec agent PPO/DQN."""
        def __init__(self, model, env):
            # COPIER depuis ancien (lines 637-702)
            pass
    
    # ========== MÃ‰TIER PUR ==========
    def run_control_simulation(self, controller, scenario_path: Path, ...):
        """
        Simule trafic avec un controller (baseline ou RL).
        INNOVATION PRÃ‰SERVÃ‰E: State continuation (initial_state support).
        """
        # COPIER depuis ancien (lines 705-938)
        # MAIS: remplacer I/O par dÃ©lÃ©gation artifact_manager
        pass
    
    def evaluate_traffic_performance(self, states_history, scenario_type):
        """Calcule mÃ©triques performance (travel time, throughput, etc.)."""
        # COPIER depuis ancien (lines 941-1021)
        pass
    
    def train_rl_agent(self, scenario_type: str, total_timesteps: int, device: str):
        """
        EntraÃ®ne agent RL (PPO/DQN) sur scenario.
        INNOVATION PRÃ‰SERVÃ‰E: Checkpoint system (dÃ©lÃ©guÃ© Ã  artifact_manager).
        """
        # COPIER depuis ancien (lines 1024-1283)
        # MAIS: remplacer checkpoint logic par artifact_manager.save_checkpoint()
        pass
    
    def run_performance_comparison(self, scenario_type: str, device: str):
        """Compare baseline vs RL performance."""
        # COPIER depuis ancien (lines 1286-1468)
        pass
```

## 3. DÃ‰LÃ‰GATIONS Ã€ FAIRE

**Checkpoint loading**:
```python
# âŒ ANCIEN (lines 1100-1150)
checkpoint_path = self._get_checkpoint_dir() / f"{scenario}_checkpoint_{config_hash}_{steps}_steps.zip"
if checkpoint_path.exists():
    model = DQN.load(checkpoint_path)

# âœ… NOUVEAU
checkpoint_path = self.artifact_manager.load_checkpoint(scenario, config_hash)
if checkpoint_path:
    model = DQN.load(checkpoint_path)
```

**Cache loading**:
```python
# âŒ ANCIEN (lines 388-416)
cached_states = self._load_baseline_cache(scenario, scenario_path, duration)

# âœ… NOUVEAU
cached_states = self.artifact_manager.load_baseline_cache(scenario, duration)
```

**Logging**:
```python
# âŒ ANCIEN (line 1200)
self.debug_logger.info(f"[CHECKPOINT] Loaded from {path}")

# âœ… NOUVEAU
self.logger.info(f"{DEBUG_CHECKPOINT} Loaded from {path}")
```

# CONTRAINTES
- MÃ‰TIER PUR uniquement (pas d'I/O direct)
- Dependency Injection (artifact_manager, logger passÃ©s en __init__)
- Type hints partout
- Docstrings DÃ‰TAILLÃ‰ES (expliquer les innovations prÃ©servÃ©es)
- Target: ~400 lignes (vs 1876 avant)

# GÃ‰NÃˆRE LE CODE
Extrais la logique mÃ©tier depuis l'ancien fichier et crÃ©e le nouveau.
```

**RÃ©sultat attendu**: Fichier `domain/section_7_6_rl_performance.py` (~400 lignes mÃ©tier pur)

---

**Validation Phase 3**:
```python
# Test d'instanciation
from validation_ch7_v2.scripts.domain.section_7_6_rl_performance import RLPerformanceTest
from validation_ch7_v2.scripts.infrastructure.artifact_manager import ArtifactManager
from validation_ch7_v2.scripts.infrastructure.config import SectionConfig

config = SectionConfig(name="section_7_6_rl_performance", ...)
artifact_mgr = ArtifactManager(Path("validation_ch7_v2/"))
test = RLPerformanceTest(config, artifact_mgr)

# â†’ Pas d'erreur d'instantiation
# â†’ test.run() retourne ValidationResult
```

---

### Phase 4: Orchestration Layer (3 heures)
**Objectif**: Orchestrer l'exÃ©cution des tests

#### ğŸ¤– PROMPT COPILOT #4.1: Test Factory

```markdown
# CONTEXTE
Pattern Factory pour crÃ©er les tests selon la configuration.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/orchestration/test_factory.py` avec:

```python
class TestFactory:
    @staticmethod
    def create(section_config: SectionConfig, artifact_manager: ArtifactManager) -> ValidationTest:
        """
        Factory pattern: CrÃ©er le bon test selon la config.
        
        Args:
            section_config: Configuration de la section
            artifact_manager: Manager d'artefacts (injectÃ©)
        
        Returns:
            Instance de ValidationTest concrÃ¨te
        
        Raises:
            ConfigError: Si section inconnue
        """
        if section_config.name == "section_7_3_analytical":
            from validation_ch7_v2.scripts.domain.section_7_3_analytical import AnalyticalTest
            return AnalyticalTest(section_config, artifact_manager)
        
        elif section_config.name == "section_7_6_rl_performance":
            from validation_ch7_v2.scripts.domain.section_7_6_rl_performance import RLPerformanceTest
            return RLPerformanceTest(section_config, artifact_manager)
        
        # ... autres sections
        
        else:
            raise ConfigError(f"Unknown section: {section_config.name}")
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `orchestration/test_factory.py` (~80 lignes)

---

#### ğŸ¤– PROMPT COPILOT #4.2: Validation Orchestrator

```markdown
# CONTEXTE
Orchestrator qui exÃ©cute tous les tests dans l'ordre.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/orchestration/validation_orchestrator.py` avec:

```python
class ValidationOrchestrator:
    """
    Orchestre l'exÃ©cution de tous les tests de validation.
    
    ResponsabilitÃ©s:
    - Charger les configurations des sections
    - CrÃ©er les tests via factory
    - ExÃ©cuter les tests dans l'ordre
    - AgrÃ©ger les rÃ©sultats
    - Logger le progrÃ¨s
    """
    
    def __init__(self, config_manager: ConfigManager, artifact_manager: ArtifactManager):
        self.config_manager = config_manager
        self.artifact_manager = artifact_manager
        self.logger = get_logger(__name__)
    
    def run_all_tests(self) -> List[ValidationResult]:
        """ExÃ©cute tous les tests (sections 7.3 Ã  7.7)."""
        pass
    
    def run_single_test(self, test: ValidationTest) -> ValidationResult:
        """
        Template method: Flux standard d'exÃ©cution.
        
        1. Log dÃ©but
        2. ExÃ©cuter test.run()
        3. Log fin
        4. Handle erreurs
        5. Retourner rÃ©sultat
        """
        pass
    
    def run_section(self, section_name: str) -> ValidationResult:
        """ExÃ©cute une section spÃ©cifique."""
        pass
```

# CONTRAINTES
- Template Method Pattern (flux standard)
- Error handling avec exceptions custom
- Logging dÃ©taillÃ© du progrÃ¨s
- Retour de rÃ©sultats agrÃ©gÃ©s

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `orchestration/validation_orchestrator.py` (~200 lignes)

---

**Validation Phase 4**:
```python
# Test orchestration
from validation_ch7_v2.scripts.orchestration.validation_orchestrator import ValidationOrchestrator
from validation_ch7_v2.scripts.infrastructure.config import ConfigManager
from validation_ch7_v2.scripts.infrastructure.artifact_manager import ArtifactManager

config_mgr = ConfigManager(Path("validation_ch7_v2/configs"))
artifact_mgr = ArtifactManager(Path("validation_ch7_v2/"))
orchestrator = ValidationOrchestrator(config_mgr, artifact_mgr)

# Test d'exÃ©cution d'une section
result = orchestrator.run_section("section_7_6_rl_performance")
print(f"Test passed: {result.passed}")
print(f"Metrics: {result.metrics}")
```

---

### Phase 5: Configuration YAML (2 heures)
**Objectif**: Externaliser toutes les configs

#### ğŸ¤– PROMPT COPILOT #5.1: Config YAML Section 7.6

```markdown
# TÃ‚CHE
CrÃ©er `validation_ch7_v2/configs/sections/section_7_6.yml` avec:

```yaml
# Section 7.6: RL Performance Validation
name: section_7_6_rl_performance
description: "Validation de la performance des agents RL vs baseline fixed-time"
revendication: "R5: Performance supÃ©rieure des agents RL dans le contexte bÃ©ninois"

# DurÃ©es
estimated_duration_minutes: 180  # 3h sur Kaggle GPU
quick_test_duration_minutes: 15  # 15min pour CI/CD

# HyperparamÃ¨tres RL (CODE_RL compatibility)
hyperparameters:
  # Configuration entraÃ®nement RL
  training:
    algorithm: "DQN"  # ou "PPO"
    episodes: 5000
    buffer_size: 50000
    learning_rate: 1e-3
    batch_size: 32
    tau: 1.0
    gamma: 0.99
    train_freq: 4
    gradient_steps: 1
    target_update_interval: 1000
    exploration_fraction: 0.1
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.05
  
  # Mode quick test (CI/CD)
  quick_test:
    episodes: 100
    duration_per_episode: 120  # 2 minutes
    control_interval: 15
  
  # Mode full test (Kaggle GPU)
  full_test:
    episodes: 5000
    duration_per_episode: 3600  # 1 heure
    control_interval: 15
  
  # ScÃ©narios disponibles
  scenarios:
    - traffic_light_control
    - ramp_metering
    - adaptive_speed_control
  
  # Baseline parameters
  baseline:
    green_duration: 60  # seconds
    red_duration: 60    # seconds

# Chemins relatifs
output_subdir: "section_7_6_rl_performance"
```

# GÃ‰NÃˆRE LE YAML
```

**RÃ©sultat attendu**: Fichier `configs/sections/section_7_6.yml` (~80 lignes)

---

### Phase 6: Reporting Layer (3 heures)
**Objectif**: GÃ©nÃ©ration LaTeX automatisÃ©e

#### ğŸ¤– PROMPT COPILOT #6.1: LaTeX Generator

```markdown
# CONTEXTE
Ancien systÃ¨me: gÃ©nÃ©ration LaTeX mÃ©langÃ©e dans les tests.
Nouveau systÃ¨me: reporting sÃ©parÃ©, rÃ©utilisable.

# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/reporting/latex_generator.py` avec:

```python
class LatexGenerator:
    """GÃ©nÃ¨re du contenu LaTeX depuis templates et donnÃ©es."""
    
    def __init__(self, template_dir: Path):
        self.template_dir = template_dir
    
    def generate_section(
        self,
        section_name: str,
        metrics: Dict[str, float],
        artifacts: Dict[str, Path]
    ) -> str:
        """
        GÃ©nÃ¨re contenu LaTeX pour une section.
        
        1. Charge template (e.g., templates/section_7_6.tex)
        2. Remplit placeholders avec metrics
        3. Retourne LaTeX compilÃ©
        """
        pass
    
    def plot_before_after(
        self,
        baseline_trajectory: np.ndarray,
        rl_trajectory: np.ndarray,
        output_path: Path
    ):
        """GÃ©nÃ¨re figure before/after (baseline vs RL)."""
        pass
    
    def plot_learning_curve(
        self,
        rewards: List[float],
        output_path: Path
    ):
        """GÃ©nÃ¨re courbe d'apprentissage RL."""
        pass
```

# CONTRAINTES
- Utiliser matplotlib pour figures
- Templates avec placeholders {variable_name}
- Type hints partout
- Sauvegarder figures en PNG + PDF

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `reporting/latex_generator.py` (~250 lignes)

---

### Phase 7: Entry Points (2 heures)
**Objectif**: CLI, Kaggle manager, local runner

#### ğŸ¤– PROMPT COPILOT #7.1: CLI Principal

```markdown
# TÃ‚CHE
CrÃ©er `validation_ch7_v2/scripts/entry_points/cli.py` avec:

```python
def main():
    """CLI principal pour validation CH7."""
    parser = argparse.ArgumentParser(
        description="Validation Chapitre 7 - ARZ-RL System"
    )
    parser.add_argument(
        "--section",
        choices=["section_7_3_analytical", "section_7_4_calibration", ...],
        help="Section Ã  exÃ©cuter (ou 'all')"
    )
    parser.add_argument("--quick-test", action="store_true", help="Mode rapide")
    parser.add_argument("--scenario", help="ScÃ©nario spÃ©cifique (section 7.6)")
    parser.add_argument("--device", choices=["cpu", "gpu"], default="cpu")
    
    args = parser.parse_args()
    
    # Setup
    config_mgr = ConfigManager(Path("validation_ch7_v2/configs"))
    artifact_mgr = ArtifactManager(Path("validation_ch7_v2/"))
    orchestrator = ValidationOrchestrator(config_mgr, artifact_mgr)
    
    # ExÃ©cution
    if args.section == "all":
        results = orchestrator.run_all_tests()
    else:
        result = orchestrator.run_section(args.section)
    
    # Reporting
    print_results(results)
```

# GÃ‰NÃˆRE LE CODE
```

**RÃ©sultat attendu**: Fichier `entry_points/cli.py` (~150 lignes)

---

## ğŸ¯ VALIDATION FINALE DU SYSTÃˆME

### Test End-to-End

```bash
# Test complet section 7.6 (mode rapide)
cd validation_ch7_v2/
python -m scripts.entry_points.cli --section section_7_6_rl_performance --quick-test --device cpu

# VÃ©rifications:
# 1. âœ“ Cache baseline crÃ©Ã© (validation_ch7_v2/cache/section_7_6/traffic_light_control_baseline_cache.pkl)
# 2. âœ“ Checkpoint RL crÃ©Ã© (validation_ch7_v2/checkpoints/section_7_6/traffic_light_control_checkpoint_abc12345_100_steps.zip)
# 3. âœ“ Figures gÃ©nÃ©rÃ©es (validation_ch7_v2/outputs/section_7_6_rl_performance/figures/before_after.png)
# 4. âœ“ MÃ©triques JSON (validation_ch7_v2/outputs/section_7_6_rl_performance/data/metrics/performance.json)
# 5. âœ“ Session summary (validation_ch7_v2/outputs/section_7_6_rl_performance/session_summary.json)
# 6. âœ“ LaTeX gÃ©nÃ©rÃ© (validation_ch7_v2/outputs/section_7_6_rl_performance/latex/section_7_6_content.tex)
```

### Checklist de PrÃ©servation des Innovations

```
â˜ Innovation #1: Cache additif intelligent
  âœ“ Extension 600s â†’ 3600s SANS recalcul complet
  âœ“ Reprise depuis cached_states[-1]
  âœ“ Ã‰conomie 85% du temps

â˜ Innovation #2: Config-hashing MD5
  âœ“ Validation checkpoint â†” config
  âœ“ Archivage automatique si mismatch
  âœ“ TraÃ§abilitÃ© complÃ¨te

â˜ Innovation #3: Controller autonome
  âœ“ State tracking interne
  âœ“ Reprise possible (controller.time_step = cached_duration)

â˜ Innovation #4: Dual cache system
  âœ“ Baseline universel (PAS de config_hash)
  âœ“ RL config-specific (AVEC config_hash)

â˜ Innovation #5: Checkpoint rotation
  âœ“ Sauvegarde tous les N steps
  âœ“ Rotation automatique (garder 3 derniers)

â˜ Innovation #6: Templates LaTeX
  âœ“ Placeholders {variable_name}
  âœ“ RÃ©utilisables

â˜ Innovation #7: Session tracking
  âœ“ session_summary.json avec artifact counting
```

### MÃ©triques de SuccÃ¨s

```
| MÃ©trique | Avant (validation_ch7) | AprÃ¨s (validation_ch7_v2) | Objectif | Status |
|----------|------------------------|---------------------------|----------|--------|
| Lignes domain test (7.6) | 1876 | ~400 | <500 | âœ“ |
| Temps ajout section 7.8 | ~4h (4 fichiers modifiÃ©s) | <30min (2 fichiers crÃ©Ã©s) | <30min | âœ“ |
| TestabilitÃ© (mocks) | Impossible | 100% mockable | 100% | âœ“ |
| Couverture tests unitaires | 0% | >80% | >80% | âœ“ |
| Duplication code (logging, cache) | 5x rÃ©pÃ©tÃ© | 1x centralisÃ© | 1x | âœ“ |
```

---

## ğŸ“š DOCUMENTATION COMPLÃ‰MENTAIRE

### README.md du nouveau systÃ¨me

```markdown
# Validation CH7 V2 - Architecture Clean

## DiffÃ©rences avec l'ancien systÃ¨me

| Aspect | Ancien (validation_ch7) | Nouveau (validation_ch7_v2) |
|--------|-------------------------|------------------------------|
| Architecture | Monolithique (1876 lignes test) | Layered (domain, infra, orchestration) |
| TestabilitÃ© | Impossible | 100% mockable |
| Configuration | HardcodÃ©e | YAML externalisÃ©e |
| Logging | RÃ©pÃ©tÃ© 5x | CentralisÃ© |
| Cache/Checkpoints | MÃ©langÃ© dans tests | ArtifactManager dÃ©diÃ© |
| Ajout section 7.8 | 4 fichiers modifiÃ©s | 2 fichiers crÃ©Ã©s |

## Innovations prÃ©servÃ©es

âœ… Cache additif intelligent (Ã©conomie 85%)
âœ… Config-hashing MD5 (validation checkpoint)
âœ… Controller autonome (state tracking)
âœ… Dual cache system (baseline + RL)
âœ… Checkpoint rotation automatique
âœ… Templates LaTeX rÃ©utilisables
âœ… Session tracking JSON

## Usage

```bash
# Test section 7.6 (mode rapide)
python -m scripts.entry_points.cli --section section_7_6_rl_performance --quick-test

# Test complet (Kaggle GPU)
python -m scripts.entry_points.cli --section section_7_6_rl_performance --device gpu

# Tous les tests
python -m scripts.entry_points.cli --section all
```

## Architecture

```
validation_ch7_v2/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ entry_points/      â† Layer 0: CLI, Kaggle
â”‚   â”œâ”€â”€ orchestration/     â† Layer 1: Orchestration
â”‚   â”œâ”€â”€ domain/            â† Layer 2: MÃ©tier pur
â”‚   â”œâ”€â”€ infrastructure/    â† Layer 3: I/O, caching, logging
â”‚   â””â”€â”€ reporting/         â† Sub-layer: LaTeX, figures
â”œâ”€â”€ configs/               â† YAML configs
â”œâ”€â”€ templates/             â† LaTeX templates
â”œâ”€â”€ tests/                 â† Tests unitaires (NOUVEAU!)
â”œâ”€â”€ checkpoints/           â† Checkpoints RL
â””â”€â”€ cache/                 â† Cache simulation
```
```

---

## ğŸš¦ PLAN D'ACTION IMMÃ‰DIAT

### Ã‰tape 1: CrÃ©er la structure (30 min)
```bash
cd "d:\Projets\Alibi\Code project"
mkdir -p validation_ch7_v2/scripts/{entry_points,orchestration,domain,infrastructure,reporting}
mkdir -p validation_ch7_v2/{configs/sections,templates,tests,checkpoints,cache}
touch validation_ch7_v2/README.md
# CrÃ©er tous les __init__.py
```

### Ã‰tape 2: GÃ©nÃ©rer code via Copilot (8-12 heures)
- Utiliser les prompts ci-dessus dans l'ordre
- Phase 1 â†’ Phase 2 â†’ Phase 3 (critique!) â†’ Phase 4 â†’ Phase 5 â†’ Phase 6 â†’ Phase 7
- Valider chaque phase avant de passer Ã  la suivante

### Ã‰tape 3: Tests unitaires (4 heures)
- CrÃ©er tests pour infrastructure (artifact_manager, config, logger)
- CrÃ©er tests pour domain (mocking simulator/model)
- CrÃ©er tests d'intÃ©gration (workflow complet)

### Ã‰tape 4: Migration progressive (variable)
- Comparer rÃ©sultats ancien vs nouveau
- VÃ©rifier mÃ©triques identiques
- Basculer progressivement

---

## âš ï¸ POINTS D'ATTENTION CRITIQUES

### 1. PrÃ©servation du Cache Additif (Innovation #1)
Le cache additif est l'innovation la plus complexe. VÃ©rifier:
- `extend_baseline_cache()` reprend bien depuis `cached_states[-1]`
- Simulation de l'extension UNIQUEMENT (pas tout refaire)
- Validation de cohÃ©rence (mass conservation)

### 2. Config-Hashing MD5 (Innovation #2)
VÃ©rifier:
- Hash calculÃ© AVANT checkpoint save
- Validation hash AVANT checkpoint load
- Archivage automatique si mismatch

### 3. Controller State Tracking (Innovation #3)
VÃ©rifier:
- `controller.time_step` incrÃ©mentÃ© correctement
- Reprise possible (`controller.time_step = cached_duration`)

### 4. Dual Cache System (Innovation #4)
VÃ©rifier:
- Baseline cache: SANS config_hash (universel)
- RL cache: AVEC config_hash (config-specific)
- Pas de confusion entre les deux

---

**FIN DU PLAN DE REFACTORISATION**
**Avec respect pour le systÃ¨me existant et confiance dans le nouveau.**
