#!/usr/bin/env python3
"""
OSM Metadata Enrichment Tool for Road Network Corridors
========================================================

Enrichit un fichier Excel de segments routiers (u, v, ...) avec des m√©tadonn√©es
OpenStreetMap pour chaque n≈ìud : feux de signalisation, types de jonction,
phases de signalisation, croisements, etc.

Usage:
    python enrich_osm_metadata.py <input_file.xlsx> [output_file.xlsx]

Arguments:
    input_file.xlsx   : Fichier Excel avec colonnes 'u' et 'v' (OSM node IDs)
    output_file.xlsx  : Fichier de sortie (par d√©faut: input_enriched.xlsx)

Format d'entr√©e requis:
    - Colonnes obligatoires: 'u', 'v' (node IDs entiers ou n√©gatifs)
    - Colonnes optionnelles: name_clean, highway, length, oneway, etc.

Format de sortie:
    - Colonnes originales +
    - Pour chaque n≈ìud (u/v):
        * {prefix}_has_signal        : bool - Pr√©sence de feu tricolore
        * {prefix}_junction_type     : str  - Type de jonction (si d√©fini)
        * {prefix}_signal_tags       : str  - Toutes les tags traffic_signals:*
        * {prefix}_signal_phases     : str  - Nombre/config de phases
        * {prefix}_signal_controller : str  - Type de contr√¥leur
        * {prefix}_crossing_type     : str  - Type de passage pi√©ton
        * {prefix}_traffic_calming   : str  - Dispositifs d'apaisement
        * {prefix}_highway_tag       : str  - Tag highway du n≈ìud
        * {prefix}_lat               : float - Latitude
        * {prefix}_lon               : float - Longitude

Auteur: Alibi Traffic Analysis Team
Date: 2025-11-18
"""

import sys
import argparse
from pathlib import Path
import time
import pandas as pd
import requests


def fetch_osm_nodes(node_ids, chunk_size=50, timeout=120, verbose=True):
    """
    R√©cup√®re les m√©tadonn√©es OSM pour une liste de node IDs via Overpass API.
    
    Args:
        node_ids: Liste de node IDs OSM (entiers positifs)
        chunk_size: Taille des batches pour √©viter timeouts
        timeout: Timeout pour chaque requ√™te HTTP
        verbose: Afficher progression
    
    Returns:
        dict: {node_id: {'lat': float, 'lon': float, 'tags': dict}}
    """
    url = "https://overpass-api.de/api/interpreter"
    node_data = {}
    
    # Filtrer les IDs positifs seulement
    valid_ids = sorted([n for n in node_ids if n > 0])
    
    if verbose:
        print(f"üì° R√©cup√©ration des m√©tadonn√©es pour {len(valid_ids)} n≈ìuds OSM...")
    
    for i in range(0, len(valid_ids), chunk_size):
        chunk = valid_ids[i:i+chunk_size]
        ids_str = ",".join(str(n) for n in chunk)
        query = f"[out:json][timeout:{timeout}];node(id:{ids_str});out body;"
        
        try:
            resp = requests.get(url, params={'data': query}, timeout=timeout)
            resp.raise_for_status()
            data = resp.json()
            
            for element in data.get('elements', []):
                node_data[element['id']] = {
                    'lat': element.get('lat'),
                    'lon': element.get('lon'),
                    'tags': element.get('tags', {})
                }
            
            if verbose and (i // chunk_size + 1) % 5 == 0:
                print(f"  ‚úì {min(i + chunk_size, len(valid_ids))}/{len(valid_ids)} n≈ìuds trait√©s")
            
            # Pause courtoisie pour Overpass API
            time.sleep(0.5)
            
        except requests.exceptions.RequestException as e:
            print(f"  ‚ö†Ô∏è  Erreur requ√™te batch {i//chunk_size + 1}: {e}")
            continue
    
    if verbose:
        missing = set(valid_ids) - set(node_data.keys())
        if missing:
            print(f"  ‚ö†Ô∏è  {len(missing)} n≈ìuds non trouv√©s dans OSM (supprim√©s ou invisibles)")
        print(f"‚úÖ M√©tadonn√©es r√©cup√©r√©es pour {len(node_data)} n≈ìuds\n")
    
    return node_data


def extract_node_metadata(node_id, node_data):
    """
    Extrait les m√©tadonn√©es structur√©es pour un n≈ìud OSM.
    
    Args:
        node_id: ID du n≈ìud (peut √™tre n√©gatif = virtuel)
        node_data: dict retourn√© par fetch_osm_nodes
    
    Returns:
        tuple: (has_signal, junction_type, signal_tags, signal_phases,
                signal_controller, crossing_type, traffic_calming,
                highway_tag, lat, lon)
    """
    # N≈ìuds virtuels (n√©gatifs) ou absents
    if node_id <= 0 or int(node_id) not in node_data:
        return (False, None, None, None, None, None, None, None, None, None)
    
    info = node_data[int(node_id)]
    tags = info.get('tags', {})
    
    # D√©tection feu tricolore
    has_signal = (
        tags.get('highway') == 'traffic_signals' or
        bool(tags.get('traffic_signals')) or
        any(k.startswith('traffic_signals:') for k in tags)
    )
    
    # Type de jonction
    junction = tags.get('junction')
    
    # Tags de signalisation (format cl√©=valeur)
    signal_tags_list = [
        f"{k}={v}" for k, v in tags.items()
        if k.startswith('traffic_signals')
    ]
    signal_tags = "; ".join(signal_tags_list) if signal_tags_list else None
    
    # Phases de signalisation
    signal_phases = (
        tags.get('traffic_signals:phases') or
        tags.get('traffic_signals:multiphase')
    )
    
    # Type de contr√¥leur
    signal_controller = (
        tags.get('traffic_signals:direction') or
        tags.get('traffic_signals:control')
    )
    
    # Type de passage pi√©ton
    crossing = tags.get('crossing')
    
    # Dispositifs d'apaisement
    traffic_calming = tags.get('traffic_calming')
    
    # Tag highway du n≈ìud
    highway_tag = tags.get('highway')
    
    return (
        has_signal,
        junction,
        signal_tags,
        signal_phases,
        signal_controller,
        crossing,
        traffic_calming,
        highway_tag,
        info.get('lat'),
        info.get('lon')
    )


def enrich_corridor_data(input_path, output_path=None, verbose=True):
    """
    Pipeline complet d'enrichissement d'un fichier corridor.
    
    Args:
        input_path: Chemin vers fichier Excel d'entr√©e
        output_path: Chemin vers fichier Excel de sortie (auto si None)
        verbose: Afficher progression
    
    Returns:
        Path: Chemin du fichier enrichi
    """
    input_path = Path(input_path)
    
    if output_path is None:
        output_path = input_path.parent / f"{input_path.stem}_enriched{input_path.suffix}"
    else:
        output_path = Path(output_path)
    
    if verbose:
        print(f"\n{'='*70}")
        print(f"üöÄ ENRICHISSEMENT OSM METADATA")
        print(f"{'='*70}")
        print(f"üìÇ Fichier d'entr√©e : {input_path}")
        print(f"üìÇ Fichier de sortie: {output_path}\n")
    
    # Chargement donn√©es
    if verbose:
        print("üìñ Chargement du fichier corridor...")
    
    try:
        df = pd.read_excel(input_path)
    except Exception as e:
        print(f"‚ùå Erreur lecture fichier: {e}")
        sys.exit(1)
    
    # Validation colonnes
    required_cols = ['u', 'v']
    missing = set(required_cols) - set(df.columns)
    if missing:
        print(f"‚ùå Colonnes manquantes: {missing}")
        print(f"   Colonnes pr√©sentes: {df.columns.tolist()}")
        sys.exit(1)
    
    if verbose:
        print(f"‚úÖ {len(df)} segments charg√©s")
        print(f"   Colonnes: {df.columns.tolist()}\n")
    
    # Extraction node IDs uniques
    raw_nodes = set(df['u'].dropna().astype(int)).union(set(df['v'].dropna().astype(int)))
    node_ids = sorted([n for n in raw_nodes if n > 0])
    
    if verbose:
        print(f"üîç N≈ìuds uniques d√©tect√©s: {len(raw_nodes)} total, {len(node_ids)} positifs (OSM)")
        negative_count = len(raw_nodes) - len(node_ids)
        if negative_count > 0:
            print(f"   ‚ÑπÔ∏è  {negative_count} n≈ìuds virtuels (ID < 0) seront marqu√©s sans m√©tadonn√©es\n")
    
    # R√©cup√©ration m√©tadonn√©es OSM
    node_data = fetch_osm_nodes(node_ids, verbose=verbose)
    
    # Extraction m√©tadonn√©es pour chaque n≈ìud
    if verbose:
        print("üîß Extraction des m√©tadonn√©es structur√©es...")
    
    u_metadata = df['u'].apply(lambda x: extract_node_metadata(int(x), node_data))
    v_metadata = df['v'].apply(lambda x: extract_node_metadata(int(x), node_data))
    
    # Unpacking en colonnes
    u_cols = list(zip(*u_metadata))
    v_cols = list(zip(*v_metadata))
    
    col_names = [
        'has_signal', 'junction_type', 'signal_tags', 'signal_phases',
        'signal_controller', 'crossing_type', 'traffic_calming',
        'highway_tag', 'lat', 'lon'
    ]
    
    for i, name in enumerate(col_names):
        df[f'u_{name}'] = u_cols[i]
        df[f'v_{name}'] = v_cols[i]
    
    if verbose:
        print(f"‚úÖ {len(col_names)*2} nouvelles colonnes ajout√©es\n")
    
    # Statistiques
    if verbose:
        u_signals = df['u_has_signal'].sum()
        v_signals = df['v_has_signal'].sum()
        unique_signal_nodes = len(
            set(df.loc[df['u_has_signal'], 'u']).union(
                set(df.loc[df['v_has_signal'], 'v'])
            )
        )
        
        print(f"üìä Statistiques:")
        print(f"   ‚Ä¢ Segments avec feux √† l'origine (u): {u_signals}")
        print(f"   ‚Ä¢ Segments avec feux √† la destination (v): {v_signals}")
        print(f"   ‚Ä¢ N≈ìuds uniques avec feux: {unique_signal_nodes}")
        
        # Junctions
        u_junctions = df['u_junction_type'].dropna().nunique()
        v_junctions = df['v_junction_type'].dropna().nunique()
        if u_junctions + v_junctions > 0:
            print(f"   ‚Ä¢ Types de jonctions trouv√©s: {u_junctions + v_junctions}")
        
        # Phases
        u_phases = df['u_signal_phases'].dropna().count()
        v_phases = df['v_signal_phases'].dropna().count()
        if u_phases + v_phases > 0:
            print(f"   ‚Ä¢ N≈ìuds avec info phases: {u_phases + v_phases}")
        print()
    
    # Sauvegarde
    if verbose:
        print(f"üíæ Sauvegarde du fichier enrichi...")
    
    try:
        df.to_excel(output_path, index=False)
        if verbose:
            print(f"‚úÖ Fichier sauvegard√©: {output_path}")
            print(f"   Taille: {output_path.stat().st_size / 1024:.1f} KB\n")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde: {e}")
        sys.exit(1)
    
    if verbose:
        print(f"{'='*70}")
        print(f"‚úÖ ENRICHISSEMENT TERMIN√â AVEC SUCC√àS")
        print(f"{'='*70}\n")
    
    return output_path


def main():
    """Point d'entr√©e CLI."""
    parser = argparse.ArgumentParser(
        description="Enrichit un fichier corridor avec m√©tadonn√©es OSM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
    python enrich_osm_metadata.py input/corridor.xlsx
    python enrich_osm_metadata.py input/data.xlsx output/enriched.xlsx
    python enrich_osm_metadata.py corridor.xlsx --quiet

Format d'entr√©e requis:
    ‚Ä¢ Colonnes 'u' et 'v' avec OSM node IDs (entiers)
    ‚Ä¢ Format Excel (.xlsx)
        """
    )
    
    parser.add_argument(
        'input_file',
        type=str,
        help="Fichier Excel d'entr√©e (avec colonnes u, v)"
    )
    
    parser.add_argument(
        'output_file',
        type=str,
        nargs='?',
        default=None,
        help="Fichier Excel de sortie (optionnel, auto-nomm√© si absent)"
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help="Mode silencieux (pas de logs)"
    )
    
    args = parser.parse_args()
    
    # Validation existence fichier
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"‚ùå Fichier introuvable: {input_path}")
        sys.exit(1)
    
    # Ex√©cution
    try:
        enrich_corridor_data(
            input_path,
            args.output_file,
            verbose=not args.quiet
        )
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interruption utilisateur - arr√™t du programme")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
