import json
import re

# Read complete JSON array
with open('validation_output/results/elonmj_arz-validation-76rlperformance-peum/arz-validation-76rlperformance-peum.log', 'r', encoding='utf-8') as f:
    content = f.read()
    log_lines = json.loads(content)

# Extract all text data
full_text = ''.join([line['data'] for line in log_lines if line.get('stream_name') == 'stdout'])

print(f'üìä Total log size: {len(full_text)} characters\n')
print('='*80)

# Find BASELINE rewards
baseline_rewards = re.findall(r'\[BASELINE\].*?reward[=:]?\s*([-+]?\d+\.\d+)', full_text, re.IGNORECASE)
print('\nüîµ BASELINE REWARDS')
print('='*80)
print(f'Count: {len(baseline_rewards)}')
if baseline_rewards:
    rewards_float = [float(r) for r in baseline_rewards]
    print(f'Total: {sum(rewards_float):.4f}')
    print(f'Mean: {sum(rewards_float)/len(rewards_float):.4f}')
    print(f'Min: {min(rewards_float):.4f}')
    print(f'Max: {max(rewards_float):.4f}')
    print(f'\nFirst 10: {[f"{r:.4f}" for r in rewards_float[:10]]}')
else:
    print('‚ùå No BASELINE rewards found - BUG NOT FIXED!')

# Find RL rewards
rl_rewards = re.findall(r'\[RL\].*?reward[=:]?\s*([-+]?\d+\.\d+)', full_text, re.IGNORECASE)
print('\nüü¢ RL REWARDS')
print('='*80)
print(f'Count: {len(rl_rewards)}')
if rl_rewards:
    rewards_float = [float(r) for r in rl_rewards]
    print(f'Total: {sum(rewards_float):.4f}')
    print(f'Mean: {sum(rewards_float)/len(rewards_float):.4f}')
    print(f'Min: {min(rewards_float):.4f}')
    print(f'Max: {max(rewards_float):.4f}')
    print(f'\nFirst 10: {[f"{r:.4f}" for r in rewards_float[:10]]}')
    print(f'Last 10: {[f"{r:.4f}" for r in rewards_float[-10:]]}')
    
    # Check learning trend
    if len(rewards_float) >= 20:
        first_half = sum(rewards_float[:len(rewards_float)//2]) / (len(rewards_float)//2)
        second_half = sum(rewards_float[len(rewards_float)//2:]) / (len(rewards_float) - len(rewards_float)//2)
        improvement = ((second_half - first_half) / abs(first_half)) * 100 if first_half != 0 else 0
        print(f'\nüìà LEARNING TREND:')
        print(f'  First half mean: {first_half:.4f}')
        print(f'  Second half mean: {second_half:.4f}')
        print(f'  Improvement: {improvement:+.2f}%')
        
        if improvement > 5:
            print(f'  ‚úÖ POSITIVE LEARNING DETECTED!')
        elif improvement < -5:
            print(f'  ‚ö†Ô∏è  NEGATIVE TREND - Investigation needed')
        else:
            print(f'  ‚ö° STABLE - May need more training steps')
else:
    print('‚ùå No RL rewards found - SIMULATION CRASHED!')

# Find completion summaries
print('\nüèÅ SIMULATION COMPLETION SUMMARIES')
print('='*80)

baseline_completed = re.search(r'\[BASELINE\]\s*\[SIMULATION COMPLETED\](.*?)(?=\[(?:RL|BASELINE)|$)', full_text, re.DOTALL)
if baseline_completed:
    print('\nüîµ BASELINE COMPLETION:')
    print(baseline_completed.group(1)[:800].strip())
else:
    print('‚ùå No BASELINE completion found')

rl_completed = re.search(r'\[RL\]\s*\[SIMULATION COMPLETED\](.*?)(?=\[(?:RL|BASELINE)|$)', full_text, re.DOTALL)
if rl_completed:
    print('\nüü¢ RL COMPLETION:')
    print(rl_completed.group(1)[:800].strip())
else:
    print('‚ùå No RL completion found - CRASHED!')

# Check for errors
print('\nüîç ERROR DETECTION')
print('='*80)
errors = re.findall(r'(AttributeError.*?mean\(\)|UnboundLocalError.*?traceback)', full_text, re.IGNORECASE)
if errors:
    print(f'‚ùå Found {len(errors)} errors:')
    for i, err in enumerate(errors[:3], 1):
        print(f'\n  Error {i}: {err[:200]}...')
else:
    print('‚úÖ No critical errors detected')

# Compare performance
print('\n‚öñÔ∏è  PERFORMANCE COMPARISON')
print('='*80)
if baseline_rewards and rl_rewards:
    baseline_total = sum([float(r) for r in baseline_rewards])
    rl_total = sum([float(r) for r in rl_rewards])
    diff = rl_total - baseline_total
    pct = (diff / abs(baseline_total)) * 100 if baseline_total != 0 else 0
    
    print(f'BASELINE Total: {baseline_total:.4f}')
    print(f'RL Total:       {rl_total:.4f}')
    print(f'Difference:     {diff:+.4f} ({pct:+.2f}%)')
    print()
    
    if diff > 0:
        print('üéØ RL OUTPERFORMS BASELINE!')
        print('‚úÖ RECOMMENDATION: Run full training (5000 timesteps)')
    elif diff > -baseline_total * 0.1:  # Within 10%
        print('‚ö° RL COMPARABLE TO BASELINE')
        print('üîÑ RECOMMENDATION: Try longer training to see learning trend')
    else:
        print('‚ö†Ô∏è  RL UNDERPERFORMS BASELINE')
        print('üîç RECOMMENDATION: Check hyperparameters and reward function')
elif not rl_rewards:
    print('‚ùå CANNOT COMPARE - RL simulation crashed!')
    print('üîß BUGS NEED TO BE FIXED FIRST')
else:
    print('‚ö†Ô∏è  Incomplete data for comparison')

print('\n' + '='*80)
print('ANALYSIS COMPLETE')
print('='*80)
