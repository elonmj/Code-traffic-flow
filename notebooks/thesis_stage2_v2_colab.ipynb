{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation des dÃ©pendances\n",
    "!pip install stable-baselines3 gymnasium numba numpy pandas matplotlib --quiet\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Clone du repo et setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone if needed\n",
    "if not os.path.exists('/content/Code-traffic-flow'):\n",
    "    !git clone https://github.com/elonmj/Code-traffic-flow.git\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    # Pull latest changes\n",
    "    %cd /content/Code-traffic-flow\n",
    "    !git pull origin main\n",
    "    print(\"âœ… Repository updated\")\n",
    "\n",
    "%cd /content/Code-traffic-flow\n",
    "sys.path.insert(0, '/content/Code-traffic-flow')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: VÃ©rification GPU\n",
    "import torch\n",
    "from numba import cuda\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Numba CUDA: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    print(f\"  Device: {cuda.get_current_device().name}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Imports\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from Code_RL.src.env.traffic_signal_env_direct_v3 import TrafficSignalEnvDirectV3\n",
    "from arz_model.config import create_victoria_island_config\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081af9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Configuration du scÃ©nario congestionnÃ©\n",
    "# ==========================================\n",
    "# PARAMÃˆTRES CLÃ‰S - SCÃ‰NARIO CONGESTIONNÃ‰\n",
    "# ==========================================\n",
    "\n",
    "TRAINING_TIMESTEPS = 5000  # 5k steps comme demandÃ©\n",
    "EVAL_EPISODES = 3\n",
    "\n",
    "# DensitÃ©s Ã©levÃ©es pour crÃ©er de la congestion\n",
    "DEFAULT_DENSITY = 120.0  # veh/km (Ã©tait 80)\n",
    "INFLOW_DENSITY = 180.0   # veh/km (Ã©tait 100)\n",
    "\n",
    "# Reward weights optimisÃ©s\n",
    "REWARD_WEIGHTS = {\n",
    "    'alpha': 5.0,   # PÃ©nalitÃ© densitÃ© (Ã©tait 1.0)\n",
    "    'kappa': 0.3,   # PÃ©nalitÃ© switch (Ã©tait 0.1)\n",
    "    'mu': 0.1       # RÃ©compense dÃ©bit (Ã©tait 0.5)\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCÃ‰NARIO CONGESTIONNÃ‰ (LOS D - Rush Hour)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"DensitÃ© initiale: {DEFAULT_DENSITY} veh/km\")\n",
    "print(f\"DensitÃ© d'entrÃ©e: {INFLOW_DENSITY} veh/km\")\n",
    "print(f\"Reward weights: {REWARD_WEIGHTS}\")\n",
    "print(f\"Training timesteps: {TRAINING_TIMESTEPS}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c25348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fonction de crÃ©ation d'environnement\n",
    "def create_congested_env(quiet=True):\n",
    "    \"\"\"CrÃ©e l'environnement avec scÃ©nario congestionnÃ©.\"\"\"\n",
    "    arz_config = create_victoria_island_config(\n",
    "        t_final=450.0,\n",
    "        output_dt=15.0,\n",
    "        cells_per_100m=4,\n",
    "        default_density=DEFAULT_DENSITY,\n",
    "        inflow_density=INFLOW_DENSITY,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    arz_config.rl_metadata = {\n",
    "        'observation_segment_ids': [s.id for s in arz_config.segments],\n",
    "        'decision_interval': 15.0,\n",
    "    }\n",
    "    \n",
    "    class SimpleConfig:\n",
    "        def __init__(self, arz_config):\n",
    "            self.arz_simulation_config = arz_config\n",
    "            self.rl_env_params = {\n",
    "                'dt_decision': 15.0,\n",
    "                'observation_segment_ids': None,\n",
    "                'reward_weights': REWARD_WEIGHTS\n",
    "            }\n",
    "    \n",
    "    rl_config = SimpleConfig(arz_config)\n",
    "    \n",
    "    env = TrafficSignalEnvDirectV3(\n",
    "        simulation_config=rl_config.arz_simulation_config,\n",
    "        decision_interval=rl_config.rl_env_params.get('dt_decision', 15.0),\n",
    "        observation_segment_ids=rl_config.rl_env_params.get('observation_segment_ids'),\n",
    "        reward_weights=rl_config.rl_env_params.get('reward_weights'),\n",
    "        quiet=quiet\n",
    "    )\n",
    "    return env\n",
    "\n",
    "print(\"âœ… Environment factory ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f787bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Callback pour tracking\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_ep_reward = 0.0\n",
    "        self.current_ep_length = 0\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        self.current_ep_reward += self.locals['rewards'][0]\n",
    "        self.current_ep_length += 1\n",
    "        \n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.episode_lengths.append(self.current_ep_length)\n",
    "            self.current_ep_reward = 0.0\n",
    "            self.current_ep_length = 0\n",
    "            \n",
    "            if len(self.episode_rewards) % 10 == 0:\n",
    "                avg_last_10 = np.mean(self.episode_rewards[-10:])\n",
    "                print(f\"  Episode {len(self.episode_rewards)}: Avg reward (last 10) = {avg_last_10:.2f}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"âœ… Callback ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3682711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Fonction d'Ã©valuation\n",
    "def evaluate_policy(env, policy_type='fixed_time', model=None, n_episodes=3, fixed_time_interval=30.0):\n",
    "    \"\"\"Ã‰value une politique sur plusieurs Ã©pisodes.\"\"\"\n",
    "    metrics = {'rewards': [], 'densities': [], 'throughputs': []}\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = truncated = False\n",
    "        ep_reward = 0.0\n",
    "        ep_densities = []\n",
    "        ep_throughputs = []\n",
    "        time_since_switch = 0.0\n",
    "        current_action = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            if policy_type == 'model':\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "            else:  # fixed_time\n",
    "                time_since_switch += env.decision_interval\n",
    "                if time_since_switch >= fixed_time_interval:\n",
    "                    action = 1  # Switch\n",
    "                    time_since_switch = 0.0\n",
    "                else:\n",
    "                    action = 0  # Keep\n",
    "            \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            \n",
    "            if 'avg_density' in info:\n",
    "                ep_densities.append(info['avg_density'])\n",
    "            if 'throughput' in info:\n",
    "                ep_throughputs.append(info['throughput'])\n",
    "                \n",
    "        metrics['rewards'].append(ep_reward)\n",
    "        metrics['densities'].append(np.mean(ep_densities) if ep_densities else 0.0)\n",
    "        metrics['throughputs'].append(np.sum(ep_throughputs) if ep_throughputs else 0.0)\n",
    "        \n",
    "        print(f\"  Episode {ep+1}: Reward={ep_reward:.2f}, Density={metrics['densities'][-1]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': float(np.mean(metrics['rewards'])),\n",
    "        'std_reward': float(np.std(metrics['rewards'])),\n",
    "        'mean_density': float(np.mean(metrics['densities'])),\n",
    "        'mean_throughput': float(np.mean(metrics['throughputs'])),\n",
    "        'all_rewards': [float(r) for r in metrics['rewards']]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c97b2",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š Part 1: Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd94406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Ã‰valuation Baseline\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 1: BASELINE (Fixed-Time 30s)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = create_congested_env(quiet=True)\n",
    "baseline_results = evaluate_policy(env, policy_type='fixed_time', n_episodes=EVAL_EPISODES)\n",
    "env.close()\n",
    "\n",
    "print(f\"\\nðŸ“Š Baseline Results:\")\n",
    "print(f\"   Mean Reward: {baseline_results['mean_reward']:.2f} Â± {baseline_results['std_reward']:.2f}\")\n",
    "print(f\"   Mean Density: {baseline_results['mean_density']:.4f}\")\n",
    "print(f\"   Mean Throughput: {baseline_results['mean_throughput']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0faa",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Part 2: DQN Training (5000 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: DQN Training\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: DQN TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = create_congested_env(quiet=True)\n",
    "\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=500,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    target_update_interval=250,\n",
    "    exploration_fraction=0.3,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callback = TrainingCallback()\n",
    "\n",
    "print(f\"\\nðŸš€ Starting training for {TRAINING_TIMESTEPS} timesteps...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model.learn(total_timesteps=TRAINING_TIMESTEPS, callback=callback)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ… Training completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"   Total episodes: {len(callback.episode_rewards)}\")\n",
    "print(f\"   Mean episode reward: {np.mean(callback.episode_rewards):.2f}\")\n",
    "print(f\"   Max episode reward: {np.max(callback.episode_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b110c26",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ˆ Part 3: RL Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a354a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: RL Evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 3: RL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rl_results = evaluate_policy(env, model=model, policy_type='model', n_episodes=EVAL_EPISODES)\n",
    "env.close()\n",
    "\n",
    "print(f\"\\nðŸ“Š RL Results:\")\n",
    "print(f\"   Mean Reward: {rl_results['mean_reward']:.2f} Â± {rl_results['std_reward']:.2f}\")\n",
    "print(f\"   Mean Density: {rl_results['mean_density']:.4f}\")\n",
    "print(f\"   Mean Throughput: {rl_results['mean_throughput']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee014ea3",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ† Part 4: Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Comparaison finale\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL COMPARISON: BASELINE vs RL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcul des amÃ©liorations\n",
    "improvement = {\n",
    "    'reward': (rl_results['mean_reward'] - baseline_results['mean_reward']) / abs(baseline_results['mean_reward']) * 100,\n",
    "    'density': (rl_results['mean_density'] - baseline_results['mean_density']) / baseline_results['mean_density'] * 100,\n",
    "    'throughput': (rl_results['mean_throughput'] - baseline_results['mean_throughput']) / baseline_results['mean_throughput'] * 100 if baseline_results['mean_throughput'] > 0 else 0\n",
    "}\n",
    "\n",
    "print(f\"\\n{'MÃ©trique':<20} {'Baseline':<15} {'RL (DQN)':<15} {'Gain':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Reward':<20} {baseline_results['mean_reward']:<15.2f} {rl_results['mean_reward']:<15.2f} {improvement['reward']:+.2f}%\")\n",
    "print(f\"{'DensitÃ© (veh/m)':<20} {baseline_results['mean_density']:<15.4f} {rl_results['mean_density']:<15.4f} {improvement['density']:+.2f}%\")\n",
    "print(f\"{'DÃ©bit (veh/s)':<20} {baseline_results['mean_throughput']:<15.2f} {rl_results['mean_throughput']:<15.2f} {improvement['throughput']:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if improvement['reward'] > 0:\n",
    "    print(f\"ðŸŽ‰ L'agent RL a amÃ©liorÃ© le reward de {improvement['reward']:+.2f}%!\")\n",
    "else:\n",
    "    print(f\"âš ï¸  L'agent RL a un reward infÃ©rieur de {abs(improvement['reward']):.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: GÃ©nÃ©ration des figures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Figure 1: Courbe d'apprentissage\n",
    "ax1 = axes[0]\n",
    "episodes = range(1, len(callback.episode_rewards) + 1)\n",
    "ax1.plot(episodes, callback.episode_rewards, 'b-', alpha=0.3, label='Episode reward')\n",
    "\n",
    "# Moving average\n",
    "window = min(10, len(callback.episode_rewards))\n",
    "if len(callback.episode_rewards) >= window:\n",
    "    moving_avg = np.convolve(callback.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window, len(callback.episode_rewards)+1), moving_avg, 'r-', linewidth=2, label=f'Moving avg ({window} ep)')\n",
    "\n",
    "ax1.axhline(y=baseline_results['mean_reward'], color='green', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Reward', fontsize=12)\n",
    "ax1.set_title('Courbe d\\'apprentissage DQN (ScÃ©nario CongestionnÃ©)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Figure 2: Comparaison\n",
    "ax2 = axes[1]\n",
    "metrics = ['Reward', 'DensitÃ©\\n(Ã—1000)', 'DÃ©bit\\n(Ã—100)']\n",
    "baseline_vals = [baseline_results['mean_reward'], baseline_results['mean_density']*1000, baseline_results['mean_throughput']/100]\n",
    "rl_vals = [rl_results['mean_reward'], rl_results['mean_density']*1000, rl_results['mean_throughput']/100]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, baseline_vals, width, label='Baseline (Fixed-Time)', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, rl_vals, width, label='RL (DQN)', color='coral')\n",
    "\n",
    "ax2.set_ylabel('Valeur', fontsize=12)\n",
    "ax2.set_title('Comparaison Baseline vs RL', fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add improvement annotations\n",
    "for i, (b, r) in enumerate(zip(baseline_vals, rl_vals)):\n",
    "    if i == 0:  # Reward\n",
    "        pct = improvement['reward']\n",
    "    elif i == 1:  # Density\n",
    "        pct = improvement['density']\n",
    "    else:  # Throughput\n",
    "        pct = improvement['throughput']\n",
    "    ax2.annotate(f'{pct:+.1f}%', xy=(x[i] + width/2, r), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/thesis_stage2_v2_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure saved to /content/thesis_stage2_v2_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Sauvegarde des rÃ©sultats JSON\n",
    "results = {\n",
    "    'config': {\n",
    "        'default_density': DEFAULT_DENSITY,\n",
    "        'inflow_density': INFLOW_DENSITY,\n",
    "        'reward_weights': REWARD_WEIGHTS,\n",
    "        'training_timesteps': TRAINING_TIMESTEPS,\n",
    "        'eval_episodes': EVAL_EPISODES\n",
    "    },\n",
    "    'baseline': baseline_results,\n",
    "    'rl_dqn': rl_results,\n",
    "    'improvement': improvement,\n",
    "    'training': {\n",
    "        'total_episodes': len(callback.episode_rewards),\n",
    "        'mean_reward': float(np.mean(callback.episode_rewards)),\n",
    "        'max_reward': float(np.max(callback.episode_rewards)),\n",
    "        'min_reward': float(np.min(callback.episode_rewards)),\n",
    "        'episode_rewards': [float(r) for r in callback.episode_rewards]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/content/thesis_stage2_v2_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ… Results saved to /content/thesis_stage2_v2_results.json\")\n",
    "print(\"\\nðŸ“‹ Summary JSON:\")\n",
    "print(json.dumps({\n",
    "    'baseline_reward': baseline_results['mean_reward'],\n",
    "    'rl_reward': rl_results['mean_reward'],\n",
    "    'improvement_pct': improvement['reward']\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Download les fichiers\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ðŸ“¥ Downloading results...\")\n",
    "files.download('/content/thesis_stage2_v2_results.png')\n",
    "files.download('/content/thesis_stage2_v2_results.json')\n",
    "print(\"\\nâœ… Download complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
