{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Setup et Installation\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "\n",
    "print(f\"Environment: {'Colab' if IN_COLAB else 'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive for saving results\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/elonmj/Code-traffic-flow.git 2>/dev/null || echo \"Repo already exists\"\n",
    "    %cd Code-traffic-flow\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q stable-baselines3 gymnasium numba cupy-cuda12x\n",
    "    \n",
    "    PROJECT_ROOT = '/content/Code-traffic-flow'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/thesis_rl_results'\n",
    "elif IN_KAGGLE:\n",
    "    !pip install -q stable-baselines3 gymnasium\n",
    "    PROJECT_ROOT = '/kaggle/input/code-traffic-flow'\n",
    "    OUTPUT_DIR = '/kaggle/working/thesis_rl_results'\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'results', 'thesis_rl_results')\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Imports et Configuration\n",
    "# =============================================================================\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# RL imports\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Project imports\n",
    "from Code_RL.src.env.traffic_signal_env_direct_v3 import TrafficSignalEnvDirectV3\n",
    "from arz_model.config import create_victoria_island_config\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    'timesteps': 5000,           # 5000 steps for quick demo\n",
    "    'eval_episodes': 5,          # Evaluation episodes\n",
    "    \n",
    "    # Traffic scenario (CONGESTED)\n",
    "    'default_density': 120.0,    # veh/km - Rush hour\n",
    "    'inflow_density': 180.0,     # veh/km - High inflow\n",
    "    't_final': 450.0,            # Simulation time (seconds)\n",
    "    'decision_interval': 15.0,   # Decision interval (seconds)\n",
    "    \n",
    "    # Reward weights (optimized for congestion)\n",
    "    'alpha': 5.0,   # Density penalty (HIGH)\n",
    "    'kappa': 0.3,   # Switch penalty\n",
    "    'mu': 0.1,      # Throughput reward (LOW)\n",
    "    \n",
    "    # DQN hyperparameters\n",
    "    'learning_rate': 1e-4,\n",
    "    'buffer_size': 10000,\n",
    "    'learning_starts': 500,\n",
    "    'batch_size': 64,\n",
    "    'gamma': 0.99,\n",
    "    'exploration_fraction': 0.3,\n",
    "    'exploration_final_eps': 0.05,\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"   Timesteps: {CONFIG['timesteps']}\")\n",
    "print(f\"   Density: {CONFIG['default_density']} ‚Üí {CONFIG['inflow_density']} veh/km\")\n",
    "print(f\"   Reward weights: Œ±={CONFIG['alpha']}, Œ∫={CONFIG['kappa']}, Œº={CONFIG['mu']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Environment Factory\n",
    "# =============================================================================\n",
    "def create_env(config=CONFIG, quiet=True):\n",
    "    \"\"\"Create traffic environment with congested scenario.\"\"\"\n",
    "    arz_config = create_victoria_island_config(\n",
    "        t_final=config['t_final'],\n",
    "        output_dt=config['decision_interval'],\n",
    "        cells_per_100m=4,\n",
    "        default_density=config['default_density'],\n",
    "        inflow_density=config['inflow_density'],\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    arz_config.rl_metadata = {\n",
    "        'observation_segment_ids': [s.id for s in arz_config.segments],\n",
    "        'decision_interval': config['decision_interval'],\n",
    "    }\n",
    "    \n",
    "    env = TrafficSignalEnvDirectV3(\n",
    "        simulation_config=arz_config,\n",
    "        decision_interval=config['decision_interval'],\n",
    "        observation_segment_ids=None,\n",
    "        reward_weights={\n",
    "            'alpha': config['alpha'],\n",
    "            'kappa': config['kappa'],\n",
    "            'mu': config['mu']\n",
    "        },\n",
    "        quiet=quiet\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Test environment\n",
    "print(\"Creating test environment...\")\n",
    "test_env = create_env(quiet=True)\n",
    "obs, _ = test_env.reset()\n",
    "print(f\"‚úÖ Environment created!\")\n",
    "print(f\"   Observation shape: {obs.shape}\")\n",
    "print(f\"   Action space: {test_env.action_space}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b64efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Training Callback for Logging\n",
    "# =============================================================================\n",
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Callback to track training progress.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_rewards = 0\n",
    "        self.current_length = 0\n",
    "        self.timestep_rewards = []  # For plotting\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Track rewards\n",
    "        reward = self.locals.get('rewards', [0])[0]\n",
    "        self.current_rewards += reward\n",
    "        self.current_length += 1\n",
    "        \n",
    "        # Save every 100 steps for plotting\n",
    "        if self.num_timesteps % 100 == 0:\n",
    "            self.timestep_rewards.append({\n",
    "                'timestep': self.num_timesteps,\n",
    "                'reward': self.current_rewards\n",
    "            })\n",
    "        \n",
    "        # Check for episode end\n",
    "        done = self.locals.get('dones', [False])[0]\n",
    "        if done:\n",
    "            self.episode_rewards.append(self.current_rewards)\n",
    "            self.episode_lengths.append(self.current_length)\n",
    "            self.current_rewards = 0\n",
    "            self.current_length = 0\n",
    "            \n",
    "        return True\n",
    "\n",
    "print(\"‚úÖ Callback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Baseline Evaluation (Fixed-Time Controller)\n",
    "# =============================================================================\n",
    "def evaluate_policy(env, policy_type='random', model=None, n_episodes=5, \n",
    "                   fixed_interval=30.0):\n",
    "    \"\"\"Evaluate a policy over multiple episodes.\"\"\"\n",
    "    results = {'rewards': [], 'densities': [], 'throughputs': []}\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = truncated = False\n",
    "        ep_reward = 0.0\n",
    "        ep_densities = []\n",
    "        time_since_switch = 0.0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            if policy_type == 'model':\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "            elif policy_type == 'fixed_time':\n",
    "                time_since_switch += env.decision_interval\n",
    "                action = 1 if time_since_switch >= fixed_interval else 0\n",
    "                if action == 1:\n",
    "                    time_since_switch = 0.0\n",
    "            else:  # random\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if 'avg_density' in info:\n",
    "                ep_densities.append(info['avg_density'])\n",
    "        \n",
    "        results['rewards'].append(ep_reward)\n",
    "        results['densities'].append(np.mean(ep_densities) if ep_densities else 0)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': float(np.mean(results['rewards'])),\n",
    "        'std_reward': float(np.std(results['rewards'])),\n",
    "        'mean_density': float(np.mean(results['densities'])),\n",
    "        'all_rewards': results['rewards']\n",
    "    }\n",
    "\n",
    "# Evaluate baseline\n",
    "print(\"üìä Evaluating baseline (Random policy)...\")\n",
    "baseline_env = create_env(quiet=True)\n",
    "baseline_results = evaluate_policy(baseline_env, policy_type='random', \n",
    "                                   n_episodes=CONFIG['eval_episodes'])\n",
    "baseline_env.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Baseline Results:\")\n",
    "print(f\"   Mean Reward: {baseline_results['mean_reward']:.2f} ¬± {baseline_results['std_reward']:.2f}\")\n",
    "print(f\"   Mean Density: {baseline_results['mean_density']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: DQN Training\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING DQN TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Timesteps: {CONFIG['timesteps']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "\n",
    "# Create environment\n",
    "train_env = create_env(quiet=True)\n",
    "\n",
    "# Create model\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    buffer_size=CONFIG['buffer_size'],\n",
    "    learning_starts=CONFIG['learning_starts'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    gamma=CONFIG['gamma'],\n",
    "    exploration_fraction=CONFIG['exploration_fraction'],\n",
    "    exploration_final_eps=CONFIG['exploration_final_eps'],\n",
    "    verbose=0  # Quiet mode\n",
    ")\n",
    "\n",
    "# Create callback\n",
    "callback = TrainingCallback()\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.learn(total_timesteps=CONFIG['timesteps'], callback=callback, progress_bar=True)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "print(f\"   Episodes completed: {len(callback.episode_rewards)}\")\n",
    "if callback.episode_rewards:\n",
    "    print(f\"   Final episode reward: {callback.episode_rewards[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e08be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Evaluate Trained Agent\n",
    "# =============================================================================\n",
    "print(\"üìä Evaluating trained agent...\")\n",
    "\n",
    "rl_results = evaluate_policy(train_env, policy_type='model', model=model,\n",
    "                            n_episodes=CONFIG['eval_episodes'])\n",
    "train_env.close()\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((rl_results['mean_reward'] - baseline_results['mean_reward']) \n",
    "               / abs(baseline_results['mean_reward']) * 100)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üìà RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline (Random):  {baseline_results['mean_reward']:.2f} ¬± {baseline_results['std_reward']:.2f}\")\n",
    "print(f\"DQN Agent:          {rl_results['mean_reward']:.2f} ¬± {rl_results['std_reward']:.2f}\")\n",
    "print(f\"Improvement:        {improvement:+.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Generate Figures for Thesis\n",
    "# =============================================================================\n",
    "print(\"üìä Generating thesis figures...\")\n",
    "\n",
    "# Figure 1: Training Curve\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "if callback.episode_rewards:\n",
    "    episodes = range(1, len(callback.episode_rewards) + 1)\n",
    "    ax1.plot(episodes, callback.episode_rewards, 'b-', linewidth=2, label='Episode Reward')\n",
    "    \n",
    "    # Moving average\n",
    "    if len(callback.episode_rewards) >= 3:\n",
    "        window = min(5, len(callback.episode_rewards))\n",
    "        ma = np.convolve(callback.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(range(window, len(callback.episode_rewards)+1), ma, 'r-', \n",
    "                linewidth=2, label=f'Moving Avg ({window} ep)')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Cumulative Reward', fontsize=12)\n",
    "ax1.set_title('DQN Training Progress - Traffic Signal Control', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "fig1_path = os.path.join(OUTPUT_DIR, 'fig_8_training_curve.png')\n",
    "fig1.savefig(fig1_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {fig1_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Figure 2: Comparison Bar Chart\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = ['Random\\n(Baseline)', 'DQN Agent']\n",
    "rewards = [baseline_results['mean_reward'], rl_results['mean_reward']]\n",
    "stds = [baseline_results['std_reward'], rl_results['std_reward']]\n",
    "colors = ['#ff7f0e', '#2ca02c']\n",
    "\n",
    "bars = ax2.bar(methods, rewards, yerr=stds, capsize=5, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Mean Cumulative Reward', fontsize=12)\n",
    "ax2.set_title(f'Performance Comparison: DQN vs Baseline\\n(Improvement: {improvement:+.1f}%)', fontsize=14)\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, rewards):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "            f'{val:.1f}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "fig2_path = os.path.join(OUTPUT_DIR, 'fig_8_comparison.png')\n",
    "fig2.savefig(fig2_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {fig2_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Save Results and Model\n",
    "# =============================================================================\n",
    "print(\"üíæ Saving results and model...\")\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(OUTPUT_DIR, 'dqn_traffic_signal')\n",
    "model.save(model_path)\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "# Save results JSON\n",
    "results_summary = {\n",
    "    'config': CONFIG,\n",
    "    'baseline': baseline_results,\n",
    "    'dqn': rl_results,\n",
    "    'improvement_percent': improvement,\n",
    "    'training_time_seconds': training_time,\n",
    "    'training_episodes': len(callback.episode_rewards),\n",
    "    'episode_rewards': callback.episode_rewards,\n",
    "}\n",
    "\n",
    "results_path = os.path.join(OUTPUT_DIR, 'results_summary.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"‚úÖ Results saved: {results_path}\")\n",
    "\n",
    "# List all files\n",
    "print(f\"\\nüìÅ Files in output directory:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath) / 1024\n",
    "    print(f\"   {f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Download Results (Colab Only)\n",
    "# =============================================================================\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    \n",
    "    # Create ZIP archive\n",
    "    zip_name = 'thesis_rl_results'\n",
    "    shutil.make_archive(f'/content/{zip_name}', 'zip', OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\nüì• Downloading results...\")\n",
    "    print(\"   (A download dialog should appear)\")\n",
    "    \n",
    "    # Download ZIP\n",
    "    files.download(f'/content/{zip_name}.zip')\n",
    "    \n",
    "    print(\"\\n‚úÖ Download initiated!\")\n",
    "    print(f\"   Archive contains: fig_8_training_curve.png, fig_8_comparison.png,\")\n",
    "    print(f\"   results_summary.json, dqn_traffic_signal.zip\")\n",
    "else:\n",
    "    print(f\"\\nüìÅ Results saved locally in: {OUTPUT_DIR}\")\n",
    "    print(\"   Copy figures to: images/chapter3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3434507",
   "metadata": {},
   "source": [
    "## üìä R√©sum√© Final\n",
    "\n",
    "### R√©sultats de l'entra√Ænement:\n",
    "- **Timesteps**: 5000\n",
    "- **Baseline (Random)**: Voir cellule 7\n",
    "- **DQN Agent**: Voir cellule 7\n",
    "- **Am√©lioration**: Voir cellule 7\n",
    "\n",
    "### Fichiers g√©n√©r√©s:\n",
    "1. `fig_8_training_curve.png` - Courbe d'apprentissage\n",
    "2. `fig_8_comparison.png` - Comparaison baseline vs DQN\n",
    "3. `results_summary.json` - Donn√©es num√©riques\n",
    "4. `dqn_traffic_signal.zip` - Mod√®le entra√Æn√©\n",
    "\n",
    "### Pour la th√®se:\n",
    "Copiez les figures PNG dans `images/chapter3/` et mettez √† jour `section8_evaluation_rl.tex`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
