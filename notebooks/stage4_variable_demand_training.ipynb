{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41890ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Clone repo and install dependencies\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/elonmj/Code-traffic-flow.git\"\n",
    "REPO_DIR = \"/content/Code-traffic-flow\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "!pip install stable-baselines3 gymnasium numba --quiet\n",
    "print(f\"‚úÖ Setup complete | Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c04f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports & Setup - STAGE 4: VARIABLE DEMAND\n",
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from arz_model.config import create_victoria_island_config\n",
    "from Code_RL.src.env.traffic_signal_env_direct_v3 import TrafficSignalEnvDirectV3\n",
    "from Code_RL.src.env.variable_demand_wrapper import VariableDemandEnv  # NEW: Variable Demand\n",
    "\n",
    "# Mount Google Drive for persistence\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_DIR = \"/content/drive/MyDrive/thesis_runs_stage4\"  # Stage 4 directory\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f\"‚úÖ Google Drive mounted. Saving results to: {SAVE_DIR}\")\n",
    "except:\n",
    "    SAVE_DIR = \"/content\"\n",
    "    print(f\"‚ö†Ô∏è Google Drive not available. Saving to: {SAVE_DIR}\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration - STAGE 4: DOMAIN RANDOMIZATION\n",
    "# ============================================================================\n",
    "# THEORETICAL JUSTIFICATION (Webster's Formula):\n",
    "# For stationary demand q, the delay d = f(Œª) is convex where Œª = g/C\n",
    "# Therefore, Fixed-Time tuned to (Œª*, C*) is GLOBALLY OPTIMAL for constant q\n",
    "# RL CANNOT beat this theoretical optimum; it can only match it\n",
    "# \n",
    "# SOLUTION: Break stationarity with Domain Randomization\n",
    "# œÅ_in ~ U(80, 280) veh/km creates scenarios where RL > Fixed-Time\n",
    "# ============================================================================\n",
    "\n",
    "DEFAULT_DENSITY = 120.0  # Initial network density\n",
    "\n",
    "# Domain Randomization bounds (as per thesis section 8)\n",
    "DENSITY_RANGE = (80.0, 280.0)   # veh/km: LOS A-B to LOS E-F\n",
    "VELOCITY_RANGE = (30.0, 50.0)   # km/h: entry velocity variation\n",
    "\n",
    "# Reward weights - CONGESTION ONLY (validated by Lee et al. 2022, Wei et al. 2018)\n",
    "# Literature supports using delay/queue/density alone as reward\n",
    "# Œº=0 removes throughput which was dominating and masking congestion signal\n",
    "REWARD_WEIGHTS = {'alpha': 5.0, 'kappa': 0.0, 'mu': 0.0}\n",
    "\n",
    "def create_fixed_demand_env(inflow_density=180.0, quiet=True):\n",
    "    \"\"\"Create environment with FIXED demand (for baseline comparison).\"\"\"\n",
    "    config = create_victoria_island_config(\n",
    "        t_final=450.0, output_dt=15.0, cells_per_100m=4,\n",
    "        default_density=DEFAULT_DENSITY, inflow_density=inflow_density, use_cache=False\n",
    "    )\n",
    "    config.rl_metadata = {'observation_segment_ids': [s.id for s in config.segments], 'decision_interval': 15.0}\n",
    "    return TrafficSignalEnvDirectV3(\n",
    "        simulation_config=config,\n",
    "        decision_interval=15.0, reward_weights=REWARD_WEIGHTS, quiet=quiet\n",
    "    )\n",
    "\n",
    "def create_variable_demand_env(quiet=True, seed=None):\n",
    "    \"\"\"Create environment with VARIABLE demand (Domain Randomization).\"\"\"\n",
    "    return VariableDemandEnv(\n",
    "        density_range=DENSITY_RANGE,\n",
    "        velocity_range=VELOCITY_RANGE,\n",
    "        default_density=DEFAULT_DENSITY,\n",
    "        t_final=450.0,\n",
    "        decision_interval=15.0,\n",
    "        reward_weights=REWARD_WEIGHTS,\n",
    "        seed=seed,\n",
    "        quiet=quiet\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ STAGE 4: Variable Demand Environment Ready\")\n",
    "print(f\"   Domain Randomization: œÅ_in ~ U{DENSITY_RANGE} veh/km\")\n",
    "print(f\"   Velocity Range: v_in ~ U{VELOCITY_RANGE} km/h\")\n",
    "print(f\"   Reward Weights: Œ±={REWARD_WEIGHTS['alpha']}, Œ∫={REWARD_WEIGHTS['kappa']}, Œº={REWARD_WEIGHTS['mu']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14737e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Evaluate Baselines on FIXED Demand (Reference Point)\n",
    "# This establishes the baseline performance on medium demand (œÅ=180)\n",
    "env = create_fixed_demand_env(inflow_density=180.0)\n",
    "baseline_results = {}\n",
    "\n",
    "def eval_fixed_time(env, interval, n_ep=3):\n",
    "    \"\"\"Evaluate Fixed-Time controller with given cycle interval.\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_ep):\n",
    "        obs, _ = env.reset()\n",
    "        done, ep_r, t = False, 0.0, 0.0\n",
    "        while not done:\n",
    "            t += env.decision_interval\n",
    "            action = 1 if t >= interval else 0\n",
    "            if action == 1: t = 0.0\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            ep_r += r\n",
    "        rewards.append(ep_r)\n",
    "    return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards)}\n",
    "\n",
    "# Random baseline\n",
    "print(\"üé≤ Random (Reference)...\")\n",
    "rnd = [sum([env.step(env.action_space.sample())[1] for _ in range(30)]) for _ in [env.reset() for _ in range(3)]]\n",
    "baseline_results['Random'] = {'mean_reward': np.mean(rnd), 'std_reward': np.std(rnd)}\n",
    "\n",
    "# Fixed-time baselines on FIXED demand\n",
    "for name, interval in [('FT-30s', 30), ('FT-60s', 60), ('FT-90s', 90)]:\n",
    "    print(f\"‚è±Ô∏è {name} (œÅ=180, fixed)...\")\n",
    "    baseline_results[name] = eval_fixed_time(env, interval)\n",
    "\n",
    "print(\"\\nüìä BASELINES (Fixed Demand œÅ=180):\")\n",
    "for n, d in sorted(baseline_results.items(), key=lambda x: x[1]['mean_reward'], reverse=True):\n",
    "    print(f\"  {n:10s}: {d['mean_reward']:>8.1f} ¬± {d['std_reward']:.1f}\")\n",
    "\n",
    "FT90_REF = baseline_results['FT-90s']['mean_reward']\n",
    "print(f\"\\nüéØ Reference for comparison: FT-90s = {FT90_REF:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d52cb",
   "metadata": {},
   "source": [
    "### üìê Justification Th√©orique : Pourquoi la Demande Variable ?\n",
    "\n",
    "**Probl√®me avec la demande constante** (Stage 3) :\n",
    "- Pour une demande stationnaire $q_{in}(t) = \\bar{q}$, le d√©lai moyen $d = f(\\lambda)$ est une **fonction convexe**\n",
    "- Il existe donc un unique optimum $(\\lambda^*, C^*)$ ‚Üí Fixed-Time configur√© sur cet optimum est **globalement optimal**\n",
    "- L'agent RL ne peut math√©matiquement pas faire mieux ; il converge vers FT-90s\n",
    "\n",
    "**Solution : Domain Randomization** (Stage 4) :\n",
    "$$\\rho_{in} \\sim \\mathcal{U}(80, 280) \\text{ veh/km}$$\n",
    "\n",
    "Cette variabilit√© cr√©e des conditions o√π :\n",
    "1. **R√©gime fluide** (œÅ < 100) : FT-90s gaspille du temps de vert inutile\n",
    "2. **R√©gime satur√©** (œÅ > 200) : FT-90s cr√©e des files d'attente r√©siduelles\n",
    "3. **L'agent RL** apprend √† **diagnostiquer** l'√©tat et **adapter** dynamiquement ses d√©cisions\n",
    "\n",
    "**Hypoth√®se H5** : L'agent RL surpassera FT-90s sur les sc√©narios de demande variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4b: Test TRUE Domain Randomization\n",
    "# This verifies that inflow_density ACTUALLY changes the simulation behavior\n",
    "print(\"üî¨ Testing TRUE Domain Randomization...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_env = create_variable_demand_env(quiet=True, seed=42)\n",
    "\n",
    "# Run 5 episodes with Domain Randomization\n",
    "print(\"\\nüìä Domain Randomization Test (5 episodes):\")\n",
    "print(\"-\"*60)\n",
    "episode_densities = []\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    obs, info = test_env.reset()\n",
    "    density = info.get('inflow_density', 'N/A')\n",
    "    velocity = info.get('inflow_velocity', 'N/A')\n",
    "    episode_densities.append(density)\n",
    "    \n",
    "    ep_reward = 0.0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = test_env.action_space.sample()  # Random actions for test\n",
    "        obs, reward, done, truncated, _ = test_env.step(action)\n",
    "        ep_reward += reward\n",
    "        steps += 1\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(ep_reward)\n",
    "    print(f\"  Episode {ep+1}: œÅ_in={density:.0f} veh/km, v_in={velocity:.0f} km/h ‚Üí Reward={ep_reward:.1f}\")\n",
    "\n",
    "# Verify variation\n",
    "density_std = np.std(episode_densities)\n",
    "print(\"-\"*60)\n",
    "print(f\"\\n‚úÖ Density variation: œÉ = {density_std:.1f} veh/km\")\n",
    "if density_std > 10:\n",
    "    print(\"   ‚Üí TRUE Domain Randomization is working!\")\n",
    "    print(\"   ‚Üí Each episode has DIFFERENT traffic conditions\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è WARNING: Low variation - check implementation\")\n",
    "\n",
    "print(f\"\\nüìà Reward variation: min={min(episode_rewards):.1f}, max={max(episode_rewards):.1f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Progressive Training Callback with BEST MODEL CHECKPOINTING\n",
    "class ProgressCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback to track training progress with variable demand.\n",
    "    \n",
    "    Key feature: Saves BEST model based on evaluation reward, not just periodic saves.\n",
    "    This ensures we keep the best-performing model even if performance degrades later.\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env, ref_reward, save_path, target_pct=10.0, eval_freq=5000, n_eval=5):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.ref = ref_reward\n",
    "        self.target = ref_reward * (1 + target_pct/100)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval = n_eval\n",
    "        self.save_path = save_path  # Path to save best model\n",
    "        self.history = []\n",
    "        self.best = -np.inf\n",
    "        self.best_step = 0\n",
    "        self.reached = False\n",
    "        \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            rewards = []\n",
    "            densities = []\n",
    "            all_actions = []\n",
    "            \n",
    "            for _ in range(self.n_eval):\n",
    "                obs, info = self.eval_env.reset()\n",
    "                densities.append(info.get('inflow_density', 180))\n",
    "                done = False\n",
    "                ep_reward = 0\n",
    "                ep_actions = []\n",
    "                \n",
    "                while not done:\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    ep_actions.append(int(action))\n",
    "                    obs, reward, done, truncated, _ = self.eval_env.step(action)\n",
    "                    ep_reward += reward\n",
    "                    if truncated:\n",
    "                        break\n",
    "                        \n",
    "                rewards.append(ep_reward)\n",
    "                all_actions.append(ep_actions)\n",
    "            \n",
    "            mean_r = np.mean(rewards)\n",
    "            std_r = np.std(rewards)\n",
    "            mean_density = np.mean(densities)\n",
    "            imp = ((mean_r - self.ref) / abs(self.ref)) * 100\n",
    "            \n",
    "            self.history.append({\n",
    "                'step': self.num_timesteps, \n",
    "                'reward': mean_r, \n",
    "                'std': std_r,\n",
    "                'improvement': imp,\n",
    "                'mean_density': mean_density\n",
    "            })\n",
    "            \n",
    "            # BEST MODEL CHECKPOINTING: Save only when we improve\n",
    "            new_best = False\n",
    "            if mean_r > self.best:\n",
    "                self.best = mean_r\n",
    "                self.best_step = self.num_timesteps\n",
    "                self.model.save(f\"{self.save_path}/model_BEST\")\n",
    "                new_best = True\n",
    "            \n",
    "            # Action analysis\n",
    "            n_switches = sum(all_actions[0]) if all_actions else 0\n",
    "            n_steps = len(all_actions[0]) if all_actions else 1\n",
    "            switch_rate = n_switches / n_steps * 100\n",
    "            \n",
    "            best_marker = \"üíæ BEST\" if new_best else \"\"\n",
    "            status = \"üéØ\" if imp >= 10 else \"\"\n",
    "            print(f\"  [{self.num_timesteps:>6}] R={mean_r:>7.1f}¬±{std_r:.1f} | œÅ={mean_density:.0f} | Œî={imp:>+5.1f}% | Sw={switch_rate:.0f}% {status} {best_marker}\")\n",
    "            \n",
    "            if imp >= 10 and not self.reached:\n",
    "                self.reached = True\n",
    "                print(f\"\\nüèÜ TARGET +10% REACHED at step {self.num_timesteps}!\")\n",
    "                \n",
    "        return True\n",
    "\n",
    "print(f\"‚úÖ Callback ready with BEST MODEL CHECKPOINTING\")\n",
    "print(f\"   Reference: FT-90s (fixed œÅ=180) = {FT90_REF:.1f}\")\n",
    "print(f\"   Target: +10% improvement = {FT90_REF*1.1:.1f}\")\n",
    "print(f\"   Best model saved automatically when performance improves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Initialize DQN Model - STAGE 4 (120k Steps, 3 blocks of 40k)\n",
    "# Create environments with Domain Randomization\n",
    "train_env = create_variable_demand_env(quiet=True, seed=42)\n",
    "eval_env = create_variable_demand_env(quiet=True, seed=123)  # Different seed for eval\n",
    "\n",
    "TOTAL_STEPS = 120_000\n",
    "BLOCK_SIZE = 40_000\n",
    "N_BLOCKS = 3  # 3 blocks √ó 40k = 120k total\n",
    "\n",
    "# DQN with 30% exploration (36k steps out of 120k)\n",
    "model = DQN(\n",
    "    \"MlpPolicy\", \n",
    "    train_env, \n",
    "    learning_rate=1e-4, \n",
    "    buffer_size=50000,    # Smaller buffer for training\n",
    "    learning_starts=1000,\n",
    "    batch_size=64, \n",
    "    tau=0.005, \n",
    "    gamma=0.99,\n",
    "    exploration_fraction=0.3,  # 30% exploration (36k steps)\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=0, \n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Callback with best model saving\n",
    "callback = ProgressCallback(\n",
    "    eval_env, FT90_REF, \n",
    "    save_path=SAVE_DIR,  # Path for best model\n",
    "    target_pct=10.0, \n",
    "    eval_freq=5000,  # Evaluate every 5k steps\n",
    "    n_eval=5\n",
    ")\n",
    "STATE = {'model': model, 'callback': callback, 'steps': 0, 'block': 0, 'done': False}\n",
    "\n",
    "print(f\"üöÄ STAGE 4: DQN ready on {model.device}\")\n",
    "print(f\"   Training Environment: Variable Demand œÅ ~ U(80, 280)\")\n",
    "print(f\"   Total Steps: {TOTAL_STEPS:,} ({N_BLOCKS} blocks √ó {BLOCK_SIZE//1000}k)\")\n",
    "print(f\"   Exploration: 30% ({int(TOTAL_STEPS*0.3):,} steps)\")\n",
    "print(f\"   Best Model: Auto-saved when performance improves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463faae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Steps 0 ‚Üí 40k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 1: 0 ‚Üí 40k\")\n",
    "    STATE['model'].learn(BLOCK_SIZE, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 1, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    print(f\"‚úÖ Block 1 done | Steps: {STATE['steps']:,} | Best: {STATE['callback'].best:.1f} (at {STATE['callback'].best_step:,})\")\n",
    "    \n",
    "    # üßπ Memory Cleanup to prevent Kaggle crashes\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383cb9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üîÑ DR[1250]: œÅ=244 veh/km, v=40 km/h ‚Üí 4 segments modified\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üîÑ DR[1250]: œÅ=244 veh/km, v=40 km/h ‚Üí 4 segments modified\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ‚îî‚îÄ 168585503-&gt;128957652...: 190‚Üí244\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ‚îî‚îÄ 168585503->128957652...: 190‚Üí244\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ‚îî‚îÄ 1472142462-&gt;14721218...: 190‚Üí244\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ‚îî‚îÄ 1472142462->14721218...: 190‚Üí244\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üé≤ DR Episode 1250: œÅ=244 veh/km, v=40 km/h\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üé≤ DR Episode 1250: œÅ=244 veh/km, v=40 km/h\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üîÑ DR[1300]: œÅ=269 veh/km, v=49 km/h ‚Üí 4 segments modified\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üîÑ DR[1300]: œÅ=269 veh/km, v=49 km/h ‚Üí 4 segments modified\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ‚îî‚îÄ 168585503-&gt;128957652...: 98‚Üí269\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ‚îî‚îÄ 168585503->128957652...: 98‚Üí269\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ‚îî‚îÄ 1472142462-&gt;14721218...: 98‚Üí269\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ‚îî‚îÄ 1472142462->14721218...: 98‚Üí269\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üé≤ DR Episode 1300: œÅ=269 veh/km, v=49 km/h\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üé≤ DR Episode 1300: œÅ=269 veh/km, v=49 km/h\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  [ 40000] R=  -92.6¬±0.4 | œÅ=160 | Œî= +2.4% | Sw=0%  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "  [ 40000] R=  -92.6¬±0.4 | œÅ=160 | Œî= +2.4% | Sw=0%  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Block 2 done | Steps: 40,000 | Best: -92.3 (at 30,000)\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Steps 40k ‚Üí 80k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 2: 40k ‚Üí 80k\")\n",
    "    STATE['model'].learn(BLOCK_SIZE, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 2, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    print(f\"‚úÖ Block 2 done | Steps: {STATE['steps']:,} | Best: {STATE['callback'].best:.1f} (at {STATE['callback'].best_step:,})\")\n",
    "    \n",
    "    # üßπ Memory Cleanup to prevent Kaggle crashes\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 (FINAL): Steps 80k ‚Üí 120k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 3 (FINAL): 80k ‚Üí 120k\")\n",
    "    STATE['model'].learn(BLOCK_SIZE, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 3, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "\n",
    "# Final summary\n",
    "imp = ((STATE['callback'].best - FT90_REF) / abs(FT90_REF)) * 100\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üèÅ TRAINING COMPLETE - STAGE 4\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Total steps: {STATE['steps']:,}\")\n",
    "print(f\"   Best reward: {STATE['callback'].best:.1f} (at step {STATE['callback'].best_step:,})\")\n",
    "print(f\"   Improvement vs FT-90s: {imp:+.1f}%\")\n",
    "print(f\"   Target (+10%): {'‚úÖ ACHIEVED' if imp >= 10 else '‚ùå NOT YET'}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüíæ Best model saved at: {SAVE_DIR}/model_BEST.zip\")\n",
    "\n",
    "# üßπ Memory Cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cce65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final: Plot Learning Curve & Save Results\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "h = STATE['callback'].history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Learning Curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot([x['step'] for x in h], [x['reward'] for x in h], 'b-o', markersize=2, label='RL Agent')\n",
    "ax1.fill_between([x['step'] for x in h], \n",
    "                 [x['reward']-x.get('std',0) for x in h], \n",
    "                 [x['reward']+x.get('std',0) for x in h], alpha=0.2)\n",
    "ax1.axhline(y=FT90_REF, color='r', linestyle='--', label='FT-90s (œÅ=180)')\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.legend()\n",
    "ax1.set_title('Stage 4: Learning Curve (Variable Demand)')\n",
    "\n",
    "# Plot 2: Improvement vs FT-90s\n",
    "ax2 = axes[1]\n",
    "ax2.plot([x['step'] for x in h], [x['improvement'] for x in h], 'b-o', markersize=2)\n",
    "ax2.axhline(y=10, color='g', linestyle='--', label='Target +10%')\n",
    "ax2.axhline(y=0, color='r', linestyle='-', alpha=0.5)\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Improvement vs FT-90s (%)')\n",
    "ax2.legend()\n",
    "ax2.set_title('Improvement Over Fixed-Time Baseline')\n",
    "\n",
    "# Plot 3: Mean Density Distribution during eval\n",
    "ax3 = axes[2]\n",
    "ax3.plot([x['step'] for x in h], [x.get('mean_density', 180) for x in h], 'g-o', markersize=2)\n",
    "ax3.axhline(y=180, color='orange', linestyle='--', label='Fixed œÅ=180')\n",
    "ax3.set_xlabel('Training Steps')\n",
    "ax3.set_ylabel('Mean Eval Density (veh/km)')\n",
    "ax3.legend()\n",
    "ax3.set_title('Variable Demand Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/stage4_learning_curve.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save JSON results\n",
    "results = {\n",
    "    'stage': 'Stage 4 - Variable Demand',\n",
    "    'baselines': baseline_results, \n",
    "    'history': h, \n",
    "    'best': STATE['callback'].best,\n",
    "    'improvement': ((STATE['callback'].best - FT90_REF) / abs(FT90_REF)) * 100,\n",
    "    'density_range': list(DENSITY_RANGE),\n",
    "    'total_steps': STATE['steps']\n",
    "}\n",
    "with open(f'{SAVE_DIR}/stage4_results.json', 'w') as f: \n",
    "    json.dump(results, f, indent=2)\n",
    "    \n",
    "print(f\"üìÅ Saved: {SAVE_DIR}/stage4_learning_curve.png\")\n",
    "print(f\"üìÅ Saved: {SAVE_DIR}/stage4_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Comparative Evaluation on Multiple Demand Scenarios\n",
    "# Load BEST model for evaluation (not the final one which may be worse)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä STAGE 4: COMPARATIVE EVALUATION - RL vs FT-90s\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the best model (saved during training when performance improved)\n",
    "print(f\"üíæ Loading BEST model from: {SAVE_DIR}/model_BEST.zip\")\n",
    "best_model = DQN.load(f\"{SAVE_DIR}/model_BEST\", device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"   Best reward during training: {STATE['callback'].best:.1f}\")\n",
    "print(f\"   Best model saved at step: {STATE['callback'].best_step:,}\")\n",
    "\n",
    "# Define test scenarios covering the demand spectrum\n",
    "TEST_SCENARIOS = {\n",
    "    'Light (œÅ=100)': 100.0,    # LOS A-B: Free flow\n",
    "    'Medium (œÅ=180)': 180.0,   # LOS C-D: Reference scenario\n",
    "    'Heavy (œÅ=250)': 250.0,    # LOS E: Near saturation\n",
    "    'Peak (œÅ=300)': 300.0      # LOS F: Oversaturation\n",
    "}\n",
    "\n",
    "def evaluate_on_scenario(model, inflow_density, n_episodes=5):\n",
    "    \"\"\"Evaluate RL agent on a specific demand scenario.\"\"\"\n",
    "    env = create_fixed_demand_env(inflow_density=inflow_density, quiet=True)\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done, ep_r = False, 0.0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            ep_r += r\n",
    "        rewards.append(ep_r)\n",
    "    return {'mean': np.mean(rewards), 'std': np.std(rewards)}\n",
    "\n",
    "def evaluate_ft90_on_scenario(inflow_density, n_episodes=5):\n",
    "    \"\"\"Evaluate FT-90s baseline on a specific demand scenario.\"\"\"\n",
    "    env = create_fixed_demand_env(inflow_density=inflow_density, quiet=True)\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done, ep_r, t = False, 0.0, 0.0\n",
    "        while not done:\n",
    "            t += env.decision_interval\n",
    "            action = 1 if t >= 90 else 0\n",
    "            if action == 1: t = 0.0\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            ep_r += r\n",
    "        rewards.append(ep_r)\n",
    "    return {'mean': np.mean(rewards), 'std': np.std(rewards)}\n",
    "\n",
    "# Evaluate BEST model (not STATE['model']) on all scenarios\n",
    "rl_results = {}\n",
    "ft90_results = {}\n",
    "\n",
    "print(\"\\nüìà Evaluating BEST model on each demand scenario...\")\n",
    "for name, density in TEST_SCENARIOS.items():\n",
    "    print(f\"  Testing {name}...\")\n",
    "    rl_results[name] = evaluate_on_scenario(best_model, density)  # Use best_model\n",
    "    ft90_results[name] = evaluate_ft90_on_scenario(density)\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'Scenario':<20} | {'RL (BEST)':>12} | {'FT-90s':>12} | {'Œî (%)':>10}\")\n",
    "print(\"-\"*70)\n",
    "total_rl, total_ft = 0, 0\n",
    "for scenario in TEST_SCENARIOS:\n",
    "    rl_mean = rl_results[scenario]['mean']\n",
    "    ft_mean = ft90_results[scenario]['mean']\n",
    "    improvement = ((rl_mean - ft_mean) / abs(ft_mean)) * 100\n",
    "    total_rl += rl_mean\n",
    "    total_ft += ft_mean\n",
    "    marker = \"üéØ\" if improvement > 5 else (\"‚úì\" if improvement > 0 else \"\")\n",
    "    print(f\"{scenario:<20} | {rl_mean:>12.1f} | {ft_mean:>12.1f} | {improvement:>+9.1f}% {marker}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "avg_improvement = ((total_rl - total_ft) / abs(total_ft)) * 100\n",
    "print(f\"{'AVERAGE':<20} | {total_rl/4:>12.1f} | {total_ft/4:>12.1f} | {avg_improvement:>+9.1f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store scenario results\n",
    "scenario_results = {'rl': rl_results, 'ft90': ft90_results, 'scenarios': TEST_SCENARIOS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Visualization: Bar Chart Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Bar chart comparison\n",
    "scenarios = list(TEST_SCENARIOS.keys())\n",
    "rl_means = [rl_results[s]['mean'] for s in scenarios]\n",
    "ft_means = [ft90_results[s]['mean'] for s in scenarios]\n",
    "x = np.arange(len(scenarios))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, rl_means, width, label='RL BEST Model', color='#2ecc71')\n",
    "bars2 = ax1.bar(x + width/2, ft_means, width, label='Fixed-Time (90s)', color='#e74c3c')\n",
    "ax1.set_xlabel('Traffic Demand Scenario')\n",
    "ax1.set_ylabel('Mean Reward')\n",
    "ax1.set_title('Stage 4: RL (BEST) vs Fixed-Time Across Demand Levels')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(scenarios, rotation=15)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Improvement percentage\n",
    "improvements = [((rl_results[s]['mean'] - ft90_results[s]['mean']) / abs(ft90_results[s]['mean'])) * 100 \n",
    "                for s in scenarios]\n",
    "colors = ['#27ae60' if imp > 0 else '#c0392b' for imp in improvements]\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(scenarios, improvements, color=colors, edgecolor='black')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.axhline(y=10, color='green', linestyle='--', label='Target +10%', alpha=0.7)\n",
    "ax2.set_xlabel('Traffic Demand Scenario')\n",
    "ax2.set_ylabel('Improvement over FT-90s (%)')\n",
    "ax2.set_title('RL Advantage by Demand Level')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{imp:+.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/stage4_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÅ STAGE 4 COMPLETE: VARIABLE DEMAND VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Training: {STATE['steps']:,} steps ({N_BLOCKS} blocks √ó {BLOCK_SIZE//1000}k)\")\n",
    "print(f\"   Best Reward: {STATE['callback'].best:.1f} (at step {STATE['callback'].best_step:,})\")\n",
    "print(f\"   Overall Improvement: {avg_improvement:+.1f}%\")\n",
    "print(f\"   Target (+10%): {'‚úÖ ACHIEVED' if avg_improvement >= 10 else '‚ùå NOT YET'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save all results\n",
    "final_results = {\n",
    "    'stage': 'Stage 4 - Variable Demand Training (200k)',\n",
    "    'training': {\n",
    "        'total_steps': STATE['steps'],\n",
    "        'block_size': BLOCK_SIZE,\n",
    "        'n_blocks': N_BLOCKS,\n",
    "        'best_reward': STATE['callback'].best,\n",
    "        'best_step': STATE['callback'].best_step,\n",
    "        'history': STATE['callback'].history\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'scenarios': scenario_results,\n",
    "        'average_improvement': avg_improvement\n",
    "    },\n",
    "    'config': {\n",
    "        'density_range': list(DENSITY_RANGE),\n",
    "        'velocity_range': list(VELOCITY_RANGE),\n",
    "        'reward_weights': REWARD_WEIGHTS\n",
    "    }\n",
    "}\n",
    "with open(f'{SAVE_DIR}/stage4_final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nüìÅ All results saved to: {SAVE_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
