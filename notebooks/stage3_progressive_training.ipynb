{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41890ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Clone repo and install dependencies\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/elonmj/Code-traffic-flow.git\"\n",
    "REPO_DIR = \"/content/Code-traffic-flow\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "!pip install stable-baselines3 gymnasium numba --quiet\n",
    "print(f\"‚úÖ Setup complete | Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c04f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports & Setup\n",
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from arz_model.config import create_victoria_island_config\n",
    "from Code_RL.src.env.traffic_signal_env_direct_v3 import TrafficSignalEnvDirectV3\n",
    "\n",
    "# Mount Google Drive for persistence\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_DIR = \"/content/drive/MyDrive/thesis_runs_stage3\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f\"‚úÖ Google Drive mounted. Saving results to: {SAVE_DIR}\")\n",
    "except:\n",
    "    SAVE_DIR = \"/content\"\n",
    "    print(f\"‚ö†Ô∏è Google Drive not available. Saving to: {SAVE_DIR}\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "# FIX: Reduce phase change penalty to encourage exploration\n",
    "DEFAULT_DENSITY = 120.0  # Congested scenario\n",
    "INFLOW_DENSITY = 180.0\n",
    "\n",
    "# IMPORTANT: Set kappa=0.0 to remove phase change penalty initially\n",
    "# This allows the agent to explore switching without penalty\n",
    "REWARD_WEIGHTS = {'alpha': 5.0, 'kappa': 0.0, 'mu': 0.1}  # kappa=0.0 removes switch penalty\n",
    "\n",
    "def create_env(quiet=True):\n",
    "    \"\"\"Create traffic environment\"\"\"\n",
    "    config = create_victoria_island_config(\n",
    "        t_final=450.0, output_dt=15.0, cells_per_100m=4,\n",
    "        default_density=DEFAULT_DENSITY, inflow_density=INFLOW_DENSITY, use_cache=False\n",
    "    )\n",
    "    config.rl_metadata = {'observation_segment_ids': [s.id for s in config.segments], 'decision_interval': 15.0}\n",
    "    \n",
    "    class SimpleConfig:\n",
    "        def __init__(self, c):\n",
    "            self.arz_simulation_config = c\n",
    "            self.rl_env_params = {'dt_decision': 15.0, 'observation_segment_ids': None, 'reward_weights': REWARD_WEIGHTS}\n",
    "    \n",
    "    return TrafficSignalEnvDirectV3(\n",
    "        simulation_config=SimpleConfig(config).arz_simulation_config,\n",
    "        decision_interval=15.0, reward_weights=REWARD_WEIGHTS, quiet=quiet\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Environment ready | kappa={REWARD_WEIGHTS['kappa']} (no switch penalty)\")\n",
    "print(f\"   Congestion: alpha={REWARD_WEIGHTS['alpha']}, Throughput: mu={REWARD_WEIGHTS['mu']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14737e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Evaluate Baselines (Random, FT-30s, FT-60s, FT-90s)\n",
    "env = create_env()\n",
    "baseline_results = {}\n",
    "\n",
    "def eval_fixed_time(env, interval, n_ep=3):\n",
    "    rewards = []\n",
    "    for _ in range(n_ep):\n",
    "        obs, _ = env.reset()\n",
    "        done, ep_r, t = False, 0.0, 0.0\n",
    "        while not done:\n",
    "            t += env.decision_interval\n",
    "            action = 1 if t >= interval else 0\n",
    "            if action == 1: t = 0.0\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            ep_r += r\n",
    "        rewards.append(ep_r)\n",
    "    return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards)}\n",
    "\n",
    "# Random\n",
    "print(\"üé≤ Random...\")\n",
    "rnd = [sum([env.step(env.action_space.sample())[1] for _ in range(30)]) for _ in [env.reset() for _ in range(3)]]\n",
    "baseline_results['Random'] = {'mean_reward': np.mean(rnd), 'std_reward': np.std(rnd)}\n",
    "\n",
    "# Fixed-time baselines\n",
    "for name, interval in [('FT-30s', 30), ('FT-60s', 60), ('FT-90s', 90)]:\n",
    "    print(f\"‚è±Ô∏è {name}...\")\n",
    "    baseline_results[name] = eval_fixed_time(env, interval)\n",
    "\n",
    "print(\"\\nüìä BASELINES:\")\n",
    "for n, d in sorted(baseline_results.items(), key=lambda x: x[1]['mean_reward'], reverse=True):\n",
    "    print(f\"  {n:10s}: {d['mean_reward']:>8.1f} ¬± {d['std_reward']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d52cb",
   "metadata": {},
   "source": [
    "### üß† Analysis: Is the scenario \"too easy\"?\n",
    "\n",
    "You asked if the current scenario is \"too easy\". Here is the scientific perspective:\n",
    "\n",
    "1.  **Convexity**: In a uniform demand scenario with a macroscopic model, the optimization surface is likely \"convex\" or \"unimodal\". This means there is a single clear optimal solution (balanced green time, i.e., FT-90s).\n",
    "2.  **\"Easy\" vs \"Fundamental\"**: It is not that the problem is \"easy\" in a trivial sense, but rather that the **optimal policy is simple**. Finding that simple optimal policy is a valid test of the RL agent.\n",
    "3.  **Complexity comes from Variance**: Real-world difficulty comes from **varying demand** (morning rush vs. night). Currently, we have constant demand.\n",
    "    *   *If we added variable demand, FT-90s would fail, and RL would likely shine.*\n",
    "    *   *But for this thesis stage, proving RL can match the theoretical optimum (FT-90s) in the base case is the necessary first step.*\n",
    "\n",
    "**Conclusion:** It is \"easy\" for a human to guess the solution (balance the lights), but it is a **perfect validation test** for the RL agent. If it couldn't solve this \"easy\" case, we couldn't trust it on hard ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4b: Quick PPO Comparison (Experimental)\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "print(\"\\nüß™ EXPERIMENTAL: Quick PPO Test (50k steps)...\")\n",
    "print(\"Checking if PPO finds a better policy faster than DQN...\")\n",
    "\n",
    "ppo_env = create_env()\n",
    "# PPO often works better with default hyperparameters than DQN\n",
    "ppo_model = PPO(\"MlpPolicy\", ppo_env, verbose=0, learning_rate=3e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "ppo_model.learn(total_timesteps=5000, progress_bar=True)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate PPO\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    obs, _ = ppo_env.reset()\n",
    "    done, ep_r = False, 0.0\n",
    "    while not done:\n",
    "        action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "        obs, r, done, _, _ = ppo_env.step(action)\n",
    "        ep_r += r\n",
    "    rewards.append(ep_r)\n",
    "\n",
    "ppo_mean = np.mean(rewards)\n",
    "imp = ((ppo_mean - baseline_results['FT-90s']['mean_reward']) / abs(baseline_results['FT-90s']['mean_reward'])) * 100\n",
    "\n",
    "print(f\"‚è±Ô∏è PPO Training Time: {train_time:.1f}s\")\n",
    "print(f\"üìä PPO Result (50k steps): {ppo_mean:.1f} | vs FT-90s: {imp:+.1f}%\")\n",
    "\n",
    "if ppo_mean > baseline_results['FT-90s']['mean_reward'] * 1.05:\n",
    "    print(\"üí° INSIGHT: PPO seems promising! Consider switching if DQN struggles.\")\n",
    "else:\n",
    "    print(\"üí° INSIGHT: PPO performs similarly or worse. Sticking with DQN is fine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Progressive Training Callback\n",
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, ref_reward, target_pct=10.0, eval_freq=1000, n_eval=3):\n",
    "        super().__init__()\n",
    "        self.eval_env, self.ref = eval_env, ref_reward\n",
    "        self.target = ref_reward * (1 + target_pct/100)\n",
    "        self.eval_freq, self.n_eval = eval_freq, n_eval\n",
    "        self.history, self.best, self.reached = [], -np.inf, False\n",
    "        \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            rewards = []\n",
    "            all_actions = []  # Track actions across episodes\n",
    "            for _ in range(self.n_eval):\n",
    "                obs, done, r = self.eval_env.reset()[0], False, 0\n",
    "                ep_actions = []\n",
    "                while not done:\n",
    "                    a, _ = self.model.predict(obs, deterministic=True)\n",
    "                    ep_actions.append(int(a))\n",
    "                    obs, rew, done, _, _ = self.eval_env.step(a)\n",
    "                    r += rew\n",
    "                rewards.append(r)\n",
    "                all_actions.append(ep_actions)\n",
    "            \n",
    "            mean_r = np.mean(rewards)\n",
    "            imp = ((mean_r - self.ref) / abs(self.ref)) * 100\n",
    "            self.history.append({'step': self.num_timesteps, 'reward': mean_r, 'improvement': imp})\n",
    "            if mean_r > self.best: self.best = mean_r\n",
    "            \n",
    "            # Analyze actions\n",
    "            avg_actions = all_actions[0]  # Use first episode as representative\n",
    "            n_switches = sum(avg_actions)\n",
    "            n_steps = len(avg_actions)\n",
    "            switch_rate = n_switches / n_steps * 100\n",
    "            \n",
    "            status = \"üéØ\" if imp >= 10 else \"\"\n",
    "            print(f\"  [{self.num_timesteps:>6}] R={mean_r:>7.1f} | vs FT-90s: {imp:>+5.1f}% | Switches: {n_switches}/{n_steps} ({switch_rate:.0f}%) {status}\")\n",
    "            \n",
    "            if imp >= 10 and not self.reached:\n",
    "                self.reached = True\n",
    "                print(f\"\\nüèÜ TARGET REACHED!\")\n",
    "        return True\n",
    "\n",
    "FT90_REF = baseline_results['FT-90s']['mean_reward']\n",
    "print(f\"‚úÖ Callback ready | Reference: FT-90s = {FT90_REF:.1f} | Target: {FT90_REF*1.1:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Initialize DQN Model - LONG RUN CONFIGURATION (1M Steps)\n",
    "train_env = create_env()\n",
    "eval_env = create_env()\n",
    "\n",
    "# SCALING FOR 1 MILLION STEPS\n",
    "# We want to explore for a significant portion, but 500k steps might be excessive.\n",
    "# Let's explore for 20% (200k steps) which is plenty.\n",
    "TOTAL_STEPS = 1_000_000\n",
    "\n",
    "model = DQN(\"MlpPolicy\", train_env, learning_rate=1e-4, buffer_size=100000, # Increased buffer\n",
    "            learning_starts=1000,\n",
    "            batch_size=64, tau=0.005, gamma=0.99,\n",
    "            exploration_fraction=0.2,  # Explore for 20% (200k steps)\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.05, # Lower final epsilon for fine-tuning\n",
    "            verbose=0, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Eval freq: 1M steps / 200 points = 5000 steps\n",
    "callback = ProgressCallback(eval_env, FT90_REF, target_pct=10.0, eval_freq=5000)\n",
    "STATE = {'model': model, 'callback': callback, 'steps': 0, 'block': 0, 'done': False}\n",
    "\n",
    "print(f\"üöÄ DQN ready on {model.device}\")\n",
    "print(f\"   Target Steps: {TOTAL_STEPS} (1 Million)\")\n",
    "print(f\"   Exploration: 20% (200k steps), final eps=0.05\")\n",
    "print(f\"   Target: Beat FT-90s by +10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463faae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Steps 0 ‚Üí 100,000\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 1: 0 ‚Üí 100k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 1, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_100k\")\n",
    "    print(f\"‚úÖ Block 1 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Steps 100k ‚Üí 200k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 2: 100k ‚Üí 200k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 2, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_200k\")\n",
    "    print(f\"‚úÖ Block 2 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Steps 200k ‚Üí 300k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 3: 200k ‚Üí 300k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 3, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_300k\")\n",
    "    print(f\"‚úÖ Block 3 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Steps 300k ‚Üí 400k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 4: 300k ‚Üí 400k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 4, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_400k\")\n",
    "    print(f\"‚úÖ Block 4 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Steps 400k ‚Üí 500k (HALFWAY)\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 5: 400k ‚Üí 500k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 5, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_500k\")\n",
    "    imp = ((STATE['callback'].best - FT90_REF) / abs(FT90_REF)) * 100\n",
    "    print(f\"üìà HALFWAY: {STATE['steps']} steps | Best improvement: {imp:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7be7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Steps 500k ‚Üí 600k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 6: 500k ‚Üí 600k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 6, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_600k\")\n",
    "    print(f\"‚úÖ Block 6 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Steps 600k ‚Üí 700k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 7: 600k ‚Üí 700k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 7, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_700k\")\n",
    "    print(f\"‚úÖ Block 7 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Steps 700k ‚Üí 800k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 8: 700k ‚Üí 800k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 8, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_800k\")\n",
    "    print(f\"‚úÖ Block 8 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af228074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Steps 800k ‚Üí 900k\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 9: 800k ‚Üí 900k\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 9, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_900k\")\n",
    "    print(f\"‚úÖ Block 9 done | Steps: {STATE['steps']} | Best: {STATE['callback'].best:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b174b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10 (FINAL): 900k ‚Üí 1M\n",
    "if STATE['done']: print(\"‚úÖ Target reached, skipping\")\n",
    "else:\n",
    "    print(\"üìä BLOCK 10 (FINAL): 900k ‚Üí 1M\")\n",
    "    STATE['model'].learn(100000, callback=STATE['callback'], reset_num_timesteps=False, progress_bar=True)\n",
    "    STATE['block'], STATE['steps'] = 10, STATE['callback'].num_timesteps\n",
    "    STATE['done'] = STATE['callback'].reached\n",
    "    STATE['model'].save(f\"{SAVE_DIR}/model_FINAL\")\n",
    "\n",
    "imp = ((STATE['callback'].best - FT90_REF) / abs(FT90_REF)) * 100\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üèÅ TRAINING COMPLETE\")\n",
    "print(f\"   Total steps: {STATE['steps']}\")\n",
    "print(f\"   Best reward: {STATE['callback'].best:.1f}\")\n",
    "print(f\"   Improvement vs FT-90s: {imp:+.1f}%\")\n",
    "print(f\"   Target (+10%): {'‚úÖ ACHIEVED' if STATE['done'] else '‚ùå NOT ACHIEVED'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cce65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final: Plot & Save Results\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "h = STATE['callback'].history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot([x['step'] for x in h], [x['reward'] for x in h], 'b-o', markersize=3)\n",
    "ax1.axhline(y=FT90_REF, color='r', linestyle='--', label='FT-90s')\n",
    "ax1.axhline(y=baseline_results['FT-30s']['mean_reward'], color='g', linestyle=':', label='FT-30s')\n",
    "ax1.set_xlabel('Steps'); ax1.set_ylabel('Reward'); ax1.legend(); ax1.set_title('Learning Curve')\n",
    "\n",
    "ax2.plot([x['step'] for x in h], [x['improvement'] for x in h], 'b-o', markersize=3)\n",
    "ax2.axhline(y=10, color='g', linestyle='--', label='Target +10%')\n",
    "ax2.axhline(y=0, color='r', linestyle='-', alpha=0.5)\n",
    "ax2.set_xlabel('Steps'); ax2.set_ylabel('Improvement (%)'); ax2.legend(); ax2.set_title('Improvement vs FT-90s')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/stage3_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save JSON\n",
    "results = {'baselines': baseline_results, 'history': h, 'best': STATE['callback'].best, \n",
    "           'improvement': ((STATE['callback'].best - FT90_REF) / abs(FT90_REF)) * 100}\n",
    "with open(f'{SAVE_DIR}/stage3_results.json', 'w') as f: json.dump(results, f, indent=2)\n",
    "print(f\"üìÅ Saved: {SAVE_DIR}/stage3_results.png, {SAVE_DIR}/stage3_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
