{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379bd6f8",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Setup - Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU CHECK COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a631e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/elonmj/Code-traffic-flow.git\"\n",
    "REPO_DIR = \"/content/Code-traffic-flow\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install stable-baselines3 gymnasium numba --quiet\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08741a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path\n",
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Verify imports work\n",
    "print(\"Testing imports...\")\n",
    "try:\n",
    "    from arz_model.config import create_victoria_island_config\n",
    "    print(\"  âœ… arz_model.config\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ arz_model.config: {e}\")\n",
    "\n",
    "try:\n",
    "    from Code_RL.src.env.traffic_signal_env_direct_v3 import TrafficSignalEnvDirectV3\n",
    "    print(\"  âœ… TrafficSignalEnvDirectV3\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ TrafficSignalEnvDirectV3: {e}\")\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import DQN\n",
    "    print(\"  âœ… stable_baselines3.DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ stable_baselines3.DQN: {e}\")\n",
    "\n",
    "print(\"\\nâœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2d303",
   "metadata": {},
   "source": [
    "## â±ï¸ Step 2: Benchmark Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPONENT TIMING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Config creation time\n",
    "print(\"\\n[1] Creating ARZ Config...\")\n",
    "t0 = time.time()\n",
    "arz_config = create_victoria_island_config(\n",
    "    t_final=450.0,\n",
    "    output_dt=15.0,\n",
    "    cells_per_100m=4,\n",
    "    default_density=80.0,\n",
    "    inflow_density=100.0,\n",
    "    use_cache=False\n",
    ")\n",
    "config_time = time.time() - t0\n",
    "print(f\"    â±ï¸ Config creation: {config_time:.2f}s\")\n",
    "\n",
    "# Add RL metadata\n",
    "arz_config.rl_metadata = {\n",
    "    'observation_segment_ids': [s.id for s in arz_config.segments],\n",
    "    'decision_interval': 15.0,\n",
    "}\n",
    "\n",
    "print(f\"    ðŸ“Š Segments: {len(arz_config.segments)}\")\n",
    "print(f\"    ðŸ“Š Nodes: {len(arz_config.nodes)}\")\n",
    "print(f\"    ðŸ“Š t_final: {arz_config.t_final}s\")\n",
    "print(f\"    ðŸ“Š dt: {arz_config.dt}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Environment creation time\n",
    "print(\"\\n[2] Creating Environment...\")\n",
    "\n",
    "# Test with ORIGINAL reward weights first\n",
    "REWARD_WEIGHTS_V1 = {'alpha': 1.0, 'kappa': 0.1, 'mu': 0.5}\n",
    "REWARD_WEIGHTS_V2 = {'alpha': 5.0, 'kappa': 0.3, 'mu': 0.1}\n",
    "\n",
    "print(f\"    Using V1 weights: {REWARD_WEIGHTS_V1}\")\n",
    "\n",
    "t0 = time.time()\n",
    "env = TrafficSignalEnvDirectV3(\n",
    "    simulation_config=arz_config,\n",
    "    decision_interval=15.0,\n",
    "    observation_segment_ids=None,\n",
    "    reward_weights=REWARD_WEIGHTS_V1,\n",
    "    quiet=False  # VERBOSE MODE for debugging\n",
    ")\n",
    "env_creation_time = time.time() - t0\n",
    "print(f\"    â±ï¸ Environment creation: {env_creation_time:.2f}s\")\n",
    "print(f\"    ðŸ“Š Observation space: {env.observation_space}\")\n",
    "print(f\"    ðŸ“Š Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reset time\n",
    "print(\"\\n[3] Testing env.reset()...\")\n",
    "\n",
    "reset_times = []\n",
    "for i in range(3):\n",
    "    t0 = time.time()\n",
    "    obs, info = env.reset()\n",
    "    reset_time = time.time() - t0\n",
    "    reset_times.append(reset_time)\n",
    "    print(f\"    Reset {i+1}: {reset_time:.3f}s\")\n",
    "\n",
    "print(f\"    â±ï¸ Average reset time: {np.mean(reset_times):.3f}s\")\n",
    "print(f\"    ðŸ“Š Observation shape: {obs.shape}\")\n",
    "print(f\"    ðŸ“Š First 10 obs values: {obs[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c07dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Step time\n",
    "print(\"\\n[4] Testing env.step() - 10 steps...\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "step_times = []\n",
    "rewards = []\n",
    "densities = []\n",
    "\n",
    "print(\"\\n    Step | Action | Reward   | Density  | Time (s)\")\n",
    "print(\"    \" + \"-\"*50)\n",
    "\n",
    "for i in range(10):\n",
    "    action = i % 2  # Alternate between 0 and 1\n",
    "    \n",
    "    t0 = time.time()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    step_time = time.time() - t0\n",
    "    \n",
    "    step_times.append(step_time)\n",
    "    rewards.append(reward)\n",
    "    density = info.get('avg_density', 0)\n",
    "    densities.append(density)\n",
    "    \n",
    "    print(f\"    {i+1:4d} | {action:6d} | {reward:8.2f} | {density:8.4f} | {step_time:.3f}\")\n",
    "    \n",
    "    if done or truncated:\n",
    "        print(f\"    Episode ended at step {i+1}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n    â±ï¸ Average step time: {np.mean(step_times):.3f}s\")\n",
    "print(f\"    ðŸ“Š Reward range: [{min(rewards):.2f}, {max(rewards):.2f}]\")\n",
    "print(f\"    ðŸ“Š Total reward (10 steps): {sum(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Full episode time\n",
    "print(\"\\n[5] Running FULL EPISODE...\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "episode_rewards = []\n",
    "episode_steps = 0\n",
    "done = truncated = False\n",
    "\n",
    "t0 = time.time()\n",
    "while not (done or truncated):\n",
    "    action = episode_steps % 2\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    episode_rewards.append(reward)\n",
    "    episode_steps += 1\n",
    "\n",
    "episode_time = time.time() - t0\n",
    "\n",
    "print(f\"    â±ï¸ Episode time: {episode_time:.2f}s\")\n",
    "print(f\"    ðŸ“Š Episode steps: {episode_steps}\")\n",
    "print(f\"    ðŸ“Š Total reward: {sum(episode_rewards):.2f}\")\n",
    "print(f\"    ðŸ“Š Average reward/step: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"    ðŸ“Š Time per step: {episode_time/episode_steps:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339faf5",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Step 3: Reward Function Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"REWARD FUNCTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare reward weights\n",
    "print(\"\\nComparing V1 vs V2 reward weights:\")\n",
    "print(f\"  V1: {REWARD_WEIGHTS_V1}\")\n",
    "print(f\"  V2: {REWARD_WEIGHTS_V2}\")\n",
    "\n",
    "# Run same sequence with both\n",
    "def run_episode_with_weights(weights, num_steps=30):\n",
    "    \"\"\"Run episode with specific weights and return rewards.\"\"\"\n",
    "    # Need to create new env with different weights\n",
    "    test_env = TrafficSignalEnvDirectV3(\n",
    "        simulation_config=arz_config,\n",
    "        decision_interval=15.0,\n",
    "        observation_segment_ids=None,\n",
    "        reward_weights=weights,\n",
    "        quiet=True\n",
    "    )\n",
    "    \n",
    "    obs, _ = test_env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        action = 0  # Always keep same phase\n",
    "        obs, reward, done, truncated, info = test_env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    test_env.close()\n",
    "    return rewards\n",
    "\n",
    "print(\"\\nRunning 30 steps with action=0 (keep phase)...\")\n",
    "\n",
    "rewards_v1 = run_episode_with_weights(REWARD_WEIGHTS_V1)\n",
    "rewards_v2 = run_episode_with_weights(REWARD_WEIGHTS_V2)\n",
    "\n",
    "print(f\"\\n  V1 Rewards: total={sum(rewards_v1):.2f}, mean={np.mean(rewards_v1):.2f}, std={np.std(rewards_v1):.2f}\")\n",
    "print(f\"  V2 Rewards: total={sum(rewards_v2):.2f}, mean={np.mean(rewards_v2):.2f}, std={np.std(rewards_v2):.2f}\")\n",
    "print(f\"\\n  Ratio V2/V1: {sum(rewards_v2)/sum(rewards_v1):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test action sensitivity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ACTION SENSITIVITY TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def run_with_action_pattern(weights, actions, label):\n",
    "    \"\"\"Run with specific action pattern.\"\"\"\n",
    "    test_env = TrafficSignalEnvDirectV3(\n",
    "        simulation_config=arz_config,\n",
    "        decision_interval=15.0,\n",
    "        reward_weights=weights,\n",
    "        quiet=True\n",
    "    )\n",
    "    \n",
    "    obs, _ = test_env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        obs, reward, done, truncated, info = test_env.step(action)\n",
    "        total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    test_env.close()\n",
    "    return total_reward\n",
    "\n",
    "# Define action patterns (30 steps)\n",
    "all_keep = [0] * 30          # Never switch\n",
    "all_switch = [1] * 30        # Always switch\n",
    "switch_every_2 = [1 if i % 2 == 0 else 0 for i in range(30)]  # Switch every 2 steps\n",
    "switch_every_5 = [1 if i % 5 == 0 else 0 for i in range(30)]  # Switch every 5 steps\n",
    "\n",
    "patterns = [\n",
    "    (all_keep, \"Always Keep (action=0)\"),\n",
    "    (all_switch, \"Always Switch (action=1)\"),\n",
    "    (switch_every_2, \"Switch every 2 steps\"),\n",
    "    (switch_every_5, \"Switch every 5 steps\"),\n",
    "]\n",
    "\n",
    "print(\"\\nWith V1 weights:\")\n",
    "for actions, label in patterns:\n",
    "    reward = run_with_action_pattern(REWARD_WEIGHTS_V1, actions, label)\n",
    "    print(f\"  {label:30s}: {reward:10.2f}\")\n",
    "\n",
    "print(\"\\nWith V2 weights:\")\n",
    "for actions, label in patterns:\n",
    "    reward = run_with_action_pattern(REWARD_WEIGHTS_V2, actions, label)\n",
    "    print(f\"  {label:30s}: {reward:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2807d",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 4: Mini DQN Training (10 timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fed918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MINI DQN TRAINING - 10 TIMESTEPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh environment\n",
    "train_env = TrafficSignalEnvDirectV3(\n",
    "    simulation_config=arz_config,\n",
    "    decision_interval=15.0,\n",
    "    reward_weights=REWARD_WEIGHTS_V1,\n",
    "    quiet=True\n",
    ")\n",
    "\n",
    "print(\"\\nCreating DQN model...\")\n",
    "t0 = time.time()\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=1000,\n",
    "    learning_starts=5,  # Start learning after 5 steps\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    target_update_interval=10,\n",
    "    exploration_fraction=0.5,\n",
    "    exploration_final_eps=0.1,\n",
    "    verbose=2  # Maximum verbosity\n",
    ")\n",
    "model_creation_time = time.time() - t0\n",
    "print(f\"â±ï¸ Model creation: {model_creation_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Starting training for 10 timesteps...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "t0 = time.time()\n",
    "model.learn(total_timesteps=10)\n",
    "training_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nâ±ï¸ Training time for 10 timesteps: {training_time:.2f}s\")\n",
    "print(f\"â±ï¸ Time per timestep: {training_time/10:.2f}s\")\n",
    "\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate training time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING TIME EXTRAPOLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "time_per_step = training_time / 10\n",
    "\n",
    "print(f\"\\nBased on {training_time:.2f}s for 10 timesteps:\")\n",
    "print(f\"  - 1,000 timesteps: {time_per_step * 1000 / 60:.1f} minutes\")\n",
    "print(f\"  - 10,000 timesteps: {time_per_step * 10000 / 60:.1f} minutes\")\n",
    "print(f\"  - 50,000 timesteps: {time_per_step * 50000 / 60:.1f} minutes ({time_per_step * 50000 / 3600:.1f} hours)\")\n",
    "print(f\"  - 100,000 timesteps: {time_per_step * 100000 / 60:.1f} minutes ({time_per_step * 100000 / 3600:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a448ebc",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 5: Full Debug Run (100 timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FULL DEBUG RUN - 100 TIMESTEPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for detailed debugging.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.step_times = []\n",
    "        self.last_time = None\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Track time\n",
    "        if self.last_time is not None:\n",
    "            self.step_times.append(time.time() - self.last_time)\n",
    "        self.last_time = time.time()\n",
    "        \n",
    "        # Track reward\n",
    "        reward = self.locals.get('rewards', [0])[0]\n",
    "        self.current_episode_reward += reward\n",
    "        \n",
    "        # Check if episode done\n",
    "        dones = self.locals.get('dones', [False])\n",
    "        if dones[0]:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            print(f\"  Episode {len(self.episode_rewards)}: reward={self.current_episode_reward:.2f}\")\n",
    "            self.current_episode_reward = 0\n",
    "        \n",
    "        # Print every 20 steps\n",
    "        if self.num_timesteps % 20 == 0:\n",
    "            avg_time = np.mean(self.step_times[-20:]) if self.step_times else 0\n",
    "            print(f\"  Step {self.num_timesteps}: avg_step_time={avg_time:.3f}s\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Create environment and model\n",
    "debug_env = TrafficSignalEnvDirectV3(\n",
    "    simulation_config=arz_config,\n",
    "    decision_interval=15.0,\n",
    "    reward_weights=REWARD_WEIGHTS_V1,\n",
    "    quiet=True\n",
    ")\n",
    "\n",
    "debug_model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    debug_env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=1000,\n",
    "    learning_starts=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "callback = DebugCallback()\n",
    "\n",
    "print(\"\\nStarting 100 timesteps training with debug callback...\\n\")\n",
    "t0 = time.time()\n",
    "debug_model.learn(total_timesteps=100, callback=callback)\n",
    "total_time = time.time() - t0\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUG RUN SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Time per timestep: {total_time/100:.3f}s\")\n",
    "print(f\"Episodes completed: {len(callback.episode_rewards)}\")\n",
    "if callback.episode_rewards:\n",
    "    print(f\"Episode rewards: {callback.episode_rewards}\")\n",
    "    print(f\"Mean episode reward: {np.mean(callback.episode_rewards):.2f}\")\n",
    "\n",
    "debug_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7bf93",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 6: Evaluation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a190ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATION TEST - Baseline vs Trained Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_policy(env, model=None, policy_type='fixed_time', n_episodes=3, fixed_time_interval=30.0):\n",
    "    \"\"\"Evaluate a policy.\"\"\"\n",
    "    results = {'rewards': [], 'densities': []}\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = truncated = False\n",
    "        ep_reward = 0\n",
    "        ep_densities = []\n",
    "        time_since_switch = 0.0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            if policy_type == 'model':\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                # Fixed time\n",
    "                time_since_switch += env.decision_interval\n",
    "                if time_since_switch >= fixed_time_interval:\n",
    "                    action = 1\n",
    "                    time_since_switch = 0.0\n",
    "                else:\n",
    "                    action = 0\n",
    "            \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if 'avg_density' in info:\n",
    "                ep_densities.append(info['avg_density'])\n",
    "        \n",
    "        results['rewards'].append(ep_reward)\n",
    "        results['densities'].append(np.mean(ep_densities) if ep_densities else 0)\n",
    "        print(f\"  Episode {ep+1}: reward={ep_reward:.2f}, density={results['densities'][-1]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(results['rewards']),\n",
    "        'std_reward': np.std(results['rewards']),\n",
    "        'mean_density': np.mean(results['densities'])\n",
    "    }\n",
    "\n",
    "# Evaluate baseline\n",
    "print(\"\\n[1] Evaluating BASELINE (fixed-time 30s)...\")\n",
    "eval_env = TrafficSignalEnvDirectV3(\n",
    "    simulation_config=arz_config,\n",
    "    decision_interval=15.0,\n",
    "    reward_weights=REWARD_WEIGHTS_V1,\n",
    "    quiet=True\n",
    ")\n",
    "\n",
    "baseline_results = evaluate_policy(eval_env, policy_type='fixed_time', n_episodes=3)\n",
    "print(f\"\\nBaseline: mean={baseline_results['mean_reward']:.2f} Â± {baseline_results['std_reward']:.2f}\")\n",
    "\n",
    "# Evaluate trained model\n",
    "print(\"\\n[2] Evaluating TRAINED MODEL (100 timesteps)...\")\n",
    "model_results = evaluate_policy(eval_env, model=debug_model, policy_type='model', n_episodes=3)\n",
    "print(f\"\\nModel: mean={model_results['mean_reward']:.2f} Â± {model_results['std_reward']:.2f}\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "improvement = (model_results['mean_reward'] - baseline_results['mean_reward']) / abs(baseline_results['mean_reward']) * 100\n",
    "print(f\"Baseline: {baseline_results['mean_reward']:.2f}\")\n",
    "print(f\"Model:    {model_results['mean_reward']:.2f}\")\n",
    "print(f\"Change:   {improvement:+.2f}%\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc34e1",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Step 7: Summary & Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š TIMING ANALYSIS:\")\n",
    "print(f\"  - Config creation: {config_time:.2f}s\")\n",
    "print(f\"  - Environment creation: {env_creation_time:.2f}s\")\n",
    "print(f\"  - Average reset time: {np.mean(reset_times):.3f}s\")\n",
    "print(f\"  - Average step time: {np.mean(step_times):.3f}s\")\n",
    "print(f\"  - Full episode time: {episode_time:.2f}s ({episode_steps} steps)\")\n",
    "\n",
    "print(\"\\nðŸ¤– TRAINING ANALYSIS:\")\n",
    "print(f\"  - 100 timesteps took: {total_time:.2f}s\")\n",
    "print(f\"  - Time per timestep: {total_time/100:.3f}s\")\n",
    "print(f\"  - Extrapolated 50k: {total_time/100 * 50000 / 60:.1f} minutes\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ REWARD ANALYSIS:\")\n",
    "print(f\"  - Baseline reward: {baseline_results['mean_reward']:.2f}\")\n",
    "print(f\"  - Model reward (100 steps): {model_results['mean_reward']:.2f}\")\n",
    "print(f\"  - Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\nâš ï¸ POTENTIAL ISSUES:\")\n",
    "if total_time/100 > 1.0:\n",
    "    print(f\"  - SLOW TRAINING: {total_time/100:.2f}s per timestep is too slow!\")\n",
    "if baseline_results['std_reward'] < 1.0:\n",
    "    print(f\"  - LOW VARIANCE: std={baseline_results['std_reward']:.2f} suggests deterministic environment\")\n",
    "if abs(improvement) < 5:\n",
    "    print(f\"  - NO LEARNING: Only {improvement:+.2f}% improvement after training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEBUG NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
