"""
Analyse des √©v√©nements TensorBoard g√©n√©r√©s par les entra√Ænements RL.
"""
import os
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import json

# R√©pertoire TensorBoard
TB_DIR = 'validation_output/results/elonmj_arz-validation-76rlperformance-pmrk/section_7_6_rl_performance/data/models/tensorboard/'

print("=" * 80)
print("ANALYSE DES √âV√âNEMENTS TENSORBOARD")
print("=" * 80)

# Lister les runs
if not os.path.exists(TB_DIR):
    print(f"\n‚ùå ERROR: Directory not found: {TB_DIR}")
    exit(1)

runs = [d for d in os.listdir(TB_DIR) if os.path.isdir(os.path.join(TB_DIR, d))]
print(f"\nüìÇ RUNS D√âTECT√âS: {len(runs)}")
for run in runs:
    print(f"   - {run}")

# Analyser chaque run
results = {}

for run_name in runs:
    print(f"\n{'=' * 80}")
    print(f"RUN: {run_name}")
    print(f"{'=' * 80}")
    
    run_dir = os.path.join(TB_DIR, run_name)
    
    # Trouver le fichier d'√©v√©nements
    event_files = [f for f in os.listdir(run_dir) if f.startswith('events')]
    if not event_files:
        print(f"  ‚ö†Ô∏è  No event files found")
        continue
    
    event_file = event_files[0]
    event_path = os.path.join(run_dir, event_file)
    
    print(f"\nüìÑ Event file: {event_file}")
    
    # Charger avec EventAccumulator
    ea = EventAccumulator(event_path)
    ea.Reload()
    
    # Afficher les tags disponibles
    tags = ea.Tags()
    print(f"\nüìä Tags disponibles:")
    print(f"   - Scalars: {len(tags['scalars'])} tags")
    print(f"   - Images: {len(tags['images'])} tags")
    print(f"   - Histograms: {len(tags['histograms'])} tags")
    
    # Analyser les scalars
    if tags['scalars']:
        print(f"\nüìà SCALAR METRICS:")
        results[run_name] = {}
        
        for tag in tags['scalars']:
            scalar_events = ea.Scalars(tag)
            print(f"\n   üîπ {tag}:")
            print(f"      Nombre de points: {len(scalar_events)}")
            
            if scalar_events:
                values = [e.value for e in scalar_events]
                steps = [e.step for e in scalar_events]
                
                print(f"      Premier step: {steps[0]}")
                print(f"      Dernier step: {steps[-1]}")
                print(f"      Premi√®re valeur: {values[0]:.4f}")
                print(f"      Derni√®re valeur: {values[-1]:.4f}")
                
                # Sauvegarder pour analyse
                results[run_name][tag] = {
                    'steps': steps,
                    'values': values,
                    'num_points': len(scalar_events)
                }
                
                # Afficher tous les points si peu de donn√©es
                if len(scalar_events) <= 5:
                    print(f"      Tous les points:")
                    for e in scalar_events:
                        print(f"         Step {e.step}: {e.value:.6f}")
    else:
        print("   ‚ö†Ô∏è  No scalar metrics found")

# R√©sum√© comparatif
print(f"\n{'=' * 80}")
print("R√âSUM√â COMPARATIF DES 3 RUNS")
print(f"{'=' * 80}")

if results:
    # Cr√©er un tableau comparatif
    print(f"\n{'Metric':<30} | {'PPO_1':<15} | {'PPO_2':<15} | {'PPO_3':<15}")
    print("-" * 80)
    
    # Pour chaque m√©trique commune
    all_metrics = set()
    for run_data in results.values():
        all_metrics.update(run_data.keys())
    
    for metric in sorted(all_metrics):
        row = f"{metric:<30}"
        for run_name in ['PPO_1', 'PPO_2', 'PPO_3']:
            if run_name in results and metric in results[run_name]:
                data = results[run_name][metric]
                final_value = data['values'][-1] if data['values'] else 0.0
                row += f" | {final_value:>15.4f}"
            else:
                row += f" | {'N/A':>15}"
        print(row)

# Sauvegarder les r√©sultats
output_file = "tensorboard_analysis.json"
with open(output_file, 'w') as f:
    # Convertir pour JSON (enlever les numpy arrays)
    json_results = {}
    for run_name, run_data in results.items():
        json_results[run_name] = {}
        for metric, metric_data in run_data.items():
            json_results[run_name][metric] = {
                'steps': metric_data['steps'],
                'values': metric_data['values'],
                'num_points': metric_data['num_points']
            }
    json.dump(json_results, f, indent=2)

print(f"\n‚úÖ Analyse compl√®te sauvegard√©e dans: {output_file}")

# Interpr√©tation
print(f"\n{'=' * 80}")
print("INTERPR√âTATION")
print(f"{'=' * 80}")

print("""
üìå QUICK TEST (2 timesteps) - LIMITES:

Les 3 runs ont √©t√© ex√©cut√©s avec QUICK_TEST=1 (seulement 2 timesteps).
Cela signifie qu'il n'y a pratiquement AUCUNE donn√©e d'apprentissage.

üîç Ce que montre chaque m√©trique:

‚Ä¢ rollout/ep_rew_mean: R√©compense moyenne par √©pisode
  ‚Üí Avec 2 timesteps, c'est juste l'initialisation (pas d'apprentissage)
  
‚Ä¢ rollout/ep_len_mean: Longueur moyenne des √©pisodes
  ‚Üí Devrait √™tre ~2 (les 2 timesteps du quick test)
  
‚Ä¢ time/fps: Vitesse d'entra√Ænement (frames per second)
  ‚Üí Indicateur de performance du syst√®me (GPU vs CPU)

‚ö†Ô∏è  POUR VOIR L'APPRENTISSAGE R√âEL:

Vous devez lancer un entra√Ænement COMPLET (20,000+ timesteps) pour observer:
  - Convergence de la r√©compense
  - Am√©lioration progressive
  - Stabilisation du policy

üí° UTILISATION DE TENSORBOARD UI:

Pour visualiser graphiquement:
  tensorboard --logdir=validation_output/results/.../tensorboard/
  
Puis ouvrir: http://localhost:6006
""")

print("\n‚úÖ Analyse termin√©e!")
