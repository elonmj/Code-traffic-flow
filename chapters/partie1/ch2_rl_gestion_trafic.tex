\chapter{L'Apprentissage par Renforcement pour la Gestion du Trafic : Application aux Contextes Urbains Ouest-Africains}
\label{chap:rl_gestion_trafic}

\section{Introduction}
\label{sec:rl_intro}

La gestion du trafic dans les centres urbains d'Afrique de l'Ouest, comme Cotonou (Bénin) et Lagos (Nigeria), est confrontée à des défis uniques dus à l'urbanisation rapide, à l'augmentation du nombre de véhicules, et à une hétérogénéité extrême du parc roulant, dominé par les motos (Zémidjans). Les comportements spécifiques, tels que le \textit{gap-filling}, l'\textit{interweaving}, et le \textit{creeping}, combinés à des infrastructures routières souvent dégradées, rendent les approches traditionnelles de contrôle des feux de signalisation, basées sur des cycles fixes ou des règles prédéfinies, inadéquates. L'apprentissage par renforcement (RL) offre une solution prometteuse en permettant à un agent de contrôle des feux d'apprendre dynamiquement des stratégies optimales par interaction avec l'environnement, maximisant des objectifs comme la réduction des temps d'attente et l'amélioration de la fluidité du trafic. Ce chapitre explore les principes fondamentaux du RL, sa formalisation mathématique, les algorithmes pertinents, et son application au contrôle des feux de signalisation, en mettant l'accent sur les adaptations nécessaires pour les contextes ouest-africains, où les données sont limitées et les dynamiques de trafic complexes.

\section{Principes de l'Apprentissage par Renforcement (RL)}
\label{sec:rl_principes}

L'apprentissage par renforcement est une branche de l'intelligence artificielle où un agent apprend à prendre des décisions optimales par essais et erreurs, en interagissant avec un environnement. Dans le contexte de la gestion du trafic, l'agent est le système de contrôle des feux, l'environnement est le réseau routier, et l'objectif est d'optimiser des métriques telles que la réduction des files d'attente ou des émissions de CO2.

Les composants clés du RL sont :
\begin{itemize}
    \item \textbf{Agent} : Le contrôleur des feux de signalisation, qui décide des phases (vert, rouge) ou des durées.
    \item \textbf{Environnement} : Le réseau routier, incluant les véhicules, les intersections, et les conditions de trafic (densité, vitesse, comportements des motos).
    \item \textbf{État (s)} : Une représentation de l'environnement à un instant donné, par exemple, la longueur des files d'attente, la densité des motos, ou la vitesse moyenne des véhicules.
    \item \textbf{Action (a)} : Une décision, comme prolonger un feu vert ou changer de phase (par exemple, Nord-Sud vert ou Est-Ouest gauche).
    \item \textbf{Récompense (r)} : Un signal numérique évaluant l'action, par exemple, une récompense positive pour réduire le temps d'attente ou négative pour une congestion accrue.
    \item \textbf{Politique ($\pi$)} : La stratégie de l'agent pour sélectionner des actions en fonction des états, visant à maximiser la récompense cumulée à long terme.
\end{itemize}

Le RL est particulièrement adapté aux environnements dynamiques et stochastiques, comme le trafic ouest-africain, où les variations imprévisibles (par exemple, pics de motos aux heures de pointe) nécessitent une adaptabilité en temps réel.

\subsection{Formalisation en Processus de Décision Markovien (MDP)}
\label{subsec:rl_mdp}

Le RL est formalisé à l'aide d'un Processus de Décision Markovien (MDP), défini par le tuple $(S, A, P, R, \gamma)$, où :
\begin{itemize}
    \item \textbf{S} : Ensemble des états possibles, représentant les conditions de trafic (par exemple, densité par voie, phase actuelle des feux).
    \item \textbf{A} : Ensemble des actions possibles, comme changer la phase des feux ou ajuster la durée d'un cycle.
    \item \textbf{P(s' | s, a)} : Probabilité de transition vers l'état $s'$ après avoir pris l'action $a$ dans l'état $s$, capturant la dynamique stochastique du trafic.
    \item \textbf{R(s, a, s')} : Récompense immédiate reçue après la transition, souvent définie comme l'inverse du temps d'attente ou des émissions.
    \item \textbf{$\gamma$} : Facteur d'actualisation ($0 \leq \gamma < 1$), qui pondère l'importance des récompenses futures par rapport aux immédiates.
\end{itemize}

La \textbf{propriété de Markov} suppose que l'état futur dépend uniquement de l'état actuel et de l'action prise, ce qui simplifie la modélisation mais peut nécessiter des ajustements pour capturer les comportements complexes des motos (par exemple, l'\textit{interweaving} influencé par les conditions passées). L'objectif est de trouver une politique optimale $\pi^*$ qui maximise la valeur attendue des récompenses cumulées, exprimée par la fonction de valeur $V(s)$ ou la fonction d'action-valeur $Q(s, a)$, résolues via les équations de Bellman.

Dans les contextes ouest-africains, la définition des états et des récompenses doit intégrer des spécificités locales, comme la proportion élevée de motos et leurs comportements dynamiques. Par exemple, l'état pourrait inclure la densité des motos par voie, et la récompense pourrait pénaliser les congestions tout en favorisant les mouvements des deux-roues.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\textwidth]{placeholder_mdp.png}
%     \caption{Représentation d'un Processus de Décision Markovien (MDP) pour le contrôle de trafic}
%     \label{fig:mdp}
% \end{figure}

\subsection{Algorithmes Pertinents pour le Contrôle de Trafic}
\label{subsec:rl_algos}

Plusieurs algorithmes RL ont été appliqués au contrôle des feux de signalisation, chacun avec des forces et des faiblesses adaptées à différents contextes. Voici les plus pertinents, avec une analyse de leur applicabilité en Afrique de l'Ouest :

\begin{enumerate}
    \item \textbf{Q-Learning} :  
        \begin{itemize}
            \item \textbf{Description} : Algorithme sans modèle basé sur la valeur, qui apprend une fonction $Q(s, a)$ estimant la récompense cumulée pour chaque paire état-action. Il est simple et efficace pour des espaces d'états discrets.
            \item \textbf{Applications} : Utilisé dans des études comme celle de \cite{traffic2020signal}, où Q-learning a optimisé les feux pour maximiser le débit des véhicules à une intersection.
            \item \textbf{Pertinence pour l'Afrique de l'Ouest} : Convient pour des intersections simples, mais limité pour les réseaux complexes avec des états de haute dimension (par exemple, trafic multi-classe avec motos).
            \item \textbf{Limites} : Convergence lente dans des environnements à grand espace d'états, ce qui peut poser problème avec la variabilité du trafic ouest-africain.
        \end{itemize}

    \item \textbf{Deep Q-Networks (DQN)} :  
        \begin{itemize}
            \item \textbf{Description} : Utilise un réseau de neurones pour approximer la fonction $Q(s, a)$, permettant de gérer des espaces d'états continus ou complexes. Une étude récente \cite{reinforcement2024approach} a montré une réduction de 49\,\% des longueurs de files d'attente grâce à un DQN appliqué à une intersection.
            \item \textbf{Applications} : \cite{reinforcement2024approach} a démontré des améliorations significatives dans la gestion des files d'attente et des incitations par voie.
            \item \textbf{Pertinence pour l'Afrique de l'Ouest} : Adapté pour modéliser des comportements complexes comme le \textit{gap-filling} des motos, en utilisant des données de capteurs ou des simulations ARZ comme environnement d'entraînement.
            \item \textbf{Limites} : Nécessite de grandes quantités de données pour l'entraînement, un défi dans des contextes à données limitées.
        \end{itemize}

    \item \textbf{Algorithmes Acteur-Critique (Actor-Critic)} :  
        \begin{itemize}
            \item \textbf{Description} : Combine l'apprentissage de la politique (acteur) et de la valeur (critique). Des variantes comme A2C et A3C offrent une stabilité et une efficacité accrues. Une étude \cite{cooperative2019traffic} a utilisé un algorithme A2C pour coordonner les feux dans un réseau urbain, réduisant les temps de trajet.
            \item \textbf{Applications} : \cite{cooperative2019traffic} a montré des résultats prometteurs pour la coordination multi-intersections.
            \item \textbf{Pertinence pour l'Afrique de l'Ouest} : Convient pour des réseaux multi-intersections, comme le corridor Victoria Island à Lagos, où la coordination est essentielle. Les algorithmes Acteur-Critique peuvent intégrer des données hétérogènes (motos, voitures) via des états bien définis.
            \item \textbf{Limites} : Complexité computationnelle élevée, nécessitant des ressources robustes pour l'entraînement.
        \end{itemize}

    \item \textbf{Apprentissage par Renforcement Multi-Agents (MARL)} :  
        \begin{itemize}
            \item \textbf{Description} : Chaque intersection est contrôlée par un agent indépendant qui coopère avec d'autres pour optimiser le réseau global. Une étude \cite{multiagent2023reinforcement} a réduit la consommation de carburant de 11\,\% et le temps de trajet de 13\,\% en utilisant un MARL basé sur DQN.
            \item \textbf{Applications} : \cite{multiagent2023reinforcement} propose une approche multi-agents avec un nouveau schéma de récompense, améliorant la coordination entre intersections.
            \item \textbf{Pertinence pour l'Afrique de l'Ouest} : Idéal pour les réseaux urbains complexes, comme à Lagos, où les intersections interagissent fortement. Le MARL peut être adapté pour prioriser les motos dans les politiques de contrôle.
            \item \textbf{Limites} : Nécessite une communication efficace entre agents, ce qui peut être difficile avec des infrastructures technologiques limitées.
        \end{itemize}

    \item \textbf{Deep Reinforcement Learning avec Architectures Avancées} :  
        \begin{itemize}
            \item \textbf{Description} : Intègre des réseaux neuronaux graphiques (GNN) ou des mécanismes d'attention pour modéliser les relations spatiales-temporelles. Une revue \cite{survey2025reinforcement} souligne leur potentiel pour la coordination à grande échelle.
            \item \textbf{Applications} : \cite{survey2025reinforcement} discute des approches MADRL (Multi-Agent Deep RL) pour des réseaux complexes.
            \item \textbf{Pertinence pour l'Afrique de l'Ouest} : Les GNN peuvent modéliser les interactions complexes entre motos et voitures, mais leur mise en œuvre nécessite des simulations robustes, comme votre jumeau numérique ARZ.
            \item \textbf{Limites} : Exige des ressources computationnelles élevées et des données détaillées pour la calibration.
        \end{itemize}
\end{enumerate}

\begin{table}[ht]
    \centering
    \caption{Comparaison des algorithmes RL pour le contrôle de trafic}
    \label{tab:algo_comparison}
    \begin{tabular}{llll}
        \toprule
        \textbf{Algorithme} & \textbf{Avantages} & \textbf{Inconvénients} & \textbf{Pertinence Afrique de l'Ouest} \\
        \midrule
        Q-Learning & Simple, efficace pour états discrets & Convergence lente & Faible \\
        DQN & Gère états continus & Besoin de grandes données & Moyen \\
        Actor-Critic & Stable, efficace & Complexité computationnelle & Élevé \\
        MARL & Coordination multi-intersections & Communication entre agents & Élevé \\
        GNN & Modélise interactions complexes & Ressources élevées & Moyen \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Applications du RL au Contrôle des Feux de Signalisation}
\label{sec:rl_applications}

Le RL a transformé la gestion des feux de signalisation en permettant une adaptation en temps réel aux conditions de trafic, contrairement aux méthodes traditionnelles comme le contrôle à temps fixe (FTC) ou inductif. Les études récentes montrent des résultats significatifs :
\begin{itemize}
    \item \textbf{Réduction des temps de trajet et des émissions} : \cite{reinforcement2024approach} a utilisé un DQN pour réduire les longueurs de files d'attente de 49\,\% et augmenter les incitations par voie de 9\,\%, démontrant l'efficacité du RL pour améliorer l'efficacité et la durabilité.
    \item \textbf{Coordination multi-intersections} : \cite{multiagent2023reinforcement} a réduit la consommation de carburant de 11\,\% et le temps de trajet de 13\,\% dans un réseau simulé, soulignant le potentiel pour des réseaux complexes comme ceux de Lagos.
    \item \textbf{Adaptabilité aux dynamiques complexes} : \cite{survey2025reinforcement} note que les approches RL, notamment avec des architectures comme les GNN, excellent dans la gestion des environnements à haute variabilité, pertinents pour les contextes ouest-africains.
\end{itemize}


\section{Conclusion du Chapitre 2}
\label{sec:rl_conclusion}

L'apprentissage par renforcement offre un cadre puissant pour optimiser les feux de signalisation dans les contextes urbains ouest-africains, grâce à son adaptabilité et sa capacité à gérer des dynamiques complexes. Les algorithmes comme DQN, Acteur-Critique, et MARL, soutenus par des études récentes, montrent des résultats prometteurs pour réduire la congestion et améliorer la durabilité. Cependant, leur application à des villes comme Cotonou et Lagos nécessite des adaptations pour capturer l'hétérogénéité du trafic, surmonter les contraintes de données, et intégrer les spécificités infrastructurelles. En combinant RL avec un jumeau numérique basé sur le modèle ARZ, votre recherche comble une lacune importante en proposant une approche adaptée aux réalités ouest-africaines, avec un potentiel de transposition régionale via des méthodes comme l'apprentissage fédéré. Les études citées, majoritairement post-2020, fournissent une base solide pour votre revue de littérature, tout en soulignant la nécessité de validations locales pour garantir l'efficacité des modèles.
