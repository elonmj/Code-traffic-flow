\chapter{Conception et Implémentation de l'Environnement d'Apprentissage par Renforcement}
\label{chap:conception_env_rl}

\section{Introduction}
\label{sec:intro_conception_rl}

Ce chapitre décrit la transformation du jumeau numérique calibré, dont la construction a été détaillée au Chapitre~\ref{chap:construction_jn}, en un environnement standardisé et exploitable pour l'entraînement d'agents d'apprentissage par renforcement (RL). L'objectif est de définir précisément le Processus de Décision Markovien (MDP) associé au contrôle des feux dans le corridor de Victoria Island, d'expliquer l'implémentation d'une interface conforme au standard Gymnasium, et de détailler la validation de l'environnement ainsi que les métriques utilisées pour l'entraînement et l'évaluation.

L'approche par apprentissage par renforcement est particulièrement pertinente ici, car elle permet d'apprendre des politiques de contrôle adaptatives en interaction directe avec notre simulateur ARZ étendu. Ce dernier est capable de représenter l'hétérogénéité véhicules/motos, les effets d'infrastructure et les phénomènes de \textit{creeping} et \textit{interweaving} observés dans les contextes ouest-africains. Les études récentes \parencite{reinforcement2024approach, cooperative2019traffic} montrent de forts gains du RL sur le contrôle des feux. Notre jumeau ARZ fournit l'environnement hautement non-linéaire nécessaire pour entraîner des politiques robustes.

La structure de ce chapitre suit une progression logique : nous commencerons par la formalisation mathématique du problème via le cadre MDP. Nous détaillerons ensuite son implémentation technique via l'interface Gymnasium. Enfin, nous conclurons par la présentation du protocole de validation de l'environnement, garantissant sa fiabilité pour les expérimentations qui seront menées au Chapitre~\ref{chap:entrainement_agent}.

\section{Formalisation du Problème en Processus de Décision Markovien (MDP)}
\label{sec:formalisation_mdp}

Un Processus de Décision Markovien est défini par le quintuplet $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$.
Nous allons maintenant instancier chacun de ces composants dans le contexte spécifique du contrôle des feux de signalisation du corridor de Victoria Island.

\subsection{Espace d'États $\mathcal{S}$}
\label{subsec:espace_etats}

\paragraph{Variables choisies et justification.}
L'état exploite des grandeurs macroscopiques extraites du simulateur ARZ (densités $\rho$, vitesses $v$). La justification est la suivante : le modèle ARZ multi-classes capture la dynamique hors-équilibre et conserve la masse, de sorte que ces grandeurs sont physiquement pertinentes et suffisantes pour décrire l'état du trafic nécessaire au contrôle des feux.

\paragraph{Prise en compte de l'hétérogénéité.}
Nous distinguons au minimum deux classes : les motocyclettes (classe $m$) et les autres véhicules (classe $c$). Pour chaque segment de route $i$ retenu dans l'observation, nous mesurons le vecteur d'état local $\{\rho_{m,i}, v_{m,i}, \rho_{c,i}, v_{c,i}\}$. Des variables auxiliaires comme la longueur de file estimée $L_i$ et la phase de feu courante (encodée en \textit{one-hot}) sont également incluses.

\paragraph{Structure finale du vecteur d'observation.}
Soit $N_{up}$ le nombre de segments en amont et $N_{down}$ le nombre de segments en aval d'une intersection retenus pour l'observation. L'observation $o_t \in \mathcal{S}$ à l'instant $t$ est la concaténation normalisée des états locaux :
\begin{equation}
o_t = \mathrm{concat}\big( \underbrace{(\rho_{m,i}/\rho^{\max}_m, v_{m,i}/v^{free}_m, \rho_{c,i}/\rho^{\max}_c, v_{c,i}/v^{free}_c)}_{i \in \{1, ..., N_{up}+N_{down}\}}, \underbrace{\text{phase\_onehot}}_{\text{dim } N_{phases}} \big).
\end{equation}
La normalisation par la densité maximale $\rho^{\max}$ et la vitesse en flux libre $v^{free}$ est une pratique standard qui stabilise l'entraînement des réseaux de neurones en ramenant toutes les entrées à une échelle comparable.

\paragraph{Paramètres de normalisation.}
Pour normaliser les observations dans l'intervalle $[0, 1]$, nous utilisons des valeurs de référence adaptées au contexte ouest-africain et calibrées sur les données de Lagos. Ces paramètres sont distincts pour chaque classe de véhicules afin de refléter fidèlement leurs comportements spécifiques :

\begin{itemize}
    \item $\rho^{\max}_m = 300$ veh/km : densité de saturation pour les motocyclettes, significativement plus élevée que pour les voitures en raison de leur capacité à se faufiler et à occuper moins d'espace sur la chaussée.
    \item $\rho^{\max}_c = 150$ veh/km : densité de saturation pour les voitures et autres véhicules.
    \item $v^{free}_m = 40$ km/h : vitesse libre typique des motocyclettes en zone urbaine dense.
    \item $v^{free}_c = 50$ km/h : vitesse libre typique des voitures en zone urbaine.
\end{itemize}

Ces valeurs permettent de traduire les variables physiques du simulateur ARZ en observations adimensionnelles comprises entre 0 et 1, respectant l'hétérogénéité du trafic mixte motos-voitures caractéristique de l'Afrique de l'Ouest.

\subsection{Espace d'Actions $\mathcal{A}$}
\label{subsec:espace_actions}

\paragraph{Choix et justification.} Pour cette étude, nous adoptons un espace d'actions discret et simple, favorisant la robustesse et une convergence plus rapide de l'apprentissage. Pour chaque carrefour, l'agent peut choisir parmi deux actions :
\begin{itemize}
    \item $a=0$ : \textbf{Maintenir la phase actuelle}.
    \item $a=1$ : \textbf{Passer à la phase suivante} du cycle de feux.
\end{itemize}
Cette approche permet à l'agent d'apprendre un \textit{timing} de décision intelligent sans complexifier l'architecture de sortie du réseau de neurones, ce qui est cohérent avec les algorithmes de type DQN que nous prévoyons d'utiliser.

\paragraph{Intervalle de décision.} Il est crucial de distinguer le pas de temps de la simulation physique $\Delta t_{sim}$ (gouverné par la condition CFL du solveur WENO, typiquement $\leq 1s$) du pas de temps de décision de l'agent $\Delta t_{dec}$. Nous fixons ce dernier à $\Delta t_{dec}=10$ secondes. Ce choix représente un compromis entre la réactivité nécessaire en milieu urbain et la stabilité de la politique, en évitant des décisions trop fréquentes qui pourraient induire des oscillations.

\subsection{Fonction de Récompense $R$}
\label{subsec:fonction_recompense}

\paragraph{Objectif de l'optimisation.} La récompense est le signal qui guide l'apprentissage. Notre objectif principal est de minimiser la congestion, ce qui se traduit par la minimisation du temps d'attente total des véhicules.

\paragraph{Formulation mathématique.} À chaque pas de décision $t$, la récompense $R_t$ est calculée comme une somme pondérée de plusieurs métriques.
Elle se décompose en trois termes principaux :
\begin{itemize}
    \item \textbf{Un terme de congestion ($R_{congestion}$)}, qui pénalise le nombre de véhicules à l'arrêt.
    \item \textbf{Un terme de stabilité ($R_{stabilite}$)}, qui pénalise les changements de phase trop fréquents.
    \item \textbf{Un terme de fluidité ($R_{fluidite}$)}, qui encourage le flux sortant.
\end{itemize}
La récompense totale est donnée par l'équation :
\begin{equation}
R_t = R_{congestion} + R_{stabilite} + R_{fluidite}
\end{equation}
avec :
\begin{align}
R_{congestion} &= - \alpha \sum_{i \in \text{approches}} (\rho_{m,i} + \rho_{c,i}) \cdot \Delta x \\
R_{stabilite} &= - \kappa \cdot \mathbb{I}(\text{action} = \text{changer\_phase}) \\
R_{fluidite} &= + \mu \cdot F_{out, t}
\end{align}

où $\alpha, \kappa, \mu$ sont des coefficients de pondération positifs, $\mathbb{I}(\cdot)$ est la fonction indicatrice, et $F_{out, t}$ est le débit sortant total durant l'intervalle $\Delta t_{dec}$.

\paragraph{Choix des coefficients de pondération.}
Les coefficients de la fonction de récompense ont été déterminés empiriquement après une phase d'expérimentation préliminaire pour équilibrer les trois objectifs concurrents. Les valeurs retenues sont :

\begin{table}[h]
\centering
\begin{tabular}{lcp{8cm}}
\toprule
\textbf{Coefficient} & \textbf{Valeur} & \textbf{Justification} \\
\midrule
$\alpha$ & 1.0 & Poids unitaire donnant la priorité principale à la réduction de congestion, objectif primordial du contrôle. \\
$\kappa$ & 0.1 & Pénalité modérée pour limiter les changements fréquents de phase sans trop contraindre l'agent, permettant une flexibilité dans les décisions. \\
$\mu$ & 0.5 & Récompense modérée pour le débit, encourageant la fluidité du trafic sans sacrifier l'objectif principal de réduction de congestion. \\
\bottomrule
\end{tabular}
\caption{Coefficients de pondération de la fonction de récompense.}
\label{tab:reward_weights}
\end{table}

Le ratio $\alpha : \kappa : \mu = 1 : 0.1 : 0.5$ garantit que la réduction de congestion reste l'objectif principal ($\alpha$ dominant), tout en encourageant un contrôle stable ($\kappa$ faible) et un bon débit ($\mu$ modéré). Ces valeurs ont été validées lors des tests d'entraînement préliminaires et ont montré un bon compromis entre performance et stabilité de la politique apprise.

\paragraph{Approximation du débit sortant.}
En pratique, le débit sortant exact $F_{out,t}$ (nombre de véhicules quittant l'intersection) peut être difficile à mesurer directement dans le simulateur sans instrumentation spécifique. Nous utilisons donc une approximation physiquement justifiée basée sur le flux local :
\begin{equation}
F_{out, t} \approx \sum_{i \in \text{segments observés}} \left( \rho_{m,i} \cdot v_{m,i} + \rho_{c,i} \cdot v_{c,i} \right) \cdot \Delta x
\end{equation}

Cette approximation repose sur la définition fondamentale du flux en théorie du trafic : $q = \rho \times v$ (véhicules par unité de temps). En sommant les flux sur les segments observés, nous obtenons une mesure proxy du débit qui capture bien l'objectif de maximisation du nombre de véhicules en mouvement. Cette approche présente l'avantage d'encourager simultanément des densités modérées et des vitesses élevées, ce qui correspond exactement à un état de trafic fluide et optimal.

\subsection{Transitions $P$ et Facteur d'Actualisation $\gamma$}
\label{subsec:transitions_gamma}

\paragraph{Transitions.} La fonction de probabilité de transition $P(s'|s,a)$ est gérée implicitement par le jumeau numérique. L'environnement est déterministe conditionnellement à la demande de trafic en entrée, qui peut être modélisée comme un processus stochastique (par exemple, un processus de Poisson ou en utilisant des profils issus des données TomTom) pour robustifier l'entraînement.

\paragraph{Facteur d'actualisation.} Nous choisissons une valeur de $\gamma = 0.99$. Cette valeur, proche de 1, incite l'agent à adopter une vision à long terme, favorisant les politiques anticipatives qui préviennent la formation de congestion plutôt que de réagir simplement à des situations déjà critiques.

\section{Implémentation de l'Interface Standardisée Gymnasium}
\label{sec:implementation_gym}

L'utilisation de l'interface standard Gymnasium \parencite{Gymnasium:2023} garantit la reproductibilité de nos expériences et la compatibilité avec les bibliothèques RL établies comme Stable-Baselines3 ou RLlib. L'architecture de notre environnement personnalisé, `TrafficEnv`, hérite de la classe `gymnasium.Env`. Un squelette de code illustratif est présenté en Annexe~\ref{app:code_gym}. Les méthodes clés sont :

\begin{itemize}
    \item \textbf{`\_\_init\_\_()`}: Initialise le simulateur ARZ avec les paramètres calibrés du réseau, et définit les espaces d'observation et d'action (`observation\_space`, `action\_space`).
    \item \textbf{`reset()`}: Réinitialise la simulation à un état de départ (par exemple, un état de trafic typique pour une heure donnée, issu de nos données historiques TomTom) et retourne la première observation.
    \item \textbf{`step(action)`}: C'est le cœur de l'interaction. Elle reçoit une action de l'agent, la traduit en une commande pour le simulateur, fait avancer la simulation de $\Delta t_{dec}$ (en exécutant plusieurs pas internes de $\Delta t_{sim}$), puis retourne le tuple `(observation, reward, terminated, truncated, info)`.
\end{itemize}


\subsection{Indicateurs de Performance (KPIs) pour l'Évaluation}
\label{subsec:kpi_evaluation}
Pour l'entraînement et l'évaluation finale de l'agent, nous suivrons plusieurs indicateurs de performance clés (KPIs), extraits de l'environnement à la fin de chaque épisode :
\begin{itemize}
    \item \textbf{Temps d'Attente Moyen :} Le temps moyen passé par un véhicule à l'arrêt ou à très faible vitesse.
    \item \textbf{Longueur Maximale de File d'Attente :} La taille maximale atteinte par les files d'attente sur les approches.
    \item \textbf{Débit (Throughput) :} Le nombre total de véhicules ayant traversé l'intersection.
    \item \textbf{Récompense Cumulée par Épisode :} L'indicateur direct de la performance de l'agent.
\end{itemize}
Ces KPIs nous permettront de comparer quantitativement la performance de notre agent entraîné par rapport au contrôleur de référence.

\section{Conclusion du Chapitre}
\label{sec:conclusion_conception_rl}

Ce chapitre a formalisé le passage du jumeau numérique ARZ calibré à un environnement d'apprentissage par renforcement standardisé et reproductible. Nous avons défini précisément le MDP en cohérence avec les spécificités de notre modèle, présenté l'architecture logicielle conforme à Gymnasium, et établi un protocole de validation rigoureux pour garantir la fiabilité de notre outil d'expérimentation. L'environnement est maintenant prêt. Nous disposons de toutes les fondations nécessaires pour procéder à l'entraînement de notre agent intelligent et à l'analyse de ses performances, qui feront l'objet des chapitres suivants de cette thèse.