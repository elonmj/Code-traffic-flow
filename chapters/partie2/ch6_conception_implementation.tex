\chapter{Conception et Implémentation de l'Environnement d'Apprentissage par Renforcement}
\label{chap:conception_env_rl}

\section{Introduction}
\label{sec:intro_conception_rl}

Ce chapitre représente une étape fondamentale de notre travail : la transformation du jumeau numérique, un modèle de simulation physique validé, en un terrain d'expérimentation interactif pour l'intelligence artificielle. Nous allons ici forger le lien entre la théorie du contrôle et la pratique de l'ingénierie logicielle. L'objectif est de construire un environnement d'apprentissage par renforcement (RL) qui non seulement respecte les standards académiques (Gymnasium), mais qui encapsule aussi toute la complexité et la richesse de notre simulateur ARZ multi-classes.

Le défi est de taille : comment traduire le phénomène complexe et continu du trafic de Lagos en un Processus de Décision Markovien (MDP) que peut comprendre un agent numérique ? Comment définir un état, des actions et, surtout, une fonction de récompense qui guide l'agent vers une politique de contrôle véritablement efficace ?

Ce chapitre répond à ces questions en suivant une progression logique : nous commencerons par la formalisation mathématique du problème via le cadre MDP, en justifiant chaque choix de conception. Nous détaillerons ensuite son implémentation technique via l'interface Gymnasium, garantissant la reproductibilité. Enfin, nous conclurons par la présentation du protocole de validation de l'environnement, assurant sa fiabilité pour les expérimentations cruciales qui seront menées au Chapitre~\ref{chap:entrainement_agent}.

\section{Formalisation du Problème en Processus de Décision Markovien (MDP)}
\label{sec:formalisation_mdp}

Un Processus de Décision Markovien est défini par le quintuplet $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$.
Nous allons maintenant instancier chacun de ces composants dans le contexte spécifique du contrôle des feux de signalisation du corridor de Victoria Island.

\subsection{Espace d'États $\mathcal{S}$}
\label{subsec:espace_etats}

\paragraph{Variables choisies et justification.}
Pour prendre des décisions pertinentes, l'agent doit observer l'état du trafic aux abords directs de l'intersection qu'il contrôle. L'état est donc construit à partir des grandeurs macroscopiques fondamentales issues du simulateur ARZ : la densité $\rho$ (véhicules par kilomètre) et la vitesse moyenne $v$ (kilomètres par heure). Ces deux variables, conservées par le modèle ARZ, suffisent à décrire l'essentiel de la dynamique du trafic et constituent une base d'information riche et physiquement cohérente pour l'agent.

\paragraph{Prise en compte de l'hétérogénéité.}
Nous distinguons au minimum deux classes : les motocyclettes (classe $m$) et les autres véhicules (classe $c$). Pour chaque segment de route $i$ retenu dans l'observation, nous mesurons le vecteur d'état local $\{\rho_{m,i}, v_{m,i}, \rho_{c,i}, v_{c,i}\}$. Des variables auxiliaires comme la longueur de file estimée $L_i$ et la phase de feu courante (encodée en \textit{one-hot}) sont également incluses.

\paragraph{Structure finale du vecteur d'observation.}
Soit $N_{up}$ le nombre de segments en amont et $N_{down}$ le nombre de segments en aval d'une intersection retenus pour l'observation. L'observation $o_t \in \mathcal{S}$ à l'instant $t$ est la concaténation normalisée des états locaux :
\begin{equation}
o_t = \mathrm{concat}\big( \underbrace{(\rho_{m,i}/\rho^{\max}_m, v_{m,i}/v^{free}_m, \rho_{c,i}/\rho^{\max}_c, v_{c,i}/v^{free}_c)}_{i \in \{1, ..., N_{up}+N_{down}\}}, \underbrace{\text{phase\_onehot}}_{\text{dim } N_{phases}} \big).
\end{equation}
La normalisation par la densité maximale $\rho^{\max}$ et la vitesse en flux libre $v^{free}$ est une pratique standard qui stabilise l'entraînement des réseaux de neurones en ramenant toutes les entrées à une échelle comparable.

\paragraph{Paramètres de normalisation.}
Pour normaliser les observations dans l'intervalle $[0, 1]$, nous utilisons des valeurs de référence adaptées au contexte ouest-africain et calibrées sur les données de Lagos. Ces paramètres sont distincts pour chaque classe de véhicules afin de refléter fidèlement leurs comportements spécifiques :

\begin{itemize}
    \item $\rho^{\max}_m = 300$ veh/km : densité de saturation pour les motocyclettes, significativement plus élevée que pour les voitures en raison de leur capacité à se faufiler et à occuper moins d'espace sur la chaussée.
    \item $\rho^{\max}_c = 150$ veh/km : densité de saturation pour les voitures et autres véhicules.
    \item $v^{free}_m = 40$ km/h : vitesse libre typique des motocyclettes en zone urbaine dense.
    \item $v^{free}_c = 50$ km/h : vitesse libre typique des voitures en zone urbaine.
\end{itemize}

Ces valeurs permettent de traduire les variables physiques du simulateur ARZ en observations adimensionnelles comprises entre 0 et 1, respectant l'hétérogénéité du trafic mixte motos-voitures caractéristique de l'Afrique de l'Ouest.

\subsection{Espace d'Actions $\mathcal{A}$}
\label{subsec:espace_actions}

\paragraph{Choix et justification.} La puissance du contrôle ne réside pas toujours dans la complexité des actions possibles, mais dans la pertinence de leur timing. Nous faisons le choix stratégique d'un espace d'actions minimaliste mais efficace. Pour chaque carrefour, l'agent ne dispose que de deux options :
\begin{itemize}
    \item $a=0$ : \textbf{Maintenir la phase actuelle}, laissant le trafic s'écouler.
    \item $a=1$ : \textbf{Passer à la phase suivante} du cycle, provoquant une permutation des droits de passage.
\end{itemize}
Cette simplicité est un atout. Elle concentre le défi de l'apprentissage sur la question la plus importante : \textit{quand} changer de phase ? L'agent doit apprendre un timing optimal basé sur l'état du trafic, plutôt que de se perdre dans un espace d'actions complexe. Cette approche est non seulement robuste et favorise une convergence rapide, mais elle est aussi parfaitement adaptée aux algorithmes de type Q-learning (comme DQN) que nous utiliserons.

\paragraph{Intervalle de décision.} Il est crucial de distinguer le pas de temps de la simulation physique $\Delta t_{sim}$ du pas de temps de décision de l'agent $\Delta t_{dec}$. Le premier est une contrainte numérique, dictée par la condition CFL du solveur WENO pour garantir la stabilité (typiquement $\leq 1s$). Le second est un choix de conception stratégique. Nous fixons $\Delta t_{dec}=10$ secondes. Cette valeur représente un équilibre mûrement réfléchi : elle est suffisamment courte pour permettre une réaction agile aux changements de conditions de trafic, mais assez longue pour éviter des décisions "nerveuses" ou oscillatoires, laissant au trafic le temps de réagir à un changement de phase.

\subsection{Fonction de Récompense $R$}
\label{subsec:fonction_recompense}

\paragraph{Objectif de l'optimisation.} La récompense est la boussole qui guide l'apprentissage de l'agent. Que cherchons-nous à optimiser ? L'objectif est de combattre la congestion. Concrètement, cela se traduit par la minimisation du temps total que les véhicules passent immobilisés ou à très faible vitesse. Cet objectif correspond directement à l'amélioration de la fluidité du trafic et à la réduction du temps de parcours des usagers, finalité ultime de tout système de contrôle de feux.

\paragraph{Formulation mathématique.} À chaque pas de décision $t$, la récompense $R_t$ est calculée comme une somme pondérée de plusieurs métriques.
Elle se décompose en trois termes principaux :
\begin{itemize}
    \item \textbf{Un terme de congestion ($R_{congestion}$)}, qui pénalise le nombre de véhicules à l'arrêt.
    \item \textbf{Un terme de stabilité ($R_{stabilite}$)}, qui pénalise les changements de phase trop fréquents.
    \item \textbf{Un terme de fluidité ($R_{fluidite}$)}, qui encourage le flux sortant.
\end{itemize}
La récompense totale est donnée par l'équation :
\begin{equation}
R_t = R_{congestion} + R_{stabilite} + R_{fluidite}
\end{equation}
avec :
\begin{align}
R_{congestion} &= - \alpha \sum_{i \in \text{approches}} (\rho_{m,i} + \rho_{c,i}) \cdot \Delta x \\
R_{stabilite} &= - \kappa \cdot \mathbb{I}(\text{action} = \text{changer\_phase}) \\
R_{fluidite} &= + \mu \cdot F_{out, t}
\end{align}

où $\alpha, \kappa, \mu$ sont des coefficients de pondération positifs, $\mathbb{I}(\cdot)$ est la fonction indicatrice, et $F_{out, t}$ est le débit sortant total durant l'intervalle $\Delta t_{dec}$.

\paragraph{Choix des coefficients de pondération.}
Les coefficients de la fonction de récompense ont été déterminés empiriquement après une phase d'expérimentation préliminaire pour équilibrer les trois objectifs concurrents. Les valeurs retenues sont :

\begin{table}[h]
\centering
\begin{tabular}{lcp{8cm}}
\toprule
\textbf{Coefficient} & \textbf{Valeur} & \textbf{Justification} \\
\midrule
$\alpha$ & 1.0 & Poids unitaire donnant la priorité principale à la réduction de congestion, objectif primordial du contrôle. \\
$\kappa$ & 0.1 & Pénalité modérée pour limiter les changements fréquents de phase sans trop contraindre l'agent, permettant une flexibilité dans les décisions. \\
$\mu$ & 0.5 & Récompense modérée pour le débit, encourageant la fluidité du trafic sans sacrifier l'objectif principal de réduction de congestion. \\
\bottomrule
\end{tabular}
\caption{Coefficients de pondération de la fonction de récompense.}
\label{tab:reward_weights}
\end{table}

Le ratio $\alpha : \kappa : \mu = 1 : 0.1 : 0.5$ garantit que la réduction de congestion reste l'objectif principal ($\alpha$ dominant), tout en encourageant un contrôle stable ($\kappa$ faible) et un bon débit ($\mu$ modéré). Ces valeurs ont été validées lors des tests d'entraînement préliminaires et ont montré un bon compromis entre performance et stabilité de la politique apprise.

\paragraph{Approximation du débit sortant.}
En pratique, le débit sortant exact $F_{out,t}$ (nombre de véhicules quittant l'intersection) peut être difficile à mesurer directement dans le simulateur sans instrumentation spécifique. Nous utilisons donc une approximation physiquement justifiée basée sur le flux local :
\begin{equation}
F_{out, t} \approx \sum_{i \in \text{segments observés}} \left( \rho_{m,i} \cdot v_{m,i} + \rho_{c,i} \cdot v_{c,i} \right) \cdot \Delta x
\end{equation}

Cette approximation repose sur la définition fondamentale du flux en théorie du trafic : $q = \rho \times v$ (véhicules par unité de temps). En sommant les flux sur les segments observés, nous obtenons une mesure proxy du débit qui capture bien l'objectif de maximisation du nombre de véhicules en mouvement. Cette approche présente l'avantage d'encourager simultanément des densités modérées et des vitesses élevées, ce qui correspond exactement à un état de trafic fluide et optimal.

\subsection{Transitions $P$ et Facteur d'Actualisation $\gamma$}
\label{subsec:transitions_gamma}

\paragraph{Transitions.} La fonction de probabilité de transition $P(s'|s,a)$, qui définit la dynamique du monde, n'a pas besoin d'être formulée explicitement. Elle est incarnée par le jumeau numérique lui-même. Pour l'agent, le simulateur ARZ \textit{est} l'univers. Chaque action qu'il entreprend provoque une évolution de cet univers, calculée par le simulateur. L'environnement est donc déterministe : une même action dans un même état produira toujours le même état suivant. Le caractère stochastique de l'expérience globale provient de la demande de trafic en entrée, qui peut être modélisée par des processus aléatoires (comme un processus de Poisson) ou par la relecture de profils de demande réels issus de nos données, afin de garantir la robustesse de la politique apprise.

\paragraph{Facteur d'actualisation ($\gamma$).} Le facteur d'actualisation détermine l'horizon de planification de l'agent. Nous choisissons une valeur élevée, $\gamma = 0.99$. Ce n'est pas un détail technique : c'est un choix philosophique. Une valeur proche de 1 force l'agent à penser comme un urbaniste plutôt que comme un simple gestionnaire de carrefour. Il apprend que les récompenses futures sont presque aussi importantes que les récompenses immédiates. Cela l'incite à développer des stratégies anticipatives, qui préviennent la formation de la congestion en amont, plutôt que de se contenter de réagir à des files d'attente déjà formées. L'objectif est d'éviter l'engorgement systémique, pas seulement de vider une voie.

\section{Implémentation de l'Interface Standardisée Gymnasium}
\label{sec:implementation_gym}

L'utilisation de l'interface standard Gymnasium \parencite{Gymnasium:2023} garantit la reproductibilité de nos expériences et la compatibilité avec les bibliothèques RL établies comme Stable-Baselines3 ou RLlib. L'architecture de notre environnement personnalisé, `TrafficEnv`, hérite de la classe `gymnasium.Env`. Un squelette de code illustratif est présenté en Annexe~\ref{app:code_gym}. Les méthodes clés sont :

\begin{itemize}
    \item \textbf{`\_\_init\_\_()`}: Initialise le simulateur ARZ avec les paramètres calibrés du réseau, et définit les espaces d'observation et d'action (`observation\_space`, `action\_space`).
    \item \textbf{`reset()`}: Réinitialise la simulation à un état de départ (par exemple, un état de trafic typique pour une heure donnée, issu de nos données historiques TomTom) et retourne la première observation.
    \item \textbf{`step(action)`}: C'est le cœur de l'interaction. Elle reçoit une action de l'agent, la traduit en une commande pour le simulateur, fait avancer la simulation de $\Delta t_{dec}$ (en exécutant plusieurs pas internes de $\Delta t_{sim}$), puis retourne le tuple `(observation, reward, terminated, truncated, info)`.
\end{itemize}


\subsection{Indicateurs de Performance (KPIs) pour l'Évaluation}
\label{subsec:kpi_evaluation}
Pour l'entraînement et l'évaluation finale de l'agent, nous suivrons plusieurs indicateurs de performance clés (KPIs), extraits de l'environnement à la fin de chaque épisode :
\begin{itemize}
    \item \textbf{Temps d'Attente Moyen :} Le temps moyen passé par un véhicule à l'arrêt ou à très faible vitesse.
    \item \textbf{Longueur Maximale de File d'Attente :} La taille maximale atteinte par les files d'attente sur les approches.
    \item \textbf{Débit (Throughput) :} Le nombre total de véhicules ayant traversé l'intersection.
    \item \textbf{Récompense Cumulée par Épisode :} L'indicateur direct de la performance de l'agent.
\end{itemize}
Ces KPIs nous permettront de comparer quantitativement la performance de notre agent entraîné par rapport au contrôleur de référence.

\section{Conclusion du Chapitre}
\label{sec:conclusion_conception_rl}

Ce chapitre a formalisé le passage du jumeau numérique ARZ calibré à un environnement d'apprentissage par renforcement standardisé et reproductible. Nous avons défini précisément le MDP en cohérence avec les spécificités de notre modèle, présenté l'architecture logicielle conforme à Gymnasium, et établi un protocole de validation rigoureux pour garantir la fiabilité de notre outil d'expérimentation. L'environnement est maintenant prêt. Nous disposons de toutes les fondations nécessaires pour procéder à l'entraînement de notre agent intelligent et à l'analyse de ses performances, qui feront l'objet des chapitres suivants de cette thèse.