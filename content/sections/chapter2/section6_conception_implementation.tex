\section{De la Simulation à l'Action : Conception de l'Environnement d'Apprentissage par Renforcement}
\label{sec:conception_implementation}

\subsection{Introduction}
\label{sec:ch6_intro}

Le jumeau numérique, dont la construction a été détaillée au section précédente, nous offre une réplique fidèle du comportement du trafic. Cependant, pour qu'il devienne un outil de décision, il doit être capable d'interagir avec un agent intelligent. Cette section décrit la transformation du simulateur en un environnement d'apprentissage par renforcement (RL) conforme à l'API standard de \texttt{Gymnasium} (anciennement Gym).

Cette transformation est cruciale car elle jette les bases de l'entraînement d'un agent capable d'apprendre des stratégies de contrôle de trafic optimales. Nous allons définir les composants essentiels d'un problème RL, en les adaptant au contexte spécifique du contrôle des feux de signalisation dans notre corridor à Lagos :
\begin{itemize}
    \item \textbf{L'État ($S$)} : Comment l'agent perçoit-il la situation du trafic ?
    \item \textbf{L'Action ($A$)} : Quelles sont les décisions que l'agent peut prendre ?
    \item \textbf{La Récompense ($R$)} : Comment évaluer la qualité des actions de l'agent ?
\end{itemize}

L'objectif est de créer un cadre formel qui permet à un agent d'expérimenter différentes politiques de contrôle et d'apprendre, par essais et erreurs, à maximiser une récompense cumulative, qui sera conçue pour représenter la fluidité du trafic.

\subsection{Site d'Application : Le Corridor de Victoria Island}
\label{sec:ch6_site_application}

Le corridor de Victoria Island, notre cas d'étude, est caractérisé par une congestion sévère et un trafic hétérogène. Pour cette phase de conception de l'environnement RL, nous nous concentrons sur les trois intersections principales qui régulent le flux le long de l'axe majeur. Ces carrefours, déjà identifiés dans notre jumeau numérique, deviennent les points de contrôle où notre agent RL exercera son influence.

Le choix de se concentrer sur ces trois intersections est stratégique :
\begin{itemize}
    \item Elles constituent les principaux goulots d'étranglement du corridor.
    \item Elles sont suffisamment proches pour que leurs politiques de contrôle interagissent, créant un problème de coordination non trivial.
    \item Le nombre d'intersections reste suffisamment restreint pour permettre un apprentissage efficace sans faire face à une explosion combinatoire de l'espace d'états et d'actions (la "malédiction de la dimensionnalité").
\end{itemize}

\subsection{Formalisation du Problème en tant que Processus de Décision Markovien (MDP)}
\label{sec:mdp_formalization}

L'apprentissage par renforcement repose sur le cadre mathématique des Processus de Décision Markoviens (MDP). Un MDP est défini par le tuple $(S, A, P, R, \gamma)$, où :
\begin{itemize}
    \item $S$ est l'ensemble des états possibles.
    \item $A$ est l'ensemble des actions possibles.
    \item $P(s'|s, a)$ est la probabilité de transition de l'état $s$ à l'état $s'$ en prenant l'action $a$.
    \item $R(s, a, s')$ est la récompense obtenue après la transition.
    \item $\gamma$ est le facteur d'actualisation, qui pondère l'importance des récompenses futures.
\end{itemize}

Dans notre cas, le simulateur de trafic encapsule la fonction de transition $P$, qui est déterministe. Notre tâche consiste donc à définir $S$, $A$ et $R$.

\subsubsection{Définition de l'Espace d'États}
\label{sec:state_space}

L'état doit fournir à l'agent une information suffisante pour prendre une décision éclairée. Un état trop simple pourrait omettre des informations cruciales, tandis qu'un état trop complexe rendrait l'apprentissage difficile. Nous avons opté pour une représentation basée sur les files d'attente, qui est à la fois informative et compacte.

Pour chaque intersection, l'état est un vecteur qui concatène les informations suivantes pour chaque voie d'approche :
\begin{itemize}
    \item \textbf{File d'attente cumulée :} Le nombre total de véhicules (toutes classes confondues) en attente.
    \item \textbf{Temps d'attente moyen :} Le temps moyen passé par les véhicules dans la file.
\end{itemize}
Cette représentation capture à la fois l'ampleur de la congestion (nombre de véhicules) et son impact sur les usagers (temps d'attente).

\subsubsection{Définition de l'Espace d'Actions}
\label{sec:action_space}

L'action de l'agent consiste à sélectionner un \textbf{plan de feux} pour chaque intersection. Un plan de feux est une séquence de phases (par exemple, "Nord-Sud vert", "Est-Ouest vert"), chacune avec une durée spécifiée.

Pour éviter un espace d'actions continu et infini, nous avons défini un ensemble discret de plans de feux pré-calculés pour chaque intersection. L'agent choisit un index qui correspond à l'un de ces plans. Par exemple :
\begin{itemize}
    \item \textbf{Action 0 :} Plan "Équilibré" (durées égales pour les phases principales).
    \item \textbf{Action 1 :} Plan "Priorité Nord-Sud" (plus de temps de vert pour l'axe principal).
    \item \textbf{Action 2 :} Plan "Priorité Est-Ouest" (plus de temps de vert pour les axes secondaires).
\end{itemize}
L'agent sélectionne donc une action combinée pour les trois intersections, par exemple (Action 1, Action 0, Action 2), ce qui définit la stratégie de contrôle pour la prochaine période de simulation.

\subsubsection{Définition de la Fonction de Récompense}
\label{sec:reward_function}

La fonction de récompense est l'élément le plus critique, car elle guide l'apprentissage de l'agent. Elle doit traduire l'objectif de "fluidifier le trafic" en un signal numérique. Nous avons conçu une fonction de récompense qui pénalise le temps d'attente total sur le réseau.

La récompense à chaque pas de temps $t$ est définie comme l'opposé de la somme des temps d'attente de tous les véhicules sur le réseau :
$R_t = - \sum_{i \in \text{véhicules}} \Delta T_{i,t}$
où $\Delta T_{i,t}$ est le temps d'attente du véhicule $i$ pendant l'intervalle de temps $t$.

En maximisant la somme des récompenses, l'agent apprendra à minimiser le temps d'attente global, ce qui correspond à notre objectif de réduction de la congestion.

\subsection{Implémentation de l'Interface Gymnasium}
\label{sec:gym_implementation}

Pour que notre simulateur soit compatible avec les algorithmes RL standards (comme ceux de la bibliothèque \texttt{Stable-Baselines3}), nous avons créé une classe Python qui hérite de \texttt{gymnasium.Env}. Cette classe implémente les méthodes fondamentales de l'API :

\begin{itemize}
    \item \texttt{\_\_init\_\_(self)} : Initialise l'environnement, le simulateur sous-jacent, et définit les espaces d'états et d'actions (\texttt{observation\_space} et \texttt{action\_space}).
    \item \texttt{reset(self)} : Réinitialise la simulation à un état de départ (par exemple, un réseau vide ou un état de trafic initial) et retourne la première observation.
    \item \texttt{step(self, action)} : Exécute l'action choisie par l'agent dans le simulateur pour une durée déterminée, calcule la récompense, détermine si l'épisode est terminé, et retourne la nouvelle observation, la récompense, et les informations de fin.
    \item \texttt{render(self)} : (Optionnel) Fournit une visualisation de l'état de l'environnement.
\end{itemize}

Cette encapsulation transforme notre simulateur complexe en une "boîte noire" simple à utiliser pour l'entraînement d'agents RL.
% Un squelette de code illustratif est présenté en Annexe~\ref{app:code_gym}.

\subsection{Synthèse de la Section}
\label{sec:ch6_conclusion}

Cette section a jeté un pont essentiel entre la simulation passive et le contrôle actif. En formalisant le problème de contrôle des feux de signalisation comme un MDP et en encapsulant notre jumeau numérique dans une interface Gymnasium standard, nous avons créé un banc d'essai virtuel pour l'apprentissage de stratégies de trafic intelligentes.

Les choix de conception concernant les espaces d'états, d'actions et la fonction de récompense ont été guidés par un compromis entre la richesse de l'information et la faisabilité de l'apprentissage. L'environnement est maintenant prêt pour la phase suivante : l'entraînement et la validation d'un agent RL, qui feront l'objet de la Section~\ref{sec:entrainement_agents}.