# ğŸš¨ GUIDE DE SURVIE: EntraÃ®nement RL Traffic Signals

**Source**: Analyse de 351 commits, 34 bugs documentÃ©s (branche `experiment/no-behavioral-coupling`)  
**Date**: 18 Novembre 2025  
**Objectif**: **Ne JAMAIS rÃ©pÃ©ter ces erreurs fatales**

---

## âš¡ Les 5 Bugs Qui Ruinent L'EntraÃ®nement RL

### 1. ğŸ”´ BUG #37: `int(action)` Au Lieu de `round(action)`

**Impact**: CRITIQUE - Apprentissage impossible

**ProblÃ¨me**:
```python
# âŒ MAUVAIS
self.current_phase = int(action)  # Tronque TOUJOURS vers 0

# Actions continues: 0.3, 0.5, 0.7, 0.95 â†’ TOUTES deviennent 0 (RED)
# â†’ Agent bloquÃ© en phase RED â†’ Reward = 0 â†’ Pas de gradient
```

**Solution**:
```python
# âœ… CORRECT
self.current_phase = round(float(action))  # Arrondit au seuil 0.5

# 0.3 â†’ 0 (RED), 0.51 â†’ 1 (GREEN), 0.7 â†’ 1 (GREEN) âœ“
```

**VÃ©rification**:
```python
# Test unitaire Ã  ajouter AVANT l'entraÃ®nement
def test_action_mapping():
    actions = [0.0, 0.3, 0.5, 0.7, 0.95, 1.0]
    expected = [0, 0, 0, 1, 1, 1]  # round() behavior
    for action, expected_phase in zip(actions, expected):
        assert round(float(action)) == expected_phase
```

---

### 2. ğŸ”´ BUG #33: Flux Entrant < Flux Initial

**Impact**: CRITIQUE - Pas de trafic Ã  gÃ©rer

**ProblÃ¨me**:
```python
# Configuration qui Ã©vacue le trafic PAR LA GAUCHE!
rho_initial = 125 veh/km Ã— v=5.33 m/s = 0.666 veh/s  # Flux initial
rho_inflow = 200 veh/km Ã— v=2.67 m/s = 0.534 veh/s   # Flux entrant

# 0.534 < 0.666 â†’ Onde de rarÃ©faction â†’ Queue = 0.00 TOUJOURS âŒ
```

**Solution**:
```python
# âœ… Route vide au dÃ©but, forte demande Ã  l'entrÃ©e
rho_initial = max_density * 0.1  # 10% lÃ©ger (free-flow)
w_initial = free_speed_m  # Vitesse libre

rho_inflow = max_density * 0.8  # 80% demande
w_inflow = free_speed_m  # Arrivant Ã  vitesse

# Flux: q_inflow >> q_initial â†’ Queue se forme naturellement âœ“
```

**VÃ©rification**:
```python
# Logs microscopiques - queue DOIT croÃ®tre
# Step 1: queue=0.00
# Step 5: queue=2.50  â† DOIT augmenter!
# Step 10: queue=12.30
```

---

### 3. ğŸ”´ BUG #27: Intervalle ContrÃ´le = Temps Propagation

**Impact**: CRITIQUE - 0% amÃ©lioration

**ProblÃ¨me**:
```python
domain_length = 1000m
wave_speed = 17 m/s
propagation_time = 1000 / 17 â‰ˆ 59s

control_interval = 60s  # âŒ Ratio â‰ˆ 1.0 â†’ RÃ©gime stationnaire!
```

**ConsÃ©quence**: SystÃ¨me atteint Ã©tat stationnaire AVANT chaque dÃ©cision â†’ ContrÃ´le inefficace

**Solution**:
```python
# âœ… Intervalle beaucoup plus court
control_interval = 15s  # LittÃ©rature: 5-15s optimal

# Ratio: 15s / 59s = 0.25 â† Agent peut exploiter la dynamique transitoire
```

---

### 4. ğŸ”´ BUG Reward: Reward Function = 0.0 Toujours

**Impact**: CRITIQUE - Pas de signal d'apprentissage

**SymptÃ´mes**:
```python
# Logs montrent TOUJOURS:
# Reward: 0.00, 0.00, 0.00, 0.00, ...
# Unique values: 1 (devrait Ãªtre > 10)
```

**Causes Possibles**:
1. Queue toujours zÃ©ro (BUG #33)
2. Delta queue = 0 (rÃ©gime stationnaire, BUG #27)
3. Action mapping cassÃ© (BUG #37)
4. Logique reward inversÃ©e/mal implÃ©mentÃ©e

**Solution**:
```python
# Logs microscopiques OBLIGATOIRES
def step(self, action):
    # ... state update ...
    
    # âœ… TOUJOURS logger les composants de reward
    print(f"Queue: prev={prev_queue:.2f}, current={current_queue:.2f}, delta={delta_queue:.4f}")
    print(f"Reward components: R_queue={r_queue:.4f}, R_switches={r_switches:.4f}")
    print(f"TOTAL REWARD: {reward:.4f}")
    
    return obs, reward, done, info
```

---

### 5. ğŸ”´ BUG 0%: FenÃªtres Temporelles DiffÃ©rentes

**Impact**: CRITIQUE - Comparaison invalide

**ProblÃ¨me**:
```python
# âŒ Baseline et RL Ã©valuent sur des pÃ©riodes DIFFÃ‰RENTES
baseline_duration = 600s  # 10 min
rl_duration = 3600s       # 1 heure

# MÃªme si RL fonctionne, mÃ©triques sont incomparables!
```

**Solution**:
```python
# âœ… MÃŠME configuration pour baseline et RL
EVAL_CONFIG = {
    "duration": 3600.0,  # IDENTIQUE
    "control_interval": 15.0,  # IDENTIQUE
    "seed": 42,  # IDENTIQUE pour reproductibilitÃ©
}

baseline_result = run_baseline(**EVAL_CONFIG)
rl_result = run_rl_agent(**EVAL_CONFIG)
```

---

## âœ… Checklist PrÃ©-EntraÃ®nement (OBLIGATOIRE)

Avant de lancer `train_dqn.py`, vÃ©rifier:

- [ ] **Actions**: `round(float(action))` utilisÃ© (PAS `int()`)
- [ ] **Flux**: `q_inflow >> q_initial` vÃ©rifiÃ© mathÃ©matiquement
- [ ] **Intervalle**: `control_interval = 15s` (PAS 60s)
- [ ] **Reward logs**: Imprime queue, delta, composants reward Ã  chaque step
- [ ] **Test rapide**: 100 steps avec actions alÃ©atoires â†’ rewards DOIVENT varier
- [ ] **FenÃªtres identiques**: Baseline et RL mÃªme duration/interval/seed

---

## ğŸ§ª Test de SanitÃ© PrÃ©-EntraÃ®nement

```python
# Ã€ exÃ©cuter AVANT l'entraÃ®nement rÃ©el
def sanity_check(env, num_steps=100):
    """VÃ©rifie que l'environnement peut gÃ©nÃ©rer des rewards variÃ©s"""
    
    rewards = []
    queues = []
    
    env.reset()
    for _ in range(num_steps):
        action = env.action_space.sample()  # Actions alÃ©atoires
        obs, reward, done, info = env.step(action)
        
        rewards.append(reward)
        queues.append(info.get('queue_length', 0))
        
        if done:
            env.reset()
    
    # CHECKS OBLIGATOIRES
    unique_rewards = len(set(rewards))
    max_queue = max(queues)
    
    assert unique_rewards > 5, f"âŒ Rewards trop uniformes! Unique values: {unique_rewards}"
    assert max_queue > 5.0, f"âŒ Queue jamais formÃ©e! Max: {max_queue}"
    assert not all(r == 0 for r in rewards), "âŒ Tous les rewards = 0!"
    
    print(f"âœ… Sanity check PASSED:")
    print(f"   - Unique rewards: {unique_rewards}")
    print(f"   - Queue range: {min(queues):.2f} â†’ {max(queues):.2f}")
    print(f"   - Reward range: {min(rewards):.4f} â†’ {max(rewards):.4f}")
```

---

## ğŸ“Š MÃ©triques de SuccÃ¨s

**Pendant l'entraÃ®nement** (logs Ã  surveiller):

```
âœ… BON SIGNE:
  - Reward varie: min=-2.5, max=1.2, mean=0.15
  - Queue varie: 0.0 â†’ 50.0 vÃ©hicules
  - Phases changent: RED (15 steps) â†” GREEN (20 steps)

âŒ MAUVAIS SIGNE:
  - Reward = 0.00 constant (vÃ©rifier BUG #37, #33, #27)
  - Queue = 0.00 constant (vÃ©rifier BUG #33)
  - Phase = 0 constant (vÃ©rifier BUG #37)
```

---

## ğŸ“ LeÃ§ons Architecturales

### ModularitÃ© (de `niveau4_rl_performance/`)

```
training/
â”œâ”€â”€ core/           # Business logic (agnostic au framework)
â”‚   â”œâ”€â”€ controllers.py      # Baseline vs RL
â”‚   â”œâ”€â”€ evaluators.py       # MÃ©triques
â”‚   â””â”€â”€ cache_manager.py    # Cache intelligent
â”œâ”€â”€ infrastructure/ # ImplÃ©mentations techniques
â”‚   â”œâ”€â”€ rl/                 # Stable-baselines3
â”‚   â”œâ”€â”€ checkpoint/         # Rotation, validation
â”‚   â””â”€â”€ logging/            # Microscopique + TensorBoard
â””â”€â”€ entry_points/   # CLI/Scripts
    â””â”€â”€ train.py            # Point d'entrÃ©e unique
```

### DÃ©cisions MathÃ©matiques (de `test_section_7_6_rl_performance.py`)

**MÃ©triques de performance**:
```python
# Efficiency = sortie / entrÃ©e (devrait Ãªtre < 1 en congestion)
efficiency = total_outflow / total_inflow

# Delay = temps passÃ© - temps free-flow
delay = travel_time - free_flow_time

# Improvement = (baseline - rl) / baseline * 100
improvement_pct = (baseline_metric - rl_metric) / baseline_metric * 100
```

---

## ğŸš€ Workflow RecommandÃ©

1. **Sanity check** (5 min): Actions alÃ©atoires â†’ rewards doivent varier
2. **Quick test** (15 min, 5000 steps): VÃ©rifier apprentissage dÃ©but
3. **Production run** (2-4h, 100k steps): EntraÃ®nement complet
4. **Ã‰valuation**: Comparer vs baseline sur MÃŠME fenÃªtre temporelle

---

**Dernier conseil**: Si reward = 0.0 constant aprÃ¨s 1000 steps â†’ ARRÃŠTER, debugger, ne pas perdre de temps !
