# Training System - Architecture Moderne

Ce dossier contient le **systÃ¨me unique d'entraÃ®nement RL** pour le contrÃ´le de signaux de trafic.

## ğŸ¯ Philosophie

**Une seule tÃ¢che: EntraÃ®ner l'agent RL de maniÃ¨re robuste et reproductible**

- âœ… Applique TOUTES les leÃ§ons du `RL_TRAINING_SURVIVAL_GUIDE.md`
- âœ… Utilise le code moderne de `Code_RL/src/`
- âœ… Architecture modulaire inspirÃ©e de `niveau4_rl_performance/`
- âœ… DÃ©cisions mathÃ©matiques validÃ©es de `test_section_7_6_rl_performance.py`

## ğŸ“ Structure

```
training/
â”œâ”€â”€ README.md                 # Ce fichier
â”œâ”€â”€ train.py                  # ğŸš€ POINT D'ENTRÃ‰E UNIQUE
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ training_config.py    # Configuration d'entraÃ®nement (Pydantic)
â”‚   â””â”€â”€ scenarios.py          # ScÃ©narios prÃ©dÃ©finis (Lagos, etc.)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py            # Orchestrateur d'entraÃ®nement
â”‚   â”œâ”€â”€ evaluator.py          # Ã‰valuation baseline vs RL
â”‚   â””â”€â”€ sanity_checker.py     # Tests prÃ©-entraÃ®nement (BUG #37, #33, #27)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging_utils.py      # Logs microscopiques
â”‚   â””â”€â”€ checkpoint_manager.py # Gestion checkpoints avec rotation
â””â”€â”€ notebooks/
    â””â”€â”€ analyze_results.ipynb # Analyse post-entraÃ®nement
```

## ğŸš€ Usage

### Quick Start (Test Rapide)

```bash
# Test de sanitÃ© (5 min)
python training/train.py --mode sanity --timesteps 100

# Quick test (15 min, 5000 steps)
python training/train.py --mode quick --timesteps 5000

# Production (2-4h, 100k steps)
python training/train.py --mode production --timesteps 100000
```

### Avec Configuration PersonnalisÃ©e

```python
# training/config/scenarios.py
from training.config.training_config import TrainingConfig

lagos_config = TrainingConfig(
    scenario_name="lagos_victoria_island",
    control_interval=15.0,  # BUG #27: PAS 60s!
    episode_length=3600.0,
    
    # BUG #33: Flux entrant >> Flux initial
    initial_density_ratio=0.1,  # Route vide
    inflow_density_ratio=0.8,   # Forte demande
    
    # BUG #37: round() utilisÃ© automatiquement dans env
    
    timesteps=100000,
    checkpoint_freq=1000,
    eval_freq=5000,
)
```

```bash
python training/train.py --config lagos_victoria_island
```

## âœ… Checklist Automatique PrÃ©-EntraÃ®nement

Le systÃ¨me vÃ©rifie automatiquement (via `sanity_checker.py`):

1. **Actions mapping** â†’ `round()` utilisÃ© (pas `int()`)
2. **Flux configuration** â†’ `q_inflow >> q_initial`
3. **Intervalle contrÃ´le** â†’ 15s (pas 60s)
4. **Reward variance** â†’ Actions alÃ©atoires gÃ©nÃ¨rent rewards variÃ©s
5. **Queue formation** â†’ Queue > 0 dans les 100 premiers steps

**Si un check Ã©choue â†’ EntraÃ®nement s'arrÃªte avec diagnostic clair**

## ğŸ“Š Outputs

```
Code_RL/results/
â”œâ”€â”€ {experiment_name}/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ checkpoint_1000_steps.zip
â”‚   â”‚   â”œâ”€â”€ checkpoint_5000_steps.zip
â”‚   â”‚   â””â”€â”€ best_model.zip  # Meilleure Ã©valuation
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ training.log        # Logs microscopiques
â”‚   â”‚   â”œâ”€â”€ tensorboard/        # TensorBoard events
â”‚   â”‚   â””â”€â”€ sanity_check.log    # RÃ©sultats prÃ©-entraÃ®nement
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ baseline_results.json
â”‚   â”‚   â”œâ”€â”€ rl_results.json
â”‚   â”‚   â””â”€â”€ comparison.json
â”‚   â””â”€â”€ metadata.json  # Config, hyperparams, timestamps
```

## ğŸ“ LeÃ§ons AppliquÃ©es

### De `niveau4_rl_performance/` (ModularitÃ©)

- âœ… SÃ©paration `core/` (business logic) vs `infrastructure/` (technical)
- âœ… Cache intelligent avec config hashing
- âœ… Checkpoint rotation automatique

### De `test_section_7_6_rl_performance.py` (Math)

- âœ… MÃ©triques: efficiency, delay, throughput
- âœ… Baseline fixed-time (60s GREEN/RED) comme rÃ©fÃ©rence
- âœ… Ã‰valuation sur MÃŠME fenÃªtre temporelle (BUG 0%)

### De `RL_TRAINING_SURVIVAL_GUIDE.md` (Bugs)

- âœ… `round(action)` au lieu de `int(action)` (BUG #37)
- âœ… `q_inflow >> q_initial` vÃ©rifiÃ© (BUG #33)
- âœ… `control_interval = 15s` (BUG #27)
- âœ… Logs microscopiques pour debug reward (BUG Reward)
- âœ… Config identique baseline vs RL (BUG 0%)

## ğŸ”§ IntÃ©gration avec Code_RL Existant

```python
# Le systÃ¨me utilise DIRECTEMENT le code moderne de Code_RL/src/
from Code_RL.src.env.traffic_signal_env import TrafficSignalEnv
from Code_RL.src.rl.callbacks import RotatingCheckpointCallback
from Code_RL.src.utils.config import RLConfigBuilder

# Pas de duplication - on rÃ©utilise ce qui existe!
```

## ğŸ“ Exemple de Run Complet

```bash
# 1. Sanity check (OBLIGATOIRE avant entraÃ®nement long)
$ python training/train.py --mode sanity
âœ… Sanity check PASSED:
   - Action mapping: round() verified
   - Flux: q_inflow (1780) >> q_initial (222) âœ“
   - Control interval: 15.0s âœ“
   - Reward variance: 23 unique values âœ“
   - Queue formation: max=45.2 vehicles âœ“

# 2. Quick test (valider apprentissage)
$ python training/train.py --mode quick --timesteps 5000
ğŸš€ Starting QUICK training: 5000 timesteps
ğŸ“Š Episode 10/50: reward=0.15 (improving!)
ğŸ’¾ Checkpoint saved: checkpoint_1000_steps.zip
âœ… Training completed in 12.5 minutes

# 3. Production run
$ python training/train.py --mode production --timesteps 100000
ğŸš€ Starting PRODUCTION training: 100000 timesteps
ğŸ“Š Progress: 10000/100000 (10%)
   - Mean reward (last 100 episodes): 0.42
   - Best evaluation reward: 0.58
ğŸ’¾ Checkpoint saved: checkpoint_10000_steps.zip
...
âœ… Training completed in 3.2 hours
ğŸ“Š Best model: results/lagos/best_model.zip (eval_reward=0.65)
```

## ğŸ¯ Prochaines Ã‰tapes

1. ImplÃ©menter `trainer.py` (orchestrateur principal)
2. ImplÃ©menter `sanity_checker.py` (checks automatiques)
3. ImplÃ©menter `evaluator.py` (baseline vs RL comparison)
4. Tester sur Lagos Victoria Island scenario
5. DÃ©ployer sur Kaggle pour GPU training

---

**RÃ¨gle d'Or**: Si reward = 0.0 aprÃ¨s 1000 steps â†’ STOP, debug, ne pas perdre de temps !
