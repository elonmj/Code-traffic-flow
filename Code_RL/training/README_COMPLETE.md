# RL Training System - Guide Complet

Training system architecture for RL-based traffic signal control with **automatic bug detection** and **production-ready** orchestration.

## ðŸš€ Quick Start

```bash
# Sanity check (5 min) - TOUJOURS faire Ã§a d'abord!
python -m Code_RL.training.train --mode sanity --scenario lagos

# Quick test (15 min)
python -m Code_RL.training.train --mode quick --scenario lagos

# Production (2-4h)
python -m Code_RL.training.train --mode production --scenario lagos

# Kaggle GPU (9h limit)
python -m Code_RL.training.train --mode kaggle --scenario lagos --device cuda
```

## ðŸ—ï¸ Architecture Overview

```
Code_RL/training/
â”œâ”€â”€ config/                      # Pydantic configurations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ training_config.py      # TrainingConfig, DQNHyperparameters, etc.
â”‚
â”œâ”€â”€ core/                        # Core training logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py              # RLTrainer orchestrator
â”‚   â””â”€â”€ sanity_checker.py       # Pre-training validation (BUG #37, #33, #27)
â”‚
â”œâ”€â”€ __init__.py                  # Package API
â”œâ”€â”€ train.py                     # CLI entry point
â”œâ”€â”€ quick_start.py              # Minimal example
â””â”€â”€ README_COMPLETE.md           # This file
```

## ðŸ“¦ Key Components

### 1. TrainingConfig (Pydantic) - SÃ©paration des Concerns

**IMPORTANT**: `TrainingConfig` est SÃ‰PARÃ‰ de `RLConfigBuilder`:
- `RLConfigBuilder` (src/utils/config.py): Configuration de l'**ENVIRONNEMENT** (ARZ + RL env)
- `TrainingConfig` (training/config/): Configuration de l'**ENTRAÃŽNEMENT** (hyperparams DQN)

```python
from Code_RL.training.config import production_config

# ScÃ©narios prÃ©dÃ©finis
config = production_config("lagos_v1")

# Configuration manuelle
from Code_RL.training.config import TrainingConfig, DQNHyperparameters

config = TrainingConfig(
    experiment_name="lagos_v1",
    mode="production",
    total_timesteps=100000,
    device="cuda",
    dqn_hyperparams=DQNHyperparameters(
        learning_rate=1e-3,
        buffer_size=50000,
        batch_size=32
    ),
    checkpoint_strategy=CheckpointStrategy(
        save_freq=1000,
        max_checkpoints=2
    )
)
```

### 2. Trainer - Orchestrateur Principal

```python
from Code_RL.training import train_model, production_config
from Code_RL.src.utils.config import RLConfigBuilder

# Config environnement (ARZ + RL env)
rl_config = RLConfigBuilder.for_training("lagos")

# Config entraÃ®nement (DQN hyperparams)
training_config = production_config("lagos_v1")

# Train!
model = train_model(rl_config, training_config)
```

**Ou avec plus de contrÃ´le:**

```python
from Code_RL.training.core import RLTrainer

trainer = RLTrainer(rl_config, training_config)
model = trainer.train()
metrics = trainer.evaluate(n_episodes=10)
trainer.cleanup()
```

### 3. Sanity Checker - Validation PrÃ©-EntraÃ®nement

**CRITIQUE**: VÃ©rifie automatiquement les 5 BUGS MORTELS avant l'entraÃ®nement:

| Bug    | SymptÃ´me                        | Fix                                    |
|--------|---------------------------------|----------------------------------------|
| #37    | Action truncation               | `round(float(action))` vs `int()`      |
| #33    | Queue toujours zÃ©ro             | `rho_inflow >> rho_initial` (15:1)     |
| #27    | Pas d'apprentissage             | `dt_decision = 15s` (pas 60s)          |
| #36    | Erreur GPU/CPU                  | VÃ©rifier device consistency            |
| Reward | Reward constant                 | Au moins 5 valeurs uniques sur 100 steps|

```python
from Code_RL.training.core import run_sanity_checks
from Code_RL.training.config import SanityCheckConfig

sanity_config = SanityCheckConfig(
    enabled=True,
    num_steps=100,
    min_unique_rewards=5
)

# LÃ¨ve RuntimeError si les checks Ã©chouent
run_sanity_checks(rl_config, sanity_config)
```

## ðŸ“Š Modes d'EntraÃ®nement

| Mode       | Steps   | Time    | Purpose                        | Checkpoint Freq |
|------------|---------|---------|--------------------------------|-----------------|
| **sanity** | 100     | 5 min   | Validate setup, check bugs     | 50              |
| **quick**  | 5,000   | 15 min  | Test learning, verify rewards  | 500             |
| **production** | 100,000 | 2-4h | Full training              | 1,000           |
| **kaggle** | 200,000 | 9h      | GPU-optimized for Kaggle       | 2,000           |

## ðŸ“ Output Structure

```
results/
â””â”€â”€ {experiment_name}/
    â”œâ”€â”€ checkpoints/
    â”‚   â”œâ”€â”€ latest/                    # Rotating checkpoints (max 2)
    â”‚   â”‚   â”œâ”€â”€ dqn_checkpoint_1000_steps.zip
    â”‚   â”‚   â””â”€â”€ replay_buffer_1000_steps.pkl
    â”‚   â”œâ”€â”€ best/                      # Best model (eval callback)
    â”‚   â”‚   â””â”€â”€ best_model.zip
    â”‚   â”œâ”€â”€ dqn_model_final.zip       # Final model
    â”‚   â””â”€â”€ replay_buffer_final.pkl
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ training.log
    â”œâ”€â”€ eval/
    â”‚   â””â”€â”€ evaluations.npz
    â””â”€â”€ training_config.json           # Reproducibility
```

## ðŸ”§ Configuration Pydantic - DÃ©tails

### DQNHyperparameters

```python
DQNHyperparameters(
    learning_rate=1e-3,           # Learning rate (default from Code_RL)
    buffer_size=50000,            # Replay buffer size
    learning_starts=1000,         # Steps before training starts
    batch_size=32,                # Batch size
    tau=1.0,                      # Soft update coefficient
    gamma=0.99,                   # Discount factor
    train_freq=4,                 # Training frequency
    gradient_steps=1,             # Gradient steps per update
    target_update_interval=1000,  # Target network update freq
    exploration_fraction=0.1,     # Fraction of timesteps for exploration
    exploration_initial_eps=1.0,  # Initial epsilon
    exploration_final_eps=0.05    # Final epsilon
)
```

### CheckpointStrategy

```python
CheckpointStrategy(
    save_freq=1000,              # FrÃ©quence de sauvegarde (steps)
    max_checkpoints=2,           # Nombre max de checkpoints (rotation)
    save_replay_buffer=True      # Sauvegarder replay buffer (requis pour DQN)
)
```

### EvaluationStrategy

```python
EvaluationStrategy(
    eval_freq=5000,              # FrÃ©quence d'Ã©valuation (steps)
    n_eval_episodes=10,          # Nombre d'Ã©pisodes pour Ã©valuation
    deterministic=True           # Actions dÃ©terministes pour Ã©valuation
)
```

### SanityCheckConfig

```python
SanityCheckConfig(
    enabled=True,                # Activer les sanity checks
    num_steps=100,               # Nombre de steps pour le test
    min_unique_rewards=5,        # Minimum de rewards uniques requis
    min_max_queue=5.0,          # Queue maximale minimale requise
    check_action_mapping=True,   # VÃ©rifier round() vs int()
    check_flux_config=True,      # VÃ©rifier q_inflow >> q_initial
    check_control_interval=True  # VÃ©rifier interval = 15s
)
```

## ðŸ”Œ Integration with Existing Code

Le systÃ¨me rÃ©utilise l'infrastructure moderne de `Code_RL/`:

| Module Existant                  | Usage dans Training                    |
|----------------------------------|----------------------------------------|
| `RLConfigBuilder`                | Configuration de l'environnement ARZ   |
| `TrafficSignalEnvDirect`         | Environnement RL                       |
| `RotatingCheckpointCallback`     | Gestion des checkpoints (rotation)     |
| `TrainingProgressCallback`       | Logging des progrÃ¨s                    |
| `train_dqn.py`                   | Source of truth pour hyperparamÃ¨tres   |

## ðŸŽ¯ Usage Examples

### Example 1: Quick Sanity Check

```python
from Code_RL.src.utils.config import RLConfigBuilder
from Code_RL.training import train_model, sanity_check_config

rl_config = RLConfigBuilder.for_training("lagos")
training_config = sanity_check_config()

model = train_model(rl_config, training_config)
# Output: results/sanity_check/
```

### Example 2: Production Training

```python
from Code_RL.src.utils.config import RLConfigBuilder
from Code_RL.training import train_model, production_config

rl_config = RLConfigBuilder.for_training("lagos")
training_config = production_config("lagos_v1")

model = train_model(rl_config, training_config)
# Output: results/lagos_v1/
```

### Example 3: Custom Configuration

```python
from Code_RL.src.utils.config import RLConfigBuilder
from Code_RL.training import TrainingConfig, DQNHyperparameters, train_model

rl_config = RLConfigBuilder.for_training("lagos")

training_config = TrainingConfig(
    experiment_name="lagos_custom",
    mode="production",
    total_timesteps=150000,  # Custom timesteps
    device="cuda",
    dqn_hyperparams=DQNHyperparameters(
        learning_rate=5e-4,  # Custom learning rate
        buffer_size=100000   # Larger buffer
    )
)

model = train_model(rl_config, training_config)
```

### Example 4: Resume Training

```bash
# CLI
python -m Code_RL.training.train --mode production --scenario lagos --resume

# Python
training_config.resume_training = True
training_config.checkpoint_path = Path("results/lagos_v1/checkpoints/latest/...")
```

## ðŸ“ CLI Reference

```bash
python -m Code_RL.training.train [OPTIONS]

Options:
  --mode {sanity,quick,production,kaggle}
                        Training mode (default: production)
  --scenario {simple,lagos,riemann}
                        Training scenario (default: lagos)
  --device {cpu,cuda}   Device (default: cpu)
  --timesteps INT       Total timesteps (overrides mode default)
  --name STR            Experiment name (default: auto-generated)
  --resume              Resume from latest checkpoint
  --no-sanity-checks    Disable sanity checks (NOT RECOMMENDED)
```

## ðŸ› Troubleshooting

### Issue: Sanity checks fail

**Solution**: VÃ©rifier les bugs documentÃ©s dans `RL_TRAINING_SURVIVAL_GUIDE.md`:
- BUG #37: Action mapping uses `round()` not `int()`
- BUG #33: `rho_inflow >> rho_initial` (ratio 15:1)
- BUG #27: `dt_decision = 15.0` (not 60.0)

### Issue: Training doesn't resume

**Solution**: VÃ©rifier que `--resume` est spÃ©cifiÃ© ET qu'un checkpoint existe dans `results/{experiment_name}/checkpoints/latest/`

### Issue: Out of memory on GPU

**Solution**: RÃ©duire `buffer_size` ou `batch_size` dans DQNHyperparameters

### Issue: Reward constant / no learning

**Solution**: Sanity checker dÃ©tectera cela automatiquement. VÃ©rifier reward function.

## ðŸ“š See Also

- `RL_TRAINING_SURVIVAL_GUIDE.md`: Lessons learned from 351 commits
- `Code_RL/src/rl/train_dqn.py`: Source of truth for hyperparameters
- `.copilot-tracking/analysis/rl_history_analysis.md`: Full commit analysis

## ðŸŽ“ Philosophy

Ce systÃ¨me incarne les leÃ§ons des 351 commits analysÃ©s:
1. **Separation of Concerns**: Environment config â‰  Training config
2. **Fail Fast**: Sanity checks detect bugs BEFORE wasting hours
3. **Reproducibility**: All configs saved as JSON
4. **Modularity**: Reuse existing Code_RL infrastructure
5. **Safety**: Resume training, rotating checkpoints, best model tracking
