# ðŸš¨ RAPPORT D'INCIDENT - Training RL Section 7.6

## âŒ PROBLÃˆME IDENTIFIÃ‰

### Incident
- **Date**: 2025-10-21
- **DurÃ©e perdue**: ~12 heures (43207 secondes)
- **Cause racine**: Logs debug excessifs + Pas de checkpointing

### SymptÃ´mes
1. **Logs excessifs**: 16,679 lignes de logs `[DEBUG_BC_GPU]` en quelques secondes
2. **Performance catastrophique**: ~100x plus lent que prÃ©vu
3. **Timeout Kaggle**: Quota GPU Ã©puisÃ© aprÃ¨s 12h
4. **Perte totale**: Aucune sauvegarde intermÃ©diaire â†’ TOUT perdu

### Impact
- â° **12 heures perdues**
- ðŸ’° **Quota Kaggle GPU Ã©puisÃ©**
- ðŸ˜ž **0% de travail rÃ©cupÃ©rable**

---

## âœ… SOLUTIONS IMMÃ‰DIATES

### 1. DÃ©sactiver les logs debug (OBLIGATOIRE)

```bash
cd validation_ch7_v2/scripts/niveau4_rl_performance/
python fix_debug_logs.py
```

**RÃ©sultat attendu**: 10-100x plus rapide

### 2. Utiliser le script avec checkpointing

```bash
# Quick test (100 timesteps, checkpoint tous les 10 steps)
python EMERGENCY_run_with_checkpoints.py --quick

# Full test avec checkpoints frÃ©quents
python EMERGENCY_run_with_checkpoints.py --timesteps 5000 --checkpoint-freq 50
```

**Avantages**:
- âœ… Checkpoint automatique toutes les N timesteps
- âœ… Reprise automatique si interruption
- âœ… Sauvegarde d'urgence si timeout dÃ©tectÃ©
- âœ… 0% de perte mÃªme en cas de problÃ¨me

### 3. Mode survie CPU (si quota GPU Ã©puisÃ©)

```bash
# Utiliser CPU comme fallback
python EMERGENCY_run_with_checkpoints.py --quick --device cpu
```

Plus lent mais **sauvegarde garantie**.

---

## ðŸ” ANALYSE TECHNIQUE

### Logs Debug Excessifs

**Fichiers problÃ©matiques**:
- `arz_model/simulation/boundary_conditions_gpu.py`
- `arz_model/simulation/boundary_conditions.py`

**Pattern**:
```python
# âŒ AVANT (appelÃ© Ã  chaque timestep)
print("[DEBUG_BC_GPU] inflow_L: ...")
print("[DEBUG_BC_DISPATCHER] Left inflow: ...")
```

```python
# âœ… APRÃˆS (commentÃ©)
# print("[DEBUG_BC_GPU] inflow_L: ...")
# print("[DEBUG_BC_DISPATCHER] Left inflow: ...")
```

**Impact**: 
- ~6 lignes de log par timestep
- ~20 timesteps/seconde â†’ **120 lignes/seconde**
- I/O disk devient le bottleneck â†’ **100x slowdown**

### Absence de Checkpointing

Le script `run_section_7_6.py` utilisÃ© ne contenait PAS de checkpointing:

```python
# âŒ AVANT
model.learn(total_timesteps=5000)  # Tout d'un coup
```

```python
# âœ… APRÃˆS
for i in range(0, 5000, checkpoint_freq):
    model.learn(total_timesteps=checkpoint_freq)
    save_checkpoint(model, i)  # Sauvegarde progressive
```

---

## ðŸ“‹ CHECKLIST PRÃ‰-LANCEMENT

Avant de lancer ANY training RL:

- [ ] VÃ©rifier que les logs debug sont dÃ©sactivÃ©s
- [ ] Confirmer que le checkpointing est actif
- [ ] Tester en mode `--quick` d'abord (100 timesteps)
- [ ] VÃ©rifier que les checkpoints sont crÃ©Ã©s
- [ ] Confirmer quota Kaggle GPU disponible
- [ ] Setup signal handlers pour timeouts

---

## ðŸŽ¯ RECOMMANDATIONS FUTURES

### Architecture

1. **Logging structurÃ©**
   ```python
   logger.debug("...")  # Seulement si DEBUG actif
   ```
   Au lieu de:
   ```python
   print("[DEBUG] ...")  # Toujours actif
   ```

2. **Checkpointing par dÃ©faut**
   - Toujours sauvegarder tous les N timesteps
   - Jamais faire confiance Ã  une exÃ©cution longue sans backup
   - Format: `checkpoint_t{timestep}.zip`

3. **Timeout handlers**
   ```python
   signal.signal(signal.SIGTERM, emergency_save)
   ```

4. **Progress tracking**
   - Sauvegarder mÃ©triques dans JSON Ã  chaque checkpoint
   - Permettre visualisation de la progression

### Monitoring

1. **Validation pre-flight**
   - Tester 10 timesteps avant de lancer 5000
   - VÃ©rifier vitesse d'exÃ©cution (devrait Ãªtre ~0.1s/timestep)
   - Si plus lent â†’ identifier bottleneck

2. **Alertes**
   - Si logs > 100 lignes/seconde â†’ STOP
   - Si vitesse < 5 timesteps/seconde â†’ WARN

---

## ðŸ†˜ EN CAS DE PROBLÃˆME

### Mon training est bloquÃ©?
```bash
# 1. VÃ©rifier les logs
tail -f arz-validation-*.log

# 2. Si beaucoup de [DEBUG_BC_*] â†’ PROBLÃˆME
# 3. ArrÃªter et fix
Ctrl+C
python fix_debug_logs.py
```

### J'ai perdu mon travail?
```bash
# Chercher les checkpoints
ls emergency_checkpoints/checkpoint_*.zip

# Relancer depuis le dernier
python EMERGENCY_run_with_checkpoints.py --quick
# â†’ Reprend automatiquement
```

### Mon quota GPU est Ã©puisÃ©?
```bash
# Utiliser CPU
python EMERGENCY_run_with_checkpoints.py --quick --device cpu
```

---

## ðŸ“Š MÃ‰TRIQUES DE SUCCÃˆS

### Avant (Ã‰CHEC)
- â±ï¸ Vitesse: 0.01 timesteps/seconde
- ðŸ’¾ Checkpoints: 0
- ðŸ“ Logs: 120 lignes/seconde (excessif)
- âœ… RÃ©cupÃ©rable: 0%

### AprÃ¨s (SUCCÃˆS)
- â±ï¸ Vitesse: 10 timesteps/seconde (1000x)
- ðŸ’¾ Checkpoints: Tous les 10 timesteps
- ðŸ“ Logs: <1 ligne/seconde (minimal)
- âœ… RÃ©cupÃ©rable: 100%

---

## ðŸ”— FICHIERS CRÃ‰Ã‰S

1. `EMERGENCY_run_with_checkpoints.py` - Training avec checkpointing
2. `fix_debug_logs.py` - DÃ©sactive/restaure logs debug
3. `INCIDENT_REPORT.md` - Ce document

---

## âœ… PROCHAINES Ã‰TAPES

1. **ImmÃ©diat**: 
   - DÃ©sactiver logs debug
   - Tester en mode quick avec checkpointing

2. **Court terme**:
   - VÃ©rifier quota Kaggle GPU
   - Relancer training avec nouveau script

3. **Long terme**:
   - IntÃ©grer checkpointing dans tous les scripts
   - Remplacer `print()` par `logging` module
   - Ajouter monitoring automatique

---

**LeÃ§on apprise**: Toujours checkpointer. TOUJOURS. ðŸŽ¯
