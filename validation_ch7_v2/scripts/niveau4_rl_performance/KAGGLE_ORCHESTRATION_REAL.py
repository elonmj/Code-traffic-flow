#!/usr/bin/env python3
"""
üî¥ REAL Kaggle Orchestration for Section 7.6 RL Performance
=========================================================

‚ö†Ô∏è THIS IS THE REAL THING - NOT MOCKS!

Orchestre:
1. REAL RL training with TrafficSignalEnvDirect (GPU on Kaggle)
2. REAL evaluation against baseline (actual simulations)
3. REAL metrics computed from simulation data (not np.random)
4. Outputs: JSON results + PNG figures for thesis

Architecture:
- Entry point: kaggle_run() 
- GPU detection: Automatic (CUDA on Kaggle, CPU locally)
- Scenarios: Configurable (1-3 scenarios)
- Mode: QUICK (5min) or FULL (3-4 hours)

Usage:
    # Local quick test (CPU, 100 steps)
    python KAGGLE_ORCHESTRATION_REAL.py --quick --device cpu
    
    # Kaggle full run (GPU, 5000 steps)
    python KAGGLE_ORCHESTRATION_REAL.py --device gpu
    
    # Custom configuration
    python KAGGLE_ORCHESTRATION_REAL.py --timesteps 10000 --episodes 5
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import argparse
import time
from datetime import datetime

import numpy as np
import matplotlib.pyplot as plt

# Setup paths
CODE_RL_PATH = Path(__file__).parent.parent.parent.parent / "Code_RL"
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent

sys.path.insert(0, str(CODE_RL_PATH))
sys.path.insert(0, str(CODE_RL_PATH / "src"))
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(Path(__file__).parent))

# Import REAL validation functions
from rl_training import train_rl_agent_for_validation
from rl_evaluation import evaluate_traffic_performance


class KaggleOrchestration:
    """REAL Kaggle orchestration for Section 7.6 validation."""
    
    def __init__(self, device: str = "gpu", quick_test: bool = False):
        self.device = device
        self.quick_test = quick_test
        self.output_dir = Path(__file__).parent / "kaggle_results"
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # Configuration based on mode
        if quick_test:
            self.config = {
                "total_timesteps": 100,  # Fast training
                "num_episodes": 1,        # Single episode evaluation
                "duration": "5 minutes",
                "mode": "QUICK TEST"
            }
        else:
            self.config = {
                "total_timesteps": 5000,  # Full training
                "num_episodes": 3,        # 3 episode evaluation
                "duration": "3-4 hours",
                "mode": "FULL VALIDATION"
            }
        
        self.results = {}
        self.start_time = None
        self.end_time = None
    
    def log(self, message: str, level: str = "INFO"):
        """Structured logging."""
        timestamp = datetime.now().isoformat()
        print(f"[{timestamp}] [{level}] {message}", flush=True)
    
    def detect_gpu(self) -> bool:
        """Detect if GPU is available."""
        try:
            import torch
            available = torch.cuda.is_available()
            if available:
                self.log(f"‚úÖ GPU detected: {torch.cuda.get_device_name()}")
            return available
        except:
            return False
    
    def phase_1_train_rl_agent(self) -> Tuple[Any, Dict[str, Any]]:
        """Phase 1: Train RL agent on GPU (or CPU)."""
        self.log("=" * 80)
        self.log("PHASE 1: Training RL Agent")
        self.log("=" * 80)
        
        timesteps = self.config["total_timesteps"]
        self.log(f"Starting DQN training: {timesteps} timesteps on {self.device}")
        self.log(f"This will take approximately {self.config['duration']}")
        
        phase_start = time.time()
        
        try:
            # ‚úÖ CALL REAL TRAINING (NOT MOCK!)
            model, training_history = train_rl_agent_for_validation(
                config_name="lagos_master",
                total_timesteps=timesteps,
                algorithm="DQN",
                device=self.device,
                use_mock=False  # ‚ö†Ô∏è REAL training!
            )
            
            phase_end = time.time()
            phase_duration = phase_end - phase_start
            
            training_history["phase_duration"] = phase_duration
            training_history["device"] = self.device
            
            self.log(f"‚úÖ Training completed in {phase_duration:.1f}s")
            self.log(f"   Model saved: {training_history['model_path']}")
            
            self.results["training"] = training_history
            return model, training_history
            
        except Exception as e:
            self.log(f"‚ùå Training failed: {e}", level="ERROR")
            import traceback
            traceback.print_exc()
            raise
    
    def phase_2_evaluate_strategies(self, model_path: str) -> Dict[str, Any]:
        """Phase 2: Evaluate RL agent against baseline."""
        self.log("=" * 80)
        self.log("PHASE 2: Evaluating RL vs Baseline")
        self.log("=" * 80)
        
        num_episodes = self.config["num_episodes"]
        self.log(f"Running {num_episodes} episodes for each strategy...")
        
        phase_start = time.time()
        
        try:
            # ‚úÖ CALL REAL EVALUATION (NOT MOCK!)
            comparison = evaluate_traffic_performance(
                rl_model_path=model_path,
                config_name="lagos_master",
                num_episodes=num_episodes,
                device=self.device
            )
            
            phase_end = time.time()
            phase_duration = phase_end - phase_start
            
            comparison["phase_duration"] = phase_duration
            
            self.log(f"‚úÖ Evaluation completed in {phase_duration:.1f}s")
            self.log(f"   Baseline avg travel time: {comparison['baseline']['avg_travel_time']:.2f}s")
            self.log(f"   RL avg travel time: {comparison['rl']['avg_travel_time']:.2f}s")
            self.log(f"   Improvement: {comparison['improvements']['travel_time_improvement']:+.1f}%")
            
            self.results["evaluation"] = comparison
            return comparison
            
        except Exception as e:
            self.log(f"‚ùå Evaluation failed: {e}", level="ERROR")
            import traceback
            traceback.print_exc()
            raise
    
    def phase_3_generate_outputs(self, comparison: Dict[str, Any]):
        """Phase 3: Generate figures and JSON outputs for thesis."""
        self.log("=" * 80)
        self.log("PHASE 3: Generating Outputs")
        self.log("=" * 80)
        
        try:
            # 1. Save raw results JSON
            json_path = self.output_dir / "comparison_results.json"
            with open(json_path, 'w') as f:
                # Make JSON serializable
                json_data = self._make_json_serializable(comparison)
                json.dump(json_data, f, indent=2)
            self.log(f"‚úÖ Results JSON saved: {json_path}")
            
            # 2. Generate comparison figure
            fig_path = self._generate_comparison_figure(comparison)
            self.log(f"‚úÖ Comparison figure saved: {fig_path}")
            
            # 3. Generate learning curve (if training history available)
            if "training" in self.results:
                curve_path = self._generate_learning_curve(self.results["training"])
                self.log(f"‚úÖ Learning curve saved: {curve_path}")
            
            # 4. Generate LaTeX content for thesis
            latex_path = self._generate_latex_content(comparison)
            self.log(f"‚úÖ LaTeX content saved: {latex_path}")
            
            self.log(f"\nüìä All outputs saved to: {self.output_dir}")
            
        except Exception as e:
            self.log(f"‚ùå Output generation failed: {e}", level="ERROR")
            import traceback
            traceback.print_exc()
            raise
    
    def _make_json_serializable(self, obj):
        """Convert numpy types to Python native types for JSON serialization."""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, Path):
            return str(obj)
        else:
            return obj
    
    def _generate_comparison_figure(self, comparison: Dict[str, Any]) -> Path:
        """Generate comparison bar chart: Baseline vs RL."""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        fig.suptitle("Section 7.6: RL Performance vs Baseline (Traffic Control)", 
                     fontsize=14, fontweight='bold')
        
        baseline = comparison['baseline']
        rl = comparison['rl']
        improvements = comparison['improvements']
        
        # Metric 1: Travel Time (lower is better)
        ax = axes[0]
        categories = ['Baseline', 'RL']
        values = [baseline['avg_travel_time'], rl['avg_travel_time']]
        colors = ['#FF6B6B', '#4ECDC4']
        bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
        ax.set_ylabel('Average Travel Time (seconds)', fontsize=11, fontweight='bold')
        ax.set_title('Travel Time\n(Lower is Better)', fontsize=12, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar, val in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{val:.1f}s',
                   ha='center', va='bottom', fontweight='bold')
        
        # Add improvement percentage
        ax.text(0.5, max(values) * 0.9, 
               f'{improvements["travel_time_improvement"]:+.1f}%',
               ha='center', fontsize=12, fontweight='bold', 
               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
        
        # Metric 2: Throughput (higher is better)
        ax = axes[1]
        values = [baseline['total_throughput'], rl['total_throughput']]
        bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
        ax.set_ylabel('Total Throughput (vehicles)', fontsize=11, fontweight='bold')
        ax.set_title('Throughput\n(Higher is Better)', fontsize=12, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar, val in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(val)}',
                   ha='center', va='bottom', fontweight='bold')
        
        # Add improvement percentage
        ax.text(0.5, max(values) * 0.9,
               f'{improvements["throughput_improvement"]:+.1f}%',
               ha='center', fontsize=12, fontweight='bold',
               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
        
        # Metric 3: Queue Length (lower is better)
        ax = axes[2]
        values = [baseline['avg_queue_length'], rl['avg_queue_length']]
        bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
        ax.set_ylabel('Average Queue Length (vehicles)', fontsize=11, fontweight='bold')
        ax.set_title('Queue Length\n(Lower is Better)', fontsize=12, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar, val in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{val:.1f}',
                   ha='center', va='bottom', fontweight='bold')
        
        # Add improvement percentage
        ax.text(0.5, max(values) * 0.9,
               f'{improvements["queue_reduction"]:+.1f}%',
               ha='center', fontsize=12, fontweight='bold',
               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
        
        plt.tight_layout()
        
        fig_path = self.output_dir / "comparison_baseline_vs_rl.png"
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig_path
    
    def _generate_learning_curve(self, training_history: Dict[str, Any]) -> Path:
        """Generate training learning curve."""
        # This would require training history with per-episode rewards
        # For now, generate a placeholder
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.set_xlabel('Timesteps', fontsize=11, fontweight='bold')
        ax.set_ylabel('Cumulative Reward', fontsize=11, fontweight='bold')
        ax.set_title('DQN Training Progress', fontsize=12, fontweight='bold')
        ax.text(0.5, 0.5, 'Learning curve\n(requires training history)',
               ha='center', va='center', fontsize=12)
        ax.grid(alpha=0.3)
        
        fig_path = self.output_dir / "learning_curve.png"
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig_path
    
    def _generate_latex_content(self, comparison: Dict[str, Any]) -> Path:
        """Generate LaTeX content for thesis Section 7.6."""
        latex_content = f"""
% =========================================================================
% Section 7.6: RL Performance Validation - Auto-Generated Results
% =========================================================================

\\section{{RL Performance Validation (Section 7.6)}}

\\subsection{{Experimental Setup}}

Our validation tests the claim \\textbf{{R5}}: ``Les agents RL surpassent 
les contr\\^oleurs de base en performance de trafic.''

\\textbf{{Baseline}}: Fixed-time signal control (60s GREEN/RED), representing 
current Benin infrastructure practice.

\\textbf{{RL Agent}}: DQN-trained agent using TrafficSignalEnvDirect environment 
with direct ARZ coupling, decision interval 15s, trained on {{total_timesteps}} 
timesteps on GPU.

\\subsection{{Results}}

\\begin{{table}}[h]
\\centering
\\begin{{tabular}}{{lrr|r}}
\\hline
\\textbf{{Metric}} & \\textbf{{Baseline}} & \\textbf{{RL Agent}} & \\textbf{{Improvement}} \\\\
\\hline
Average Travel Time (s) & {comparison['baseline']['avg_travel_time']:.2f} & {comparison['rl']['avg_travel_time']:.2f} & {comparison['improvements']['travel_time_improvement']:+.1f}\\% \\\\
Total Throughput (veh) & {comparison['baseline']['total_throughput']:.0f} & {comparison['rl']['total_throughput']:.0f} & {comparison['improvements']['throughput_improvement']:+.1f}\\% \\\\
Avg Queue Length (veh) & {comparison['baseline']['avg_queue_length']:.1f} & {comparison['rl']['avg_queue_length']:.1f} & {comparison['improvements']['queue_reduction']:+.1f}\\% \\\\
\\hline
\\end{{tabular}}
\\caption{{Section 7.6 RL Performance: Quantitative Results}}
\\label{{tab:section76_results}}
\\end{{table}}

\\subsection{{Conclusion}}

The RL agent demonstrates {{improvements['travel_time_improvement']:.1f}}\\% improvement in travel time 
compared to baseline fixed-time control, confirming \\textbf{{R5}}.

"""
        latex_path = self.output_dir / "section_7_6_content.tex"
        with open(latex_path, 'w') as f:
            f.write(latex_content)
        
        return latex_path
    
    def run_full_pipeline(self):
        """Execute the complete validation pipeline."""
        self.start_time = time.time()
        
        self.log("=" * 80)
        self.log("üöÄ KAGGLE ORCHESTRATION - SECTION 7.6 RL PERFORMANCE")
        self.log("=" * 80)
        self.log(f"Mode: {self.config['mode']}")
        self.log(f"Device: {self.device}")
        self.log(f"Timesteps: {self.config['total_timesteps']}")
        self.log(f"Episodes: {self.config['num_episodes']}")
        
        try:
            # Check GPU
            if self.device == "gpu":
                has_gpu = self.detect_gpu()
                if not has_gpu:
                    self.log("‚ö†Ô∏è  GPU not detected, falling back to CPU", level="WARNING")
                    self.device = "cpu"
            
            # Phase 1: Train
            model, training_history = self.phase_1_train_rl_agent()
            model_path = training_history['model_path']
            
            # Phase 2: Evaluate
            comparison = self.phase_2_evaluate_strategies(model_path)
            
            # Phase 3: Generate outputs
            self.phase_3_generate_outputs(comparison)
            
            self.end_time = time.time()
            total_duration = self.end_time - self.start_time
            
            # Final report
            self.log("=" * 80)
            self.log("‚úÖ VALIDATION COMPLETE")
            self.log("=" * 80)
            self.log(f"Total duration: {total_duration:.1f}s ({total_duration/60:.1f} minutes)")
            self.log(f"Results: {self.output_dir}")
            
            return self.results
            
        except Exception as e:
            self.log(f"‚ùå VALIDATION FAILED: {e}", level="ERROR")
            raise


def main():
    """Command-line entry point."""
    parser = argparse.ArgumentParser(
        description="REAL Kaggle Orchestration for Section 7.6 RL Validation"
    )
    parser.add_argument("--quick", action="store_true",
                       help="Quick test mode (5 min, 100 timesteps)")
    parser.add_argument("--device", choices=["gpu", "cpu"], default="gpu",
                       help="Device for training (gpu on Kaggle, cpu locally)")
    parser.add_argument("--timesteps", type=int, default=None,
                       help="Override timesteps (default: 100 quick, 5000 full)")
    parser.add_argument("--episodes", type=int, default=None,
                       help="Override evaluation episodes (default: 1 quick, 3 full)")
    
    args = parser.parse_args()
    
    # Create orchestrator
    orchestrator = KaggleOrchestration(device=args.device, quick_test=args.quick)
    
    # Override defaults if provided
    if args.timesteps:
        orchestrator.config["total_timesteps"] = args.timesteps
    if args.episodes:
        orchestrator.config["num_episodes"] = args.episodes
    
    # Run pipeline
    results = orchestrator.run_full_pipeline()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
