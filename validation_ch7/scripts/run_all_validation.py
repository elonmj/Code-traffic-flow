#!/usr/bin/env python3
"""
Script Ma√Ætre - Orchestration de Tous les Tests de Validation Chapitre 7
Ex√©cute s√©quentiellement tous les tests et g√©n√®re le rapport de synth√®se
"""

import sys
import subprocess
from pathlib import Path
import json
import pandas as pd
from datetime import datetime
import argparse

# Ajout du chemin vers les utilitaires
sys.path.append(str(Path(__file__).parent))
from validation_utils import create_summary_table, generate_tex_snippet

class ValidationOrchestrator:
    """Orchestrateur principal pour tous les tests de validation"""
    
    def __init__(self, output_dir="validation_ch7/results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.test_scripts = [
            {
                "name": "Section 7.3 - Tests Analytiques",
                "script": "test_section_7_3_analytical.py",
                "revendications": ["R1", "R3"],
                "description": "Validation solutions analytiques et convergence num√©rique"
            },
            {
                "name": "Section 7.4 - Calibration Donn√©es R√©elles", 
                "script": "test_section_7_4_calibration.py",
                "revendications": ["R2"],
                "description": "Calibration et validation sur donn√©es Victoria Island"
            },
            {
                "name": "Section 7.5 - Jumeau Num√©rique",
                "script": "test_section_7_5_digital_twin.py", 
                "revendications": ["R3", "R4", "R6"],
                "description": "Validation comportementale et conservation propri√©t√©s"
            },
            {
                "name": "Section 7.6 - Performance RL",
                "script": "test_section_7_6_rl_performance.py",
                "revendications": ["R5"], 
                "description": "Validation couplage ARZ-RL et performance agents"
            },
            {
                "name": "Section 7.7 - Tests Robustesse",
                "script": "test_section_7_7_robustness.py",
                "revendications": ["R6"],
                "description": "Tests conditions extr√™mes et stabilit√© num√©rique"
            }
        ]
        
        # Crit√®res de validation pour chaque revendication
        self.validation_criteria = {
            "R1": {
                "description": "Pr√©cision m√©thodes num√©riques",
                "metrics": ["convergence_order", "l2_error"],
                "thresholds": {"convergence_order": 4.5, "l2_error": 1e-3}
            },
            "R2": {
                "description": "Reproduction donn√©es observ√©es", 
                "metrics": ["mape", "geh_acceptance"],
                "thresholds": {"mape": 15.0, "geh_acceptance": 85.0}
            },
            "R3": {
                "description": "Conservation propri√©t√©s physiques",
                "metrics": ["mass_conservation_error"],
                "thresholds": {"mass_conservation_error": 1e-6}
            },
            "R4": {
                "description": "Reproduction comportements observ√©s",
                "metrics": ["behavioral_correlation", "temporal_accuracy"],
                "thresholds": {"behavioral_correlation": 0.8, "temporal_accuracy": 0.75}
            },
            "R5": {
                "description": "Performance agents RL",
                "metrics": ["travel_time_improvement", "coupling_stability"],
                "thresholds": {"travel_time_improvement": 5.0, "coupling_stability": 0.99}
            },
            "R6": {
                "description": "Robustesse conditions d√©grad√©es",
                "metrics": ["degraded_performance", "numerical_stability"],
                "thresholds": {"degraded_performance": 75.0, "numerical_stability": 0.95}
            }
        }
    
    def run_single_test(self, test_config, verbose=True):
        """Ex√©cute un test individuel"""
        script_path = Path(__file__).parent / test_config["script"]
        
        if not script_path.exists():
            print(f"‚ùå Script non trouv√© : {test_config['script']}")
            return {
                "test_name": test_config["name"],
                "status": "ERROR",
                "error": f"Script file not found: {test_config['script']}",
                "revendications": test_config["revendications"]
            }
        
        try:
            if verbose:
                print(f"\n{'='*60}")
                print(f"üîÑ Ex√©cution : {test_config['name']}")
                print(f"Script : {test_config['script']}")
                print(f"Revendications : {', '.join(test_config['revendications'])}")
                print(f"{'='*60}")
            
            # Ex√©cution du script dans le bon r√©pertoire de travail
            work_dir = Path(__file__).parent.parent.parent  # Code project directory
            result = subprocess.run(
                [sys.executable, str(script_path.relative_to(work_dir))],
                capture_output=True,
                text=True,
                cwd=work_dir
            )
            
            if result.returncode == 0:
                print(f"‚úÖ {test_config['name']} : SUCC√àS")
                return {
                    "test_name": test_config["name"],
                    "status": "SUCCESS",
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "revendications": test_config["revendications"]
                }
            else:
                print(f"‚ùå {test_config['name']} : √âCHEC (code {result.returncode})")
                if result.stderr:
                    print(f"üîç Erreur stderr: {result.stderr[:500]}")
                if result.stdout:
                    print(f"üîç Sortie stdout: {result.stdout[-500:]}")
                return {
                    "test_name": test_config["name"],
                    "status": "FAILED", 
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "returncode": result.returncode,
                    "revendications": test_config["revendications"]
                }
                
        except Exception as e:
            print(f"‚ùå {test_config['name']} : ERREUR - {e}")
            return {
                "test_name": test_config["name"],
                "status": "ERROR",
                "error": str(e),
                "revendications": test_config["revendications"]
            }

    
    def analyze_revendications_status(self, test_results):
        """Analyse le statut de validation de chaque revendication"""
        revendications_status = {}
        
        for rev_id, criteria in self.validation_criteria.items():
            revendications_status[rev_id] = {
                "description": criteria["description"],
                "status": "NON_TESTED",
                "tests_passed": 0,
                "tests_total": 0,
                "details": []
            }
        
        # Analyse des r√©sultats de tests
        for result in test_results:
            if result["status"] == "SUCCESS":
                for rev_id in result["revendications"]:
                    if rev_id in revendications_status:
                        revendications_status[rev_id]["tests_passed"] += 1
                        revendications_status[rev_id]["tests_total"] += 1
                        revendications_status[rev_id]["details"].append(f"‚úÖ {result['test_name']}")
            else:
                for rev_id in result["revendications"]:
                    if rev_id in revendications_status:
                        revendications_status[rev_id]["tests_total"] += 1
                        revendications_status[rev_id]["details"].append(f"‚ùå {result['test_name']}")
        
        # D√©termination du statut final
        for rev_id, status in revendications_status.items():
            if status["tests_total"] == 0:
                status["status"] = "NON_TESTED"
            elif status["tests_passed"] == status["tests_total"]:
                status["status"] = "VALIDATED"
            elif status["tests_passed"] > 0:
                status["status"] = "PARTIAL"
            else:
                status["status"] = "FAILED"
        
        return revendications_status
    
    def generate_executive_summary(self, test_results, revendications_status):
        """G√©n√®re le r√©sum√© ex√©cutif de la validation"""
        
        # Statistiques globales
        total_tests = len(test_results)
        successful_tests = sum(1 for r in test_results if r["status"] == "SUCCESS")
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Statut des revendications
        validated_revs = sum(1 for r in revendications_status.values() if r["status"] == "VALIDATED")
        total_revs = len(revendications_status)
        validation_rate = (validated_revs / total_revs) * 100 if total_revs > 0 else 0
        
        summary = {
            "execution_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "validated_revendications": validated_revs,
            "total_revendications": total_revs,
            "validation_rate": validation_rate,
            "overall_status": "VALIDATED" if validation_rate >= 83.33 else "PARTIAL"  # 5/6 revendications
        }
        
        return summary
    
    def generate_latex_synthesis(self, test_results, revendications_status, executive_summary):
        """G√©n√®re la synth√®se LaTeX finale"""
        
        # Template pour tableau de synth√®se des revendications
        synthesis_template = """
% Synth√®se Validation Chapitre 7 - Auto-generated
% Date: {execution_date}

\\subsection{{Synth√®se de la Validation}}

Cette section pr√©sente la synth√®se compl√®te de la validation des six revendications du Chapitre 7.

\\subsubsection{{R√©sum√© Ex√©cutif}}

\\begin{{itemize}}
    \\item \\textbf{{Tests ex√©cut√©s}} : {successful_tests}/{total_tests} ({success_rate:.1f}\\% de succ√®s)
    \\item \\textbf{{Revendications valid√©es}} : {validated_revendications}/{total_revendications} ({validation_rate:.1f}\\% de validation)
    \\item \\textbf{{Statut global}} : \\textbf{{{overall_status_text}}}
\\end{{itemize}}

\\subsubsection{{Statut des Revendications}}

\\begin{{table}}[H]
\\centering
\\caption{{Synth√®se de validation des revendications}}
\\label{{tab:revendications_synthesis}}
\\begin{{tabular}}{{|l|l|c|c|c|}}
\\hline
\\textbf{{ID}} & \\textbf{{Description}} & \\textbf{{Tests}} & \\textbf{{R√©ussis}} & \\textbf{{Statut}} \\\\
\\hline
{revendications_rows}
\\hline
\\end{{tabular}}
\\end{{table}}

\\subsubsection{{D√©tail par Section}}

\\begin{{table}}[H]
\\centering
\\caption{{R√©sultats par section de validation}}
\\label{{tab:sections_results}}
\\begin{{tabular}}{{|l|c|c|l|}}
\\hline
\\textbf{{Section}} & \\textbf{{Statut}} & \\textbf{{Revendications}} & \\textbf{{Commentaires}} \\\\
\\hline
{sections_rows}
\\hline
\\end{{tabular}}
\\end{{table}}

\\textbf{{Conclusion}} : {conclusion_text}
"""
        
        # G√©n√©ration des lignes du tableau des revendications
        revendications_rows = []
        for rev_id, status in revendications_status.items():
            status_symbol = {
                "VALIDATED": "‚úì",
                "PARTIAL": "‚ö†",
                "FAILED": "‚úó",
                "NON_TESTED": "‚óØ"
            }.get(status["status"], "?")
            
            row = f"{rev_id} & {status['description']} & {status['tests_total']} & {status['tests_passed']} & {status_symbol} \\\\"
            revendications_rows.append(row)
        
        # G√©n√©ration des lignes du tableau des sections
        sections_rows = []
        for result in test_results:
            status_symbol = "‚úì" if result["status"] == "SUCCESS" else "‚úó"
            revendications_list = ", ".join(result["revendications"])
            comment = "Validation r√©ussie" if result["status"] == "SUCCESS" else "N√©cessite attention"
            
            row = f"{result['test_name']} & {status_symbol} & {revendications_list} & {comment} \\\\"
            sections_rows.append(row)
        
        # D√©termination du texte de conclusion
        if executive_summary["validation_rate"] >= 100:
            conclusion_text = "Toutes les revendications sont valid√©es avec succ√®s. Le mod√®le ARZ et le framework RL satisfont tous les crit√®res de validation."
        elif executive_summary["validation_rate"] >= 83.33:
            conclusion_text = "La majorit√© des revendications sont valid√©es. Les √©l√©ments non valid√©s n√©cessitent des am√©liorations cibl√©es."
        else:
            conclusion_text = "La validation r√©v√®le des lacunes importantes n√©cessitant des r√©visions majeures du mod√®le et/ou des m√©thodes."
        
        # Formatage du template
        template_data = executive_summary.copy()
        template_data.update({
            "revendications_rows": "\n".join(revendications_rows),
            "sections_rows": "\n".join(sections_rows),
            "overall_status_text": "VALID√â" if executive_summary["overall_status"] == "VALIDATED" else "PARTIEL",
            "conclusion_text": conclusion_text
        })
        
        # G√©n√©ration du fichier
        synthesis_content = synthesis_template.format(**template_data)
        output_path = self.output_dir / "chapter_7_synthesis.tex"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(synthesis_content)
        
        print(f"üìÑ Synth√®se LaTeX g√©n√©r√©e : {output_path}")
        return output_path
    
    def run_all_tests(self, sections=None, verbose=True):
        """Ex√©cute tous les tests de validation"""
        
        print("üöÄ D√âMARRAGE VALIDATION COMPL√àTE CHAPITRE 7")
        print(f"R√©pertoire de sortie : {self.output_dir}")
        print(f"Nombre de tests : {len(self.test_scripts)}")
        
        test_results = []
        
        # Ex√©cution des tests s√©lectionn√©s
        for test_config in self.test_scripts:
            if sections is None or any(sec in test_config["name"] for sec in sections):
                result = self.run_single_test(test_config, verbose)
                test_results.append(result)
        
        # Analyse des revendications
        revendications_status = self.analyze_revendications_status(test_results)
        
        # G√©n√©ration r√©sum√© ex√©cutif
        executive_summary = self.generate_executive_summary(test_results, revendications_status)
        
        # G√©n√©ration synth√®se LaTeX
        synthesis_path = self.generate_latex_synthesis(test_results, revendications_status, executive_summary)
        
        # Sauvegarde des r√©sultats complets
        full_results = {
            "executive_summary": executive_summary,
            "test_results": test_results,
            "revendications_status": revendications_status,
            "synthesis_file": str(synthesis_path)
        }
        
        results_json_path = self.output_dir / "full_validation_results.json"
        with open(results_json_path, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False, default=str)
        
        # Affichage du r√©sum√© final
        self.print_final_summary(executive_summary, revendications_status)
        
        return full_results
    
    def print_final_summary(self, executive_summary, revendications_status):
        """Affiche le r√©sum√© final de la validation"""
        
        print(f"\n{'='*80}")
        print("üéØ R√âSUM√â FINAL DE LA VALIDATION")
        print(f"{'='*80}")
        
        print(f"üìä Tests : {executive_summary['successful_tests']}/{executive_summary['total_tests']} r√©ussis ({executive_summary['success_rate']:.1f}%)")
        print(f"üéØ Revendications : {executive_summary['validated_revendications']}/{executive_summary['total_revendications']} valid√©es ({executive_summary['validation_rate']:.1f}%)")
        
        print(f"\nüìã D√âTAIL DES REVENDICATIONS :")
        for rev_id, status in revendications_status.items():
            status_icon = {
                "VALIDATED": "‚úÖ",
                "PARTIAL": "‚ö†Ô∏è", 
                "FAILED": "‚ùå",
                "NON_TESTED": "‚≠ï"
            }.get(status["status"], "‚ùì")
            
            print(f"  {status_icon} {rev_id}: {status['description']} ({status['tests_passed']}/{status['tests_total']} tests)")
        
        print(f"\nüèÜ STATUT GLOBAL : {executive_summary['overall_status']}")
        
        if executive_summary['overall_status'] == 'VALIDATED':
            print("üéâ F√âLICITATIONS ! Toutes les validations sont r√©ussies.")
        else:
            print("üîß Des am√©liorations sont n√©cessaires sur certains aspects.")
        
        print(f"{'='*80}")

def main():
    """Fonction principale avec arguments en ligne de commande"""
    parser = argparse.ArgumentParser(description="Orchestrateur de validation Chapitre 7")
    parser.add_argument("--sections", nargs="*", help="Sections sp√©cifiques √† tester")
    parser.add_argument("--quiet", action="store_true", help="Mode silencieux")
    parser.add_argument("--output", default="validation_ch7/results", help="R√©pertoire de sortie")
    
    args = parser.parse_args()
    
    # Initialisation de l'orchestrateur
    orchestrator = ValidationOrchestrator(output_dir=args.output)
    
    # Ex√©cution des tests
    results = orchestrator.run_all_tests(
        sections=args.sections,
        verbose=not args.quiet
    )
    
    return results

if __name__ == "__main__":
    main()