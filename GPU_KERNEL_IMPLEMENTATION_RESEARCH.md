# Rapport de Recherche: Impl√©mentation des Kernels GPU pour ARZ Model

**Date:** 2025-11-04  
**Objectif:** Documenter les recherches et d√©couvertes avant impl√©mentation des kernels GPU purs

---

## üîç D√âCOUVERTES CRITIQUES

### 1. **CODE GPU EXISTANT AVEC NUMBA CUDA** 
‚úÖ **Le projet utilise d√©j√† Numba CUDA (pas CuPy)** pour les kernels GPU!

**Fichiers GPU existants:**
```
arz_model/numerics/gpu/
‚îú‚îÄ‚îÄ ssp_rk3_cuda.py          # Int√©grateur SSP-RK3 avec kernels Numba
‚îú‚îÄ‚îÄ weno_cuda.py              # Reconstruction WENO5 GPU
‚îî‚îÄ‚îÄ utils.py                  # Utilitaires GPU
```

**Fonctions GPU d√©j√† impl√©ment√©es:**
1. `solve_ode_step_gpu()` - Ligne 715 dans `time_integration.py`
   - Utilise Numba CUDA kernels (`@cuda.jit`)
   - Op√®re sur `cuda.devicearray.DeviceNDArray`
   - **D√©j√† fonctionnel!**

2. `solve_hyperbolic_step_ssprk3_gpu()` - Ligne 1519
   - Utilise la classe `SSP_RK3_GPU` 
   - Support WENO5 + Godunov via kernels existants
   - **D√©j√† fonctionnel!**

3. Classe `SSP_RK3_GPU` dans `ssp_rk3_cuda.py`
   - 3 kernels: `ssp_rk3_stage1_kernel`, `stage2_kernel`, `stage3_kernel`
   - G√®re l'orchestration des 3 √©tapes SSP-RK3
   - **Compl√®tement impl√©ment√©!**

### 2. **PROBL√àME IDENTIFI√â: M√âLANGE NUMBA/CUPY**

Notre impl√©mentation temporaire dans `strang_splitting_step_gpu()` (ligne 1244) fait:
```python
import cupy as cp
U_cpu = cp.asnumpy(U_gpu)  # ‚ùå Transfert GPU‚ÜíCPU
U_star = solve_ode_step_cpu(U_cpu, dt/2, ...)  # ‚ùå Calcul CPU
U_new_gpu = cp.asarray(U_new)  # ‚ùå Transfert CPU‚ÜíGPU
```

**Probl√®me:** M√©lange CuPy (NetworkGrid) avec Numba CUDA (kernels existants)
- CuPy arrays (`cp.ndarray`) ‚â† Numba arrays (`cuda.devicearray.DeviceNDArray`)
- Transferts CPU‚ÜîGPU inutiles √† chaque timestep

---

## üìä ARCHITECTURE ACTUELLE vs CIBLE

### √âtat Actuel (Hybride)
```
NetworkGrid (CuPy)
    ‚Üì cp.asnumpy()
CPU Memory
    ‚Üì solve_ode_step_cpu()
CPU Computation
    ‚Üì cp.asarray()
NetworkGrid (CuPy)
```
**Performance:** ~23 minutes pour test partiel (interrupted)

### Architecture Cible (Pure GPU)
```
NetworkGrid (CuPy)
    ‚Üì CuPy ‚Üí Numba conversion
Numba GPU kernels
    ‚Üì solve_ode_step_gpu()
    ‚Üì solve_hyperbolic_step_ssprk3_gpu()
Numba computation (GPU)
    ‚Üì Numba ‚Üí CuPy conversion
NetworkGrid (CuPy)
```
**Performance attendue:** 2-10x speedup (√©limine transferts CPU)

---

## üõ†Ô∏è SOLUTIONS IDENTIFI√âES

### Solution 1: Conversion CuPy ‚Üî Numba (RECOMMAND√âE)

**Via CUDA Array Interface:**
```python
# CuPy ‚Üí Numba (zero-copy)
cp_array = cp.ndarray(...)
numba_array = cuda.as_cuda_array(cp_array)

# Numba ‚Üí CuPy (zero-copy)
numba_array = cuda.device_array(...)
cp_array = cp.asarray(numba_array)
```

**Source:** CuPy/Numba sont compatibles via [CUDA Array Interface](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html)

### Solution 2: Standardiser sur Numba uniquement

Remplacer NetworkGrid CuPy par Numba partout:
```python
# Au lieu de:
segment['U_gpu'] = cp.asarray(U)

# Utiliser:
segment['U_gpu'] = cuda.to_device(U)
```

**Avantage:** Coh√©rence totale  
**Inconv√©nient:** Refactoring plus important de NetworkGrid

---

## üìö MEILLEURES PRATIQUES GPU (Recherche Web)

### 1. CuPy Documentation
**Source:** https://docs.cupy.dev/en/stable/user_guide/kernel.html

**3 types de kernels CuPy:**
- `ElementwiseKernel`: Op√©rations √©l√©ment par √©l√©ment (similaire √† broadcasting NumPy)
- `ReductionKernel`: Op√©rations de r√©duction (sum, max, etc.)
- `RawKernel`: Kernels CUDA C/C++ bruts (maximum de contr√¥le)

**Exemple ElementwiseKernel:**
```python
squared_diff = cp.ElementwiseKernel(
    'float32 x, float32 y',  # Input
    'float32 z',              # Output
    'z = (x - y) * (x - y)',  # Operation
    'squared_diff'            # Name
)
```

### 2. Performance Best Practices
**Source:** https://docs.cupy.dev/en/stable/user_guide/performance.html

**Benchmarking:**
```python
from cupyx.profiler import benchmark
print(benchmark(my_func, (a,), n_repeat=20))
# Output: CPU: 44.407 us, GPU-0: 181.565 us
```

**Optimisations cl√©s:**
1. **Minimiser transferts CPU‚ÜîGPU** (notre probl√®me actuel!)
2. **Utiliser CUB backend** pour r√©ductions: `CUPY_ACCELERATORS=cub`
3. **Batch operations** plut√¥t qu'op√©rations s√©quentielles
4. **Overlapping work** avec streams CUDA

### 3. Numba CUDA
**Source:** https://numba.readthedocs.io/en/stable/cuda/index.html

**Kernel signature:**
```python
@cuda.jit
def my_kernel(input_arr, output_arr, N):
    i = cuda.grid(1)  # Thread index
    if i < N:
        output_arr[i] = input_arr[i] * 2
```

**Device functions (r√©utilisables):**
```python
@cuda.jit(device=True)
def helper_function(x):
    return x * x

@cuda.jit
def kernel_using_helper(arr, N):
    i = cuda.grid(1)
    if i < N:
        arr[i] = helper_function(arr[i])
```

---

## üéØ PLAN D'IMPL√âMENTATION RECOMMAND√â

### Phase 1: Conversion CuPy ‚Üî Numba (PRIORIT√â 1)
**Fichier:** `arz_model/numerics/time_integration.py`

Modifier `strang_splitting_step_gpu()`:
```python
def strang_splitting_step_gpu(U_gpu_cupy, dt, grid, params, seg_id=None):
    """GPU Strang splitting using existing Numba kernels."""
    from numba import cuda
    import cupy as cp
    
    # Convert CuPy ‚Üí Numba (zero-copy via CUDA Array Interface)
    U_gpu_numba = cuda.as_cuda_array(U_gpu_cupy)
    
    # Step 1: ODE (dt/2) - GPU via existing kernel
    d_R = cuda.to_device(grid.road_quality[grid.physical_cell_indices])
    U_star_numba = solve_ode_step_gpu(U_gpu_numba, dt/2, grid, params, d_R)
    
    # Step 2: Hyperbolic (dt) - GPU via existing kernel
    U_ss_numba = solve_hyperbolic_step_ssprk3_gpu(U_star_numba, dt, grid, params, None)
    
    # Step 3: ODE (dt/2) - GPU via existing kernel
    U_new_numba = solve_ode_step_gpu(U_ss_numba, dt/2, grid, params, d_R)
    
    # Convert Numba ‚Üí CuPy (zero-copy)
    U_new_cupy = cp.asarray(U_new_numba)
    
    return U_new_cupy
```

**Temps estim√©:** 30-60 minutes  
**Gain attendu:** 5-10x speedup (√©limine transferts CPU)

### Phase 2: Tests et Validation (PRIORIT√â 2)
**Fichier:** `tests/test_gpu_small_timestep.py`

1. V√©rifier que le test passe avec GPU purs
2. Benchmarker CPU vs GPU avec `cupyx.profiler.benchmark`
3. Valider stabilit√© (v_max < 20 m/s, rho > 0.08)

**Temps estim√©:** 15-30 minutes

### Phase 3: Optimisations Avanc√©es (OPTIONNEL)
Si besoin de plus de performance:
1. Profiler avec Nsight Systems
2. Optimiser les kernels existants (shared memory, coalescing)
3. Overlapping computation avec streams CUDA

---

## ‚úÖ ACTIONS IMM√âDIATES

1. **Impl√©menter conversion CuPy‚ÜîNumba** dans `strang_splitting_step_gpu()`
   - Utiliser `cuda.as_cuda_array()` et `cp.asarray()`
   - Appeler `solve_ode_step_gpu()` et `solve_hyperbolic_step_ssprk3_gpu()` existants

2. **Tester avec `pytest tests/test_gpu_small_timestep.py`**
   - V√©rifier que v_max reste stable (<20 m/s)
   - Mesurer le temps d'ex√©cution

3. **Benchmarker performance**
   - Comparer avec version CPU
   - Documenter les gains

---

## üìñ R√âF√âRENCES

1. **CuPy User Guide - Kernels**  
   https://docs.cupy.dev/en/stable/user_guide/kernel.html

2. **CuPy Performance Best Practices**  
   https://docs.cupy.dev/en/stable/user_guide/performance.html

3. **Numba CUDA Documentation**  
   https://numba.readthedocs.io/en/stable/cuda/index.html

4. **CUDA Array Interface (Interop CuPy/Numba)**  
   https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html

5. **Existing GPU Code in Project:**
   - `arz_model/numerics/gpu/ssp_rk3_cuda.py`
   - `arz_model/numerics/gpu/weno_cuda.py`
   - `arz_model/numerics/time_integration.py` (lines 715, 1519)

---

## üí° CONCLUSION

**D√âCOUVERTE MAJEURE:** Le code GPU existe d√©j√† avec Numba CUDA! Notre probl√®me n'est pas d'impl√©menter les kernels depuis z√©ro, mais de **connecter correctement** le frontend CuPy (NetworkGrid) avec les kernels Numba existants.

**Solution:** Utiliser CUDA Array Interface pour conversion zero-copy entre CuPy et Numba.

**Temps d'impl√©mentation:** 1-2 heures maximum (pas plusieurs jours!)

**Gain de performance attendu:** 5-10x speedup en √©liminant les transferts CPU‚ÜîGPU actuels.
